\chapter{Grundlagen}
Dieses Kapitel bildet die technische und theoretische Grundlage der Arbeit. Es erläutert die wichtigsten Konzepte, auf denen das entwickelte System basiert und vermittelt somit das notwendige Verständnis für die folgenden Kapitel.

\section{Virtuelle Realität und Extended Reality}
Als \textit{Virtuelle Realität} (VR) wird eine durch Computer generierte, interaktive Darstellung einer dreidimensionalen Umgebung bezeichnet, in der sich Nutzer immersiv bewegen und mit Objekten interagieren können. \needcitation Durch sogenannte Head-Mounted Displays (HMDs) wird ein räumliches Verständnis geschaffen, indem den Nutzern ein stereoskopisches Bild, Head-Tracking und 3D-Audio geboten werden. \needcitation

Die Weiterentwicklung der klassischen virtuellen Realität ist die sogenannte \textit{Extended Reality} (XR), welche virtuelle und reale Elemente zu einem gemeinsamen Erlebniskontext vereint. Dies wird bei unterstützten HMDs durch eine Passthrough-Funktion ermöglicht, bei der das Bild einer Headet-Kamera an den Nutzer ausgegeben und in Echtzeit um virtuelle Objekte ergänzt wird. Der Grad der Immersion reicht dabei von einfacher \textit{Augmented Reality} (AR), bei der die reale Umgebung lediglich um digitale Informationen erweitert wird, bis hin zur \textit{Mixed Reality} (MR), in der eine kohärente Interaktion zwischen realer und virtueller Welt möglich ist. \needcitation

Gerade für Mixed-Reality-Anwendungen ist ein präzises räumliches Verständnis der Umgebung essenziell, da virtuelle Objekte nur dann glaubwürdig mit der physischen Welt interagieren können, wenn diese zuvor exakt rekonstruiert wurde. \needcitation

\section{3D-Rekonstruktion}

Die 3D-Rekonstruktion beschreibt den Prozess, bei dem aus zweidimensionalen Bilddaten eine virtuelle, möglichst akkurate dreidimensionale Repräsentation der realen Umgebung erzeugt wird. Ziel ist es, Geometrie, Textur und räumliche Struktur so zu rekonstruieren, dass sie der ursprünglichen Szene möglichst genau entsprechen. \needcitation

Ergänzt man die Eingangsdaten um Kameraparameter, sogenannte \textit{Intrinsiken} (z.\,B.\ Brennweite und Hauptpunkt) und \textit{Extrinsiken} (Kameraposition und -orientierung), lässt sich die Genauigkeit der Rekonstruktion erheblich steigern. In der Forschung existieren hierfür verschiedene Ansätze, die sich vor allem in ihrer Datenbasis, Genauigkeit und Echtzeitfähigkeit unterscheiden.

\subsection{Photogrammetrie}

Die Photogrammetrie nutzt mehrere überlappende Bilder, um die räumliche Struktur einer Szene zu rekonstruieren. 
Zunächst werden charakteristische Punkte in den Bildern identifiziert, die in mehreren Aufnahmen vorkommen. 
Dieser Schritt wird als \textit{Korrespondenzsuche} bezeichnet und dient dazu, gleiche reale Punkte in unterschiedlichen Perspektiven wiederzuerkennen. 
Aus den gefundenen Punktpaaren lässt sich durch \textit{Triangulation} die räumliche Position der Punkte bestimmen. 
So entsteht eine dichte \textit{Punktwolke}, aus der anschließend ein 3D-Modell abgeleitet wird. 
Photogrammetrische Verfahren liefern sehr detaillierte Ergebnisse, sind jedoch rechenintensiv und daher meist nur für Offline-Verarbeitung geeignet. \needcitation

\subsection{SLAM-Verfahren}

\textit{Simultaneous Localization and Mapping} (SLAM) bezeichnet Verfahren, bei denen ein System seine eigene Position bestimmt und gleichzeitig eine Karte der Umgebung erstellt. Während die Lokalisierung durch die kontinuierliche Schätzung der Kamerapose erfolgt, wird die Szene in Form einer Karte oder Punktwolke aufgebaut. SLAM-Ansätze arbeiten typischerweise in Echtzeit und werden daher häufig in Robotik-, AR- und VR-Anwendungen eingesetzt. Sie liefern zwar weniger detailreiche, dafür aber latenzarme Rekonstruktionen, die besonders für dynamische Szenen geeignet sind. \needcitation

\subsection{Neuronale Rekonstruktion}

Neuere Verfahren nutzen tief neuronale Netze, um Bildfolgen direkt in volumetrische oder strahlbasierte 3D-Darstellungen zu überführen. 
Volumetrische Ansätze, wie \textit{NeuralRecon} oder \textit{VisFusion}, rekonstruieren Szenen in einem \textit{Truncated Signed Distance Field} (TSDF) — einer voxelbasierten Darstellung, die für jedes Volumenelement den Abstand zur nächsten Oberfläche speichert. 
Strahlbasierte Verfahren, wie \textit{SLAM3R}, modellieren dagegen die Szene kontinuierlich entlang von Lichtstrahlen, wie es aus sogenannten \textit{Neural Radiance Fields} (NeRFs) bekannt ist. 
Diese Methoden erlauben eine dichtere und oft realistischere Rekonstruktion, erfordern jedoch hohe GPU-Leistung und komplexe Trainingsdaten. \needcitation

\subsection{Qualitätskriterien der 3D-Rekonstruktion}

Die Qualität einer 3D-Rekonstruktion lässt sich aus verschiedenen Perspektiven bewerten.
Für das im Rahmen dieser Arbeit entwickelte System, das auf Echtzeitverarbeitung und
Integration in eine VR-Umgebung abzielt, stehen insbesondere vier Kriterien im Mittelpunkt:
\emph{Genauigkeit}, \emph{Vollständigkeit}, \emph{Latenz} und \emph{Robustheit}.
Tabelle~\ref{tab:qualitaetskriterien} fasst die zentralen Eigenschaften und deren Relevanz
für Echtzeit-VR-Szenarien zusammen.

\begin{table}[h!]
    \centering
    \caption{Qualitätskriterien der 3D-Rekonstruktion im Kontext von Echtzeit-VR}
    \label{tab:qualitaetskriterien}
    \begin{tabular}{p{3cm}p{6cm}p{5cm}}
    \toprule
    \textbf{Kriterium} & \textbf{Beschreibung} & \textbf{Bedeutung für Echtzeit-VR}\\
    \midrule
    \textbf{Genauigkeit} &
    Grad der Übereinstimmung zwischen rekonstruierter und realer Geometrie. &
    Ermöglicht realistische Maßstäblichkeit und stabile Interaktion mit der virtuellen Umgebung.\\[4pt]

    \textbf{Vollständigkeit} &
    Anteil der Szene, der erfolgreich rekonstruiert wurde. &
    Beeinflusst die visuelle Plausibilität und Immersion — Lücken oder Artefakte stören das Raumgefühl.\\[4pt]

    \textbf{Latenz} &
    Zeitspanne zwischen Datenerfassung und Anzeige des aktualisierten Modells. &
    Wesentlich für flüssige Nutzerinteraktion und Synchronität zwischen realer Bewegung und virtueller Rückmeldung.\\[4pt]

    \textbf{Robustheit} &
    Widerstandsfähigkeit gegenüber Störeinflüssen wie Beleuchtungsänderungen,
    Bewegungsunschärfe oder dynamischen Objekten. &
    Bestimmt die Stabilität und Nutzbarkeit des Systems unter realen Bedingungen.\\
    \bottomrule
    \end{tabular}
\end{table}

Diese Kriterien bilden die Grundlage für die in Kapitel~6 beschriebene
Evaluation. Sie erlauben eine systematische Bewertung der integrierten Modelle in Bezug
auf technische Leistungsfähigkeit und Praxistauglichkeit im VR-Kontext.

\section{Systemische Grundlagen}

\subsection{Container-Technologie und Docker}

Container-Technologien ermöglichen es, Anwendungen mitsamt ihren Abhängigkeiten in isolierten Umgebungen auszuführen. Dadurch lassen sich Softwarekomponenten unabhängig vom Hostsystem betreiben, was die Reproduzierbarkeit und Portabilität von Anwendungen wesentlich verbessert.\needcitation
Ein Container enthält alle zur Laufzeit benötigten Komponenten einer Anwendung, wie etwa Bibliotheken, Konfigurationsdateien und Laufzeitumgebungen. Dadurch wird sichergestellt, dass die Software in unterschiedlichen Umgebungen identisch funktioniert. Im Gegensatz zu virtuellen Maschinen teilen sich Container den Kernel des Host-Betriebssystems, wodurch sie deutlich ressourcenschonender sind.\needcitation 

Die bekannteste Implementierung einer solchen Container-Technologie ist \textit{Docker}.Docker abstrahiert den Aufbau und die Verwaltung von Containern über eine einheitliche Befehls- und Laufzeitumgebung. Anwendungen werden über sogenannte \texttt{Dockerfiles} beschrieben, welche Basis-Images, Abhängigkeiten und Startbefehle definieren. Damit lassen sich auch komplexe Softwarestapel konsistent aufbauen und verteilen.  

Für Systeme, die aus mehreren Containern bestehen, bietet Docker das Werkzeug \textit{Docker~Compose}. Compose ermöglicht die Orchestrierung mehrerer Container über eine zentrale \texttt{docker-compose.yml}-Datei. In dieser Datei werden die beteiligten Dienste, ihre Netzwerkkonfiguration, gemeinsam genutzte Volumes und Umgebungsvariablen beschrieben. Dadurch können zusammengehörige Container – beispielsweise ein Backend, mehrere GPU-Modelle und eine Datenbank – mit einem einzigen Befehl gestartet, skaliert oder beendet werden.

\subsection{WebSockets}

WebSockets sind ein Netzwerkprotokoll, das eine bidirektionale, dauerhafte Verbindung zwischen Client und Server ermöglicht. Im Gegensatz zu klassischen HTTP-Verbindungen, die nach jeder Anfrage beendet werden, erlaubt eine WebSocket-Verbindung den kontinuierlichen Austausch von Nachrichten in beide Richtungen.\needcitation 
Dies macht sie besonders geeignet für Anwendungen mit Echtzeitanforderungen, bei denen Daten fortlaufend übertragen werden müssen.

Nach dem initialen Verbindungsaufbau über ein HTTP-Handshake wird die Verbindung auf das WebSocket-Protokoll umgestellt. Von diesem Zeitpunkt an können sowohl Client als auch Server asynchron Daten senden, ohne dass eine neue Anfrage notwendig ist. Dadurch lassen sich Ereignisse nahezu verzögerungsfrei austauschen. \needcitation

\section{Zielumgebung}

\subsection{Unity als VR-Plattform}

Die Game-Engine \textit{Unity} ist eine der am weitesten verbreiteten Entwicklungsumgebungen für interaktive 3D- und Virtual-Reality-Anwendungen. \needcitation Sie ermöglicht die Erstellung immersiver Szenen, in denen Nutzer in Echtzeit mit virtuellen Objekten interagieren können. 
Unity basiert auf einer komponentenorientierten Architektur, bei der jede Entität durch sogenannte \textit{GameObjects} beschrieben wird. Diese können durch \textit{Components} um Funktionen wie Physik, Rendering, Eingabe oder Skriptlogik erweitert werden. Dadurch lassen sich komplexe Systeme modular und flexibel gestalten.

Für Virtual-Reality-Anwendungen stellt Unity eine Reihe spezialisierter XR-Subsysteme und Plugins bereit, die Head-Tracking, stereoskopisches Rendering und die Integration gängiger Head-Mounted Displays (HMDs) wie Meta Quest, HTC Vive oder Valve Index ermöglichen.\needcitation 
Darüber hinaus bietet Unity eine leistungsfähige \texttt{C\#}-Scripting-API, über die Entwickler eigene Interaktionslogiken, Benutzeroberflächen oder Netzwerkkommunikation implementieren können.  
Die Engine unterstützt moderne Grafik-APIs wie OpenGL, Vulkan und DirectX sowie unterschiedliche Renderpipelines (Built-in, Universal Render Pipeline, High Definition Render Pipeline), wodurch sie sich sowohl für mobile als auch für hochauflösende VR-Anwendungen eignet. \needcitation

Ein weiterer Vorteil von Unity ist die plattformübergreifende Unterstützung. Projekte können auf verschiedenen Zielsystemen ausgeführt werden, darunter Windows, Linux, macOS, Android und WebGL. In Verbindung mit XR-Toolkits und Netzwerkschnittstellen bietet Unity somit eine flexible Grundlage für die Entwicklung immersiver, interaktiver Umgebungen.

\subsection{Das Va.Si.Li-Lab}

Das \textit{Va.Si.Li-Lab} (\textit{Virtual and Simulation-Based Learning Laboratory}) ist eine
virtuelle Forschungs- und Lernumgebung des Text Technology Lab der Goethe-Universität
Frankfurt. Es dient als Plattform zur Untersuchung und Durchführung simulationsbasierter
Lernszenarien in immersiven, mehrbenutzerfähigen Virtual-Reality-Umgebungen. Ziel des
Labors ist es, soziale Interaktionen, Kommunikationsprozesse und kollaboratives Lernen in
realitätsnahen, virtuellen Kontexten erfahr- und analysierbar zu machen. \cite{vasililab}

Technisch basiert das Va.Si.Li-Lab auf der Game-Engine \textit{Unity} und dem Mehrbenutzer-
Framework \textit{Ubiq}, das synchrone Interaktionen mehrerer Teilnehmender in gemeinsamen
VR-Szenen ermöglicht. Ergänzend kommen Schnittstellen zur multimodalen Datenerfassung
zum Einsatz, die etwa Sprach-, Gesten- und Blickdaten erfassen und in einer zentralen Daten-
bank speichern. Dadurch wird eine detaillierte Auswertung von Lern- und Kommunikations-
prozessen innerhalb der Simulationen möglich.

Für die vorliegende Arbeit ist das Va.Si.Li-Lab von besonderer Bedeutung, da es eine
bestehende, modular erweiterbare VR-Infrastruktur bereitstellt, in die neue Systeme inte
griert werden können. Es bietet eine ausgereifte Grundlage für Mehrbenutzer-Interaktionen,
Synchronisation und Datenmanagement, jedoch bislang keine Möglichkeit zur \textbf{dynamischen
3D-Rekonstruktion realer Umgebungen}. Die in den Simulationen genutzten Szenen müssen
derzeit manuell modelliert oder aus bestehenden 3D-Ressourcen geladen werden, was die
Übertragbarkeit realer Lern- oder Forschungssituationen einschränkt.

% Das in dieser Arbeit entwickelte System adressiert genau diese Lücke. Durch die Integration
% einer Echtzeit-Rekonstruktionspipeline wird das Va.Si.Li-Lab um die Fähigkeit erweitert, reale
% Umgebungen während der Laufzeit zu erfassen, zu rekonstruieren und unmittelbar in die
% virtuelle Szene einzubinden. Damit entsteht ein neuartiger Anwendungsbereich, in dem VR-
% Sitzungen nicht länger ausschließlich auf vorab erstellten Szenen basieren, sondern auf der
% \textit{aktuellen physischen Umgebung} der Nutzenden aufbauen können.
% 
% Diese Erweiterung eröffnet Perspektiven für adaptive und kontextbezogene Lernszenarien,
% in denen reale Räume, Objekte und Bewegungen Teil der virtuellen Interaktion werden. Das
% Va.Si.Li-Lab bildet somit die Zielumgebung und Integrationsplattform für das in dieser Arbeit
% entwickelte Rekonstruktionssystem.