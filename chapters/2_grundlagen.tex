\chapter{Grundlagen}
\label{ch:grundlagen}

Die Implementierung von 3D-Rekonstruktionsverfahren in Virtual-Reality-Anwendungen erfordert das Zusammenspiel verschiedener technologischer Bereiche. Um die in dieser Arbeit entwickelte Systemarchitektur nachvollziehen zu können, sind Kenntnisse über Virtual-Reality, Rekonstruktionsverfahren und verteilte Systeme notwendig. Daher führt dieses Kapitel zuerst in die theoretischen und technischen Grundlagen ein. Zunächst werden die Konzepte der Virtual Reality, dann die relevanten Verfahren der 3D-Rekonstruktion und schließlich die verwendeten Systemtechnologien sowie die Zielumgebung vorgestellt.

\section{Virtual Reality}
\label{sec:virtual-reality}

Virtual Reality (VR) bezeichnet eine computergenerierte, dreidimensionale Umgebung, in die Nutzende vollständig eintauchen und in Echtzeit interagieren können. \needcitation Moderne VR-Systeme nutzen Head-Mounted Displays (HMDs), die stereoskopische Bilder mit hoher Bildwiederholrate darstellen und durch präzises Tracking von Kopf-, Hand- und Körperbewegungen eine natürliche Interaktion erlauben. Diese Displays bestehen typischerweise aus hochauflösenden Bildschirmen für jedes Auge, optischen Linsen zur Fokussierung und integrierten Sensoren zur Bewegungserfassung. 

Aktuelle Systeme wie die Meta-Quest Serie bieten zusätzlich eine \textit{Passthrough}-Funktionalität, bei der die in den HMDs verbauten Frontkameras die reale Umgebung erfassen und in Echtzeit darstellen können. Dadurch werden Mixed-Reality-Anwendungen ermöglicht, bei denen virtuelle Inhalte in die reale Umgebung eingebettet werden. \needcitation

Für eine konsistente VR-Erfahrung ist die präzise Erfassung der Position und Orientierung des HMDs sowie der Controller erforderlich. Moderne Systeme nutzen dafür \textit{Inside-Out-Tracking}, bei dem die im Headset verbauten Kameras die Umgebung erfassen und daraus die eigene Position relativ zu einem definierten Referenzpunkt ableiten. \needcitation Die Zusammensetzung dieser Pose wird in Kapitel~\ref{sec:extrinsics} mithilfe der extrinsischen Parameter definiert. 

\section{3D-Rekonstruktion}
\label{sec:3d-rekonstruktion}

3D-Rekonstruktion bezeichnet den Prozess, aus zweidimensionalen Bildern oder Bildsequenzen eine dreidimensionale Darstellung einer Szene zu erzeugen. In dieser Arbeit ist insbesondere die monokulare Rekonstruktion relevant, bei der die 3D-Rekonstruktion der Szene aus den Daten einer einzelnen Kamera bzw. einem fortlaufenden Bildstrom errechnet wird. Im Gegensatz zu stereoskopischen oder RGB-D-Verfahren stehen dabei keine direkten Tiefeninformationen zur Verfügung. Stattdessen muss die Tiefe aus der Bewegung der Kamera (Structure from Motion) oder durch lernbasierte Tiefenschätzung abgeleitet werden. \needcitation Monokulare Verfahren sind daher besonders für VR-Headsets relevant, da diese typischerweise RGB-Kameras ohne dedizierte Tiefensensoren verwenden.

\subsection{Kameramodell}
\label{sec:extrinsics}

Das Kameramodell beschreibt die mathematische Beziehung zwischen 3D-Punkten in der realen Welt und ihrer Projektion auf die 2D-Bildebene. Es wird durch intrinsische und extrinsische Parameter charakterisiert, die zusammen die vollständige geometrische Beschreibung der Kameraaufnahme ermöglichen. \needcitation

Die intrinsischen Parameter beschreiben die optischen Eigenschaften einer Kamera und umfassen die Brennweite $f_x, f_y$ in Pixel-Einheiten, den Hauptpunkt $(c_x, c_y)$, der die Position des optischen Zentrums im Bild angibt, sowie optionale Verzeichnungsparameter zur Korrektur optischer Verzerrungen. Diese Parameter sind kameraspezifisch und bleiben über verschiedene Aufnahmen hinweg konstant, solange die Einstellungen der Kamera nicht verändert werden.

Die extrinsischen Parameter beschreiben dagegen die Position und Orientierung der Kamera im 3D-Raum. Sie werden durch eine Translation $\mathbf{t} \in \mathbb{R}^3$ und eine Rotation $\mathbf{R} \in SO(3)$ definiert, wobei letztere alternativ auch als Quaternion $\mathbf{q}$ repräsentiert werden kann. Diese Parameter ändern sich mit jeder Kamerabewegung und müssen für jedes aufgenommene Bild bekannt oder geschätzt werden, um eine konsistente 3D-Rekonstruktion zu ermöglichen. In VR-Systemen werden die extrinsischen Parameter typischerweise durch das Tracking-System des Headsets bereitgestellt.

\subsection{Volumetrische Verfahren}

Volumetrische Verfahren repräsentieren die 3D-Szene als diskretes Gitter von Voxeln, wobei jeder Voxel Informationen über die lokale Geometrie speichert. Eine weit verbreitete Darstellung ist das \textit{Truncated Signed Distance Field} (TSDF), bei dem für jeden Voxel die Distanz zur nächsten Oberfläche gespeichert wird. Positive Distanzen kennzeichnen dabei den Bereich vor der Oberfläche, negative den Bereich dahinter. \needcitation

Der zentrale Vorteil volumetrischer Verfahren liegt in der schrittweisen Integration mehrerer Ansichten. Durch die fortlaufende Fusion von Beobachtungen aus verschiedenen Kameraperspektiven entsteht eine dichte, global konsistente 3D-Repräsentation. Aus dieser Volumenrepräsentation kann durch den Marching-Cubes-Algorithmus \needcitation, ein vollständiges Mesh der rekonstruierten Szene erzeugt werden. Diese Verfahren sind besonders geeignet für Echtzeitanwendungen, da sie inkrementelle Updates ermöglichen und eine effiziente GPU basierte Beschleunigung unterstützen.

\subsection{SLAM-Verfahren}

Simultaneous Localization and Mapping (SLAM) bezeichnet Verfahren, die die Position der Kamera schätzen während sie gleichzeitig eine Karte der Umgebung erstellen. Dieser duale Ansatz ist besonders relevant für monokulare Rekonstruktion, da ohne externe Positionsinformationen die Kamerabewegung aus den Bildern selbst abgeleitet werden muss. \needcitation

Klassische SLAM-Systeme basieren auf der Extraktion und Verfolgung visueller Merkmale über mehrere Bilder hinweg. Durch die gleichzeitige Optimierung von Kameraposen und 3D-Punktpositionen entsteht eine konsistente räumliche Repräsentation der Szene. \needcitation Moderne, lernbasierte Ansätze erweitern dieses Prinzip durch den Einsatz neuronaler Netze zur robusten Merkmals-Extraktion und Tiefenschätzung. Diese hybriden Verfahren kombinieren die geometrische Präzision klassischer SLAM-Systeme mit der Generalisierungsfähigkeit datengetriebener Modelle. Dadurch sind sie robuster gegenüber schwierigen Beleuchtungsbedingungen oder textuarmen Szenen. \needcitation

\subsection{3D-Repräsentationsformate}

Die mit den verschiedenen Verfahren erstellten Rekonstruktionen müssen für die Nutzenden visualisiert werden. Hierfür werden standardisierte Formate verwendet, die sich in Bezug auf Speichereffizienz, Detailgrad und Rendering-Performance unterscheiden.

\textbf{Punktwolken} sind ungeordnete Mengen von 3D-Punkten, die optional Farbinformationen enthalten können. Jeder Punkt repräsentiert eine diskrete Oberflächenabtastung, zwischen den Punkten besteht jedoch keine explizite Verbindung. Punktwolken sind speichersparend und eignen sich für die schnelle Visualisierung großer Szenen. Sie bieten jedoch keine explizite Oberflächenrepräsentation. Für die Darstellung in Unity werden Punktwolken über den Visual Effect Graph GPU-beschleunigt gerendert.

\textbf{Meshes} bestehen aus Vertices (Punkten), Edges (Kanten) und Faces (Flächen) und beschreiben die Oberflächengeometrie explizit als zusammenhängendes Netz. Das \textit{GLB}-Format ist eine binäre Variante des glTF-Formats (GL Transmission Format) und bündelt Geometrie, Texturen und Materialinformationen effizient in einer Datei. GLB ist besonders für die Echtzeitübertragung geeignet, da es kompakt ist und von gängigen 3D-Engines direkt geladen werden kann. \needcitation

\section{Systemtechnologien}
\label{sec:systemtechnologien}

Für die Umsetzung eines verteilten Echtzeitsystems zur 3D-Rekonstruktion sind geeignete Technologien für Kommunikation, Steuerung und Datenverarbeitung erforderlich. Die folgenden Abschnitte beschreiben die wesentlichen technologischen Grundlagen der Backend-Architektur.

\subsection{Client-Server-Architektur}

In einer Client-Server-Architektur werden Aufgaben zwischen den verschiedenen Komponenten aufgeteilt. Clients übernehmen die Rolle der Datenerzeugung und Visualisierung, während Server rechenintensive Verarbeitungsaufgaben ausführen. Diese Aufteilung ist besonders vorteilhaft für VR-Anwendungen, da mobile Endgeräte oft nur begrenzte Rechenkapazität besitzen, während die rechenintensiven Rekonstruktionsalgorithmen auf leistungsfähigen GPU-Servern ausgeführt werden können. Für Echtzeitanwendungen ist eine bidirektionale, persistente Verbindung erforderlich, die es beiden Seiten ermöglicht, asynchron Daten zu senden und zu empfangen.

\subsection{WebSocket-Protokoll}

Das WebSocket-Protokoll ermöglicht eine vollständig bidirektionale Kommunikation über eine persistente TCP-Verbindung. Im Gegensatz zum klassischen HTTP-Polling, bei dem der Client wiederholt Anfragen stellen muss, kann der Server bei WebSocket jederzeit Daten an verbundene Clients senden. Das reduziert Latenz und Netzwerk-Overhead erheblich und ist daher für Streaming-Anwendungen mit kontinuierlichem Datenfluss besonders geeignet. Nach dem anfänglichen Handshake bleibt die Verbindung dauerhaft bestehen, wodurch die Overhead-Kosten wiederholter Verbindungsaufbauten entfallen. \needcitation

\subsection{Containerisierung}

Container-Technologien ermöglichen die Ausführung von Anwendungen in isolierten Laufzeitumgebungen, die alle benötigten Abhängigkeiten enthalten. Dadurch wird sichergestellt, dass die Software unabhängig von der Host-Umgebung konsistent funktioniert. Im Gegensatz zu virtuellen Maschinen nutzen Container den Kernel des Host-Systems. Dadurch sind sie deutlich ressourcenschonender, bieten aber ähnliche Isolationseigenschaften. \needcitation

\textit{Docker} ist die verbreitetste Implementierung dieses Ansatzes. Es abstrahiert die Erstellung und Verwaltung von Containern über eine einheitliche Befehls- und Laufzeitumgebung. Anwendungen werden über sogenannte \texttt{Dockerfiles} beschrieben, die Basis-Images, Abhängigkeiten und Startbefehle definieren. Für mehrteilige Systeme bietet Docker mit \texttt{Docker Compose} ein Werkzeug zur Orchestrierung mehrerer Container über eine zentrale \texttt{docker-compose.yml}-Datei. In dieser Datei lassen sich Dienste, Netzwerkkonfiguration, gemeinsam genutzte Volumes und Umgebungsvariablen festlegen. Dadurch ist es möglich, komplexe Softwarestapel konsistent zu starten und zu skalieren. \needcitation

Für Deep-Learning-Anwendungen ist der Zugriff auf GPU-Ressourcen erforderlich. Diesen Zugriff ermöglicht das NVIDIA Container Toolkit. Das Toolkit stellt die erforderlichen Treiber und Bibliotheken innerhalb des Containers bereit, sodass CUDA-basierte Anwendungen ohne zusätzliche Konfiguration auf der Host-GPU ausgeführt werden können. \needcitation Containerisierung eignet sich besonders für modulare Forschungs- und Prototyping-Systeme, bei denen unterschiedliche Frameworks (z.\,B.\ PyTorch-Versionen) oder GPU-Abhängigkeiten parallel betrieben werden müssen.

\section{Zielumgebung}
\label{sec:zielumgebung}

Das entwickelte System wird in eine bestehende VR-Infrastruktur integriert. Der folgende Abschnitt beschreibt die technologischen Grundlagen der Frontend-Entwicklung und die spezifische Zielplattform.

\subsection{Unity und XR-Frameworks}

Unity ist eine der führenden Entwicklungsumgebungen für interaktive 3D-Anwendungen. \needcitation
Die Engine basiert auf einer komponentenorientierten Architektur. Jedes Objekt wird durch ein \textit{GameObject} beschrieben und kann durch \textit{Components} um Funktionen wie Physik, Rendering oder Benutzereingaben erweitert werden kann.

Für VR-Anwendungen stellt Unity spezialisierte XR-Subsysteme und Plug-ins bereit. Diese ermöglichen Head-Tracking, stereoskopisches Rendering und die Integration gängiger HMDs (z.\,B.\ Meta Quest, HTC Vive, Valve Index).
Über die C\#-Scripting-API lassen sich Interaktionslogik, Benutzeroberflächen und Netzwerkkommunikation flexibel implementieren.
Dank der Unterstützung moderner Grafik-APIs (OpenGL, Vulkan, DirectX) und plattformübergreifender Exportmöglichkeiten ist Unity sowohl für mobile als auch für hochauflösende VR-Anwendungen geeignet.

\subsection{Va.Si.Li-Lab}

Das Va.Si.Li-Lab (Virtual Simulation Lab) des Text Technology Labs der Goethe-Universität Frankfurt ist eine mehrbenutzerfähige VR-Plattform für simulationsbasierte Lern- und Forschungsszenarien. Sie wurde im Rahmen des Projekts \glqq Digital Teaching and Learning Lab\grqq \ entwickelt und dient primär der Analyse multimodaler Interaktionsdaten, die in simulationsbasierten Lernprozessen erhoben werden. \needcitation

Die Plattform richtet sich insbesondere an Studierende der Erziehungswissenschaften und des Lehramts. Sie bietet die Möglichkeit, pädagogisch-professionelle Handlungssituationen in einem handlungsentlastenden Kontext in virtuellen Umgebungen zu erproben. Dabei werden komplexe, nicht planbare Interaktionen simuliert, in denen professionelles Wissen auf spielerische Weise evaluiert werden kann. Ein besonderer Fokus liegt auf der Simulation komplexer pädagogischer Interaktionszusammenhänge, die in traditionellen Lernumgebungen nur schwer darstellbar wären.

Technisch basiert das Va.Si.Li-Lab auf Unity und nutzt das Ubiq-Framework für die Netzwerkkommunikation und Synchronisation zwischen mehreren VR-Clients. Die Plattform unterstützt verschiedene Endgeräte und ermöglicht sowohl lokale als auch Remote-Teilnahme über internetfähige Geräte. Zur Förderung der Barrierefreiheit werden Informationen multimodal bereitgestellt, einschließlich Untertiteln, Audiospuren und vereinfachter Sprache.

Für die vorliegende Arbeit bietet das Va.Si.Li-Lab eine ideale Zielumgebung, da es eine stabile VR-Infrastruktur mit Mehrbenutzerfähigkeit bereitstellt und gleichzeitig durch seine modulare Architektur die Integration neuer Funktionalitäten wie Echtzeit-3D-Rekonstruktion ermöglicht. Die Integration von RTReconstruct erfolgt als eigenständiges Modul, das die vorhandenen Tracking- und Visualisierungskomponenten nutzt, ohne die bestehende Szenenlogik grundlegend zu verändern.