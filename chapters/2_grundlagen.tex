\chapter{Grundlagen}

Dieses Kapitel bildet die technische und theoretische Grundlage der Arbeit. Es erläutert die zentralen Konzepte und Technologien, auf denen das entwickelte System aufbaut, und vermittelt das notwendige Verständnis für die folgenden Kapitel. 
Der Schwerpunkt liegt auf der Darstellung der Prinzipien von Virtual Reality (VR) und Extended Reality (XR), der 3D-Rekonstruktion aus monokularen Bildsequenzen, grundlegenden Systemtechnologien wie Containerisierung und WebSocket-Kommunikation sowie der Beschreibung der Zielumgebung, in die das System integriert wird.

\section{Virtuelle Realität und Extended Reality}

Virtuelle Realität (VR) bezeichnet die durch Computer generierte, interaktive Darstellung einer dreidimensionalen Umgebung, in der sich Nutzer immersiv bewegen und mit virtuellen Objekten interagieren können.\needcitation{}
Durch Head-Mounted Displays (HMDs) entsteht ein stereoskopisches Wahrnehmungserlebnis, das Tiefenwahrnehmung und Bewegungskohärenz vermittelt.

Unter dem Sammelbegriff \textit{Extended Reality (XR)} werden verschiedene Formen immersiver Technologien zusammengefasst, darunter Virtual Reality (VR), Augmented Reality (AR) und Mixed Reality (MR). Sie kombinieren reale und virtuelle Elemente zu einem gemeinsamen Wahrnehmungskontext.
Moderne HMDs ermöglichen dies durch eine Passthrough-Funktion, bei der die Bilddaten einer externen oder integrierten Kamera in Echtzeit mit virtuellen Objekten überlagert werden.
Besonders in Mixed-Reality-Szenarien ist eine präzise Erfassung der physischen Umgebung erforderlich, da virtuelle Objekte nur dann glaubwürdig interagieren, wenn die Geometrie der realen Szene exakt rekonstruiert wurde.

\section{3D-Rekonstruktion}

Unter 3D-Rekonstruktion versteht man die algorithmische Erzeugung einer dreidimensionalen Repräsentation einer realen Szene aus zweidimensionalen Bilddaten.
Ziel ist es, Geometrie, Struktur und Textur so wiederzugeben, dass sie der realen Umgebung möglichst genau entsprechen.\needcitation{}
Ergänzende Informationen wie \textit{Kameraintrinsiken} (Brennweite, Hauptpunkt) und \textit{Kameraextrinsiken} (Position, Orientierung) erhöhen die Rekonstruktionsgenauigkeit erheblich.
Je nach genutzter Datenbasis und Algorithmus unterscheiden sich Verfahren hinsichtlich Genauigkeit, Laufzeit und Echtzeitfähigkeit.

\subsection{Photogrammetrie}

Die Photogrammetrie kombiniert Methoden der \textit{Structure from Motion (SfM)} und \textit{Multi-View Stereo (MVS)}, um aus mehreren überlappenden Bildern die räumliche Struktur einer Szene zu rekonstruieren.
Charakteristische Punkte werden detektiert, zwischen Bildern korreliert und durch \textit{Triangulation} in 3D-Koordinaten überführt.
Anschließend erfolgt eine dichte Rekonstruktion über Korrespondenzsuche und Oberflächeninterpolation.
Photogrammetrische Verfahren liefern sehr detailreiche Modelle, sind jedoch aufgrund der rechenintensiven Optimierung (z.\,B.\ Bundle Adjustment) meist für \textit{Offline-Verarbeitung} konzipiert.
Ihre Nutzung in Echtzeitsystemen ist daher nur eingeschränkt möglich.

\subsection{SLAM-Verfahren}

\textit{Simultaneous Localization and Mapping (SLAM)} beschreibt den Prozess, bei dem ein System seine eigene Pose schätzt und gleichzeitig eine Karte der Umgebung aufbaut.
Während die Lokalisierung durch kontinuierliche Pose-Schätzung erfolgt, werden gleichzeitig neue Bildinformationen in eine globale Karte integriert.
SLAM-Ansätze können visuell (nur RGB), visuell-inertial (RGB + IMU-Sensoren) oder tiefenbasiert (RGB-D) arbeiten.
Sie sind für den Echtzeitbetrieb optimiert und werden daher häufig in Robotik-, AR- und VR-Anwendungen eingesetzt.
Ihre Modelle sind in der Regel weniger detailliert als photogrammetrische, bieten jedoch eine hohe Latenzstabilität und eignen sich für dynamische Szenen.

\subsection{Neuronale Rekonstruktion}

Neuronale Verfahren erweitern klassische Rekonstruktionsansätze um tief lernende Komponenten, die geometrische und semantische Informationen direkt aus Bildfolgen extrahieren.

\textbf{Volumetrische Verfahren} wie \textit{NeuralRecon} oder \textit{VisFusion} verwenden rekurrente 3D-Convolutional-Netze, die lokale Tiefenschätzungen über mehrere Frames hinweg in einem sogenannten \textit{Truncated Signed Distance Field (TSDF)} fusionieren.
Ein TSDF repräsentiert die Szene als voxelbasiertes 3D-Gitter, in dem jeder Voxel den signierten Abstand zur nächstgelegenen Oberfläche speichert.
Positive Werte liegen vor der Oberfläche, negative dahinter.
Durch die Trunkierung auf einen festen Abstand wird die Repräsentation speicher- und berechnungseffizient, während gleichzeitig eine glatte Oberflächenapproximation ermöglicht wird.

\textbf{Strahlbasierte Verfahren}, inspiriert von \textit{Neural Radiance Fields (NeRFs)}, modellieren die Szene als kontinuierliche Funktion entlang von Lichtstrahlen.
Für jeden Strahl werden Dichte und Farbe geschätzt, um volumetrische Rendering-Effekte zu erzeugen.
Moderne Ansätze wie \textit{SLAM3R} koppeln diese Darstellung mit SLAM-Mechanismen, um während der Kamerabewegung eine stabile Pose-Schätzung zu gewährleisten.

\subsection{Qualitätskriterien der 3D-Rekonstruktion}

Die Bewertung von 3D-Rekonstruktionen erfolgt anhand quantitativer und qualitativer Kriterien.
Für Systeme mit Echtzeitanforderungen stehen insbesondere \textit{Genauigkeit}, \textit{Vollständigkeit}, \textit{Latenz} und \textit{Robustheit} im Mittelpunkt.
Tabelle~\ref{tab:qualitaetskriterien} fasst diese Kriterien und ihre Bedeutung im Kontext von Echtzeit-VR-Anwendungen zusammen.

\begin{table}[H]
    \centering
    \caption{Qualitätskriterien der 3D-Rekonstruktion im Kontext von Echtzeit-VR}
    \label{tab:qualitaetskriterien}
    \begin{tabular}{p{3cm}p{5.5cm}p{5.5cm}}
        \toprule
        \textbf{Kriterium} & \textbf{Beschreibung} & \textbf{Bedeutung für Echtzeit-VR} \\
        \midrule
        Genauigkeit & Grad der Übereinstimmung zwischen rekonstruierter und realer Geometrie & Realistische Maßstäblichkeit und stabile Interaktion mit der virtuellen Umgebung. \\
        Vollständigkeit & Anteil der Szene, der erfolgreich rekonstruiert wurde & Beeinflusst die visuelle Plausibilität und Immersion; Lücken oder Artefakte stören das Raumgefühl. \\
        Latenz & Zeitspanne zwischen Datenerfassung und Anzeige des aktualisierten Modells & Wesentlich für flüssige Nutzerinteraktion und Synchronität zwischen realer Bewegung und virtueller Rückmeldung. \\
        Robustheit & Widerstandsfähigkeit gegenüber Störeinflüssen wie Beleuchtungsänderungen oder Bewegungsunschärfe & Bestimmt die Stabilität und Nutzbarkeit des Systems unter realen Bedingungen. \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Systemische Grundlagen}

\subsection{Container-Technologie und Docker}

Container-Technologien ermöglichen die Ausführung von Anwendungen in isolierten Laufzeitumgebungen, die alle benötigten Abhängigkeiten beinhalten.
Dadurch wird sichergestellt, dass Software unabhängig von der Host-Umgebung konsistent funktioniert.\needcitation{}
Im Gegensatz zu virtuellen Maschinen teilen sich Container den Kernel des Host-Systems und sind daher deutlich ressourcenschonender.

\textit{Docker} ist die verbreitetste Implementierung dieses Ansatzes.
Es abstrahiert die Erstellung und Verwaltung von Containern über eine einheitliche Befehls- und Laufzeitumgebung.
Anwendungen werden über sogenannte \texttt{Dockerfiles} beschrieben, die Basis-Images, Abhängigkeiten und Startbefehle definieren.
Für mehrteilige Systeme bietet Docker mit \texttt{Docker Compose} ein Werkzeug zur Orchestrierung mehrerer Container über eine zentrale \texttt{docker-compose.yml}-Datei.
Dort lassen sich Dienste, Netzwerkkonfiguration, gemeinsam genutzte Volumes und Umgebungsvariablen festlegen, wodurch sich komplexe Softwarestapel konsistent starten und skalieren lassen.

Containerisierung bietet sich besonders für modulare Forschungs- und Prototyping-Systeme an, bei denen unterschiedliche Frameworks (z.\,B.\ PyTorch-Versionen) oder GPU-Abhängigkeiten parallel betrieben werden müssen.

\subsection{WebSockets}

WebSockets sind ein Netzwerkprotokoll, das eine bidirektionale, persistente Verbindung zwischen Client und Server ermöglicht.
Nach einem initialen HTTP-Handshake wird die Verbindung auf das WebSocket-Protokoll umgestellt, sodass beide Seiten asynchron Daten austauschen können.\needcitation{}
Im Gegensatz zu REST- oder gRPC-Schnittstellen, die auf Anfrage-Antwort-Zyklen basieren, erlaubt WebSocket-Kommunikation den kontinuierlichen Austausch von Nachrichten bei minimaler Latenz. \needcitation
Damit eignet sich das Protokoll besonders für Streaming- und Echtzeitanwendungen, bei denen Datenströme fortlaufend verarbeitet werden müssen – beispielsweise bei der Übertragung von Sensordaten in VR- oder Robotik-Systemen.

\section{Zielumgebung}

\subsection{Unity als VR-Plattform}

Unity ist eine weit verbreitete Entwicklungsumgebung für interaktive 3D- und Virtual-Reality-Anwendungen.
Die Engine basiert auf einer komponentenorientierten Architektur: Jedes Objekt wird durch ein \textit{GameObject} beschrieben, das durch \textit{Components} um Funktionen wie Physik, Rendering oder Benutzereingaben erweitert werden kann.

Für VR-Anwendungen stellt Unity spezialisierte XR-Subsysteme und Plug-ins bereit, die Head-Tracking, stereoskopisches Rendering und die Integration gängiger HMDs (z.\,B.\ Meta Quest, HTC Vive, Valve Index) ermöglichen.
Über die C\#-Scripting-API lassen sich Interaktionslogik, Benutzeroberflächen und Netzwerkkommunikation flexibel implementieren.
Dank Unterstützung moderner Grafik-APIs (OpenGL, Vulkan, DirectX) und plattformübergreifender Exportmöglichkeiten ist Unity sowohl für mobile als auch für hochauflösende VR-Anwendungen geeignet.

\subsection{Das Va.Si.Li-Lab}

Das \textit{Va.Si.Li-Lab} (Virtual and Simulation-Based Learning Laboratory) ist eine virtuelle Forschungs- und Lernumgebung des Text Technology Lab der Goethe-Universität Frankfurt.\needcitation
Es dient als Plattform zur Untersuchung simulationsbasierter Lernszenarien in immersiven, mehrbenutzerfähigen Virtual-Reality-Umgebungen.

Technisch basiert das Lab auf Unity und dem Mehrbenutzer-Framework \textit{Ubiq}, das synchrone Interaktionen mehrerer Teilnehmender in gemeinsamen VR-Szenen ermöglicht. \needcitation
Ergänzend werden multimodale Schnittstellen zur Erfassung von Sprach-, Gesten- und Blickdaten eingesetzt, die in einer zentralen Datenbank gespeichert und für Analysen bereitgestellt werden.

Für diese Arbeit ist das Va.Si.Li-Lab besonders relevant, da es eine modulare, erweiterbare Infrastruktur bietet, jedoch bislang keine Möglichkeit zur dynamischen 3D-Rekonstruktion realer Umgebungen.
Die aktuell genutzten Szenen müssen manuell modelliert oder aus externen 3D-Ressourcen geladen werden, was die Übertragbarkeit realer Lern- oder Forschungssituationen einschränkt.
