\chapter{Evaluation}
\label{chap:evaluation} % Hinzugefügt, falls andere Kapitel darauf verweisen

Dieses Kapitel evaluiert das entwickelte RTReconstruct-System hinsichtlich seiner 
Echtzeitfähigkeit, Modularität und Rekonstruktionsqualität. Zunächst werden die 
Evaluationsziele definiert und die Testumgebung beschrieben. Anschließend erfolgt 
die Darstellung der gemessenen Performance-Metriken sowie der Rekonstruktionsqualität 
der integrierten Modelle. Das Kapitel schließt mit einer Diskussion der Ergebnisse 
im Kontext der definierten Anforderungen.

\section{Evaluationsziele}
\label{sec:eval_ziele}

Die Evaluation verfolgt drei zentrale Ziele, die direkt aus der in Abschnitt formulierten Forschungsfrage abgeleitet sind:

\begin{enumerate}
    \item \textbf{Funktionale Validierung}: Nachweis, dass die modulare Architektur 
    die definierten funktionalen Anforderungen erfüllt. Insbesondere wird geprüft, 
    ob verschiedene Rekonstruktionsmodelle parallel betrieben werden können und ob 
    die End-to-End Kommunikation zwischen VR-Frontend und Backend stabil funktioniert.
    
    \item \textbf{Echtzeitfähigkeit}: Bewertung, ob das System die für VR-Anwendungen 
    erforderlichen Performance-Anforderungen erfüllt. Dabei werden Latenz, Durchsatz 
    und Ressourcenauslastung als kritische Metriken untersucht.
    
    \item \textbf{Rekonstruktionsqualität}: Qualitative und -- soweit möglich -- 
    quantitative Bewertung der von den integrierten Modellen erzeugten 
    3D-Rekonstruktionen unter identischen Bedingungen.
\end{enumerate}

\section{Evaluationsmethodik}
Die Evaluationsmethodik beschreibt die Testumgebung, die verwendeten Testszenarien und die Messmethoden, die zur Erreichung der Evaluationsziele eingesetzt wurden. 

\subsection{Test- und Evaluationsumgebung}
\label{sec:testumgebung}

Um die \textbf{Reproduzierbarkeit} der Performance-Messungen und die \textbf{Vergleichbarkeit} der erzielten Ergebnisse zu gewährleisten, wurde die gesamte Evaluation in einer dedizierten und \textbf{kontrollierten Hard- und Software-Umgebung} durchgeführt. Die zentralen Komponenten und Spezifikationen dieser Umgebung sind in Tabelle~\ref{tab:hardware_and_software} zusammengefasst.

\begin{table}[H]
    \centering
    \label{tab:hardware_and_software}
    \caption{Spezifikationen der Hard- und Software-Umgebung}
    \begin{tabularx}{\textwidth}{l X}
        \toprule
        \textbf{Kategorie} & \textbf{Details und Spezifikationen} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Hardware-Umgebung (Backend/Server)}} \\
        \midrule
        Backend-Server CPU & AMD Ryzen 9 5900X (12 Kerne, 24 Threads) \\
        GPU & NVIDIA GeForce GTX 1070 Ti, 8 GB VRAM \\
        VR-System (Frontend) & Meta Quest 3 \\
        Netzwerk & WiFi 6 Heimnetzwerk \\
        \midrule
        \multicolumn{2}{l}{\textbf{Software-Umgebung und Frameworks}} \\
        \midrule
        Betriebssystem & Ubuntu 22.04 LTS (Host) \\
        Containerisierung & Docker (\textit{[Version einfügen, z.B. 24.0.7]}) \\
        GPU-Unterstützung & NVIDIA Container Toolkit \\
        VR-Frontend & Unity (\textit{2022.3 LTS}) \\
        \bottomrule
    \end{tabularx}
\end{table}

Alle Messungen erfolgten unter \textbf{kontrollierten Bedingungen}. Es wurde strikt darauf geachtet, dass während der Performance-Tests \textbf{keine weiteren rechenintensiven Hintergrundprozesse} liefen, um Verzerrungen zu minimieren.

\subsection{Testszenarien und Datensätze}

Für die Evaluation wurden fünf Testszenen mit unterschiedlichen Komplexitätsstufen 
konzipiert: drei virtuelle Szenen mit verfügbarem Ground-Truth zur quantitativen 
Bewertung sowie zwei reale Szenen aus einem typischen Alltagsumfeld zur Validierung der
Praxistauglichkeit. Eine kurze Übersicht über alle Testszenen findet sich in Tabelle~\ref{tab:test_scenes_overview}.
\begin{table}[H]
    \centering
    \label{tab:test_scenes_overview}
    \begin{tabularx}{\textwidth}{lcclX}
        \toprule
        \textbf{Szene} & \textbf{Größe (m)} & \textbf{Fragments} & \textbf{Frames} & \textbf{Merkmale} \\
        \midrule
        \multicolumn{5}{l}{\textbf{Virtuelle Szenen}} \\
        V1 -- Primitive & $5\times 5\times 5$ & 24 & 216 & Geometrische Grundformen, unifarbige Oberflächen \\
        V2 -- Schlafzimmer & $6\times 5\times 3$ & 44 & 396 & moderate Komplexität, Okklusionen \\
        V3 -- Mehrzweckraum & $10\times 5\times 3$ & 58 & 522 & Hohe Dichte, komplexe Geometrie \\
        \midrule
        \multicolumn{5}{l}{\textbf{Reale Szenen}} \\
        R1 & $6\times 4\times 3$ & 48 & 432 & \textit{Schlafzimmer}, Details, Schrägen, Okklusionen \\
        R2 & $7.5\times 5\times 3$ & 43 & 387 & \textit{Wohnzimmer}, Glas, Reflexionen, große Flächen \\
        \bottomrule
    \end{tabularx}
    \medskip
    \caption{Übersicht und Klassifikation der Testszenen}
\end{table}

\subsubsection{Virtuelle Szenen}

Die drei virtuellen Szenen wurden in Unity erstellt und ermöglichen durch verfügbare \textit{Ground-Truth-Meshes} eine quantitative Evaluation mittels F-Score. Die Szenen folgen einer progressiven Komplexitätssteigerung, um verschiedene Aspekte der Rekonstruktionsverfahren isoliert zu testen. Abbildung~\ref{fig:virtual_scenes} zeigt eine Übersicht aller drei Szenen.

\paragraph{Szene V1 -- Geometrische Primitive}
Szene V1 dient als Baseline-Test und enthält ausschließlich einfache geometrische Primitive (Quader, Pyramide, Zylinder, Kapsel) in einem hexagonalen Raum mit farbigen Wänden. Die unifarbigen, matten Oberflächen ohne Texturen ermöglichen die isolierte Bewertung fundamentaler Rekonstruktionsfähigkeiten: scharfe Kanten, gekrümmte Oberflächen und feature-arme Flächen.

\paragraph{Szene V2 -- Möbliertes Schlafzimmer}
Szene V2 repräsentiert einen möblierten Innenraum mittlerer Komplexität mit Doppelbett, Sessel, Sideboard, Wandbildern und Stehlampe. Diese Szene testet die Rekonstruktion komplexer Möbelgeometrie, das Verhalten bei Okklusionen, die Texturverarbeitung sowie die Detailerfassung kleiner Dekorationsobjekte.

\paragraph{Szene V3 -- Komplexer Mehrzweckraum}
Szene V3 stellt einen Stresstest für Skalierbarkeit und Detailtreue dar und simuliert einen multifunktionalen Raum mit Schlaf-, Wohn- und Arbeitsbereich. Die hohe Objektdichte mit zwei Betten, Esstisch, Stühlen und diversen Kleinobjekten erzeugt multiple Okklusionsebenen. Erwartet werden längere Inferenzzeiten, höhere GPU-Auslastung und potenzielle Artefakte bei geometrisch komplexen Strukturen und teilweise verdeckten Bereichen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\textwidth]{images/room00.png}
    \includegraphics[width=0.32\textwidth]{images/room01.png}
    \includegraphics[width=0.32\textwidth]{images/Room02.png}
    \caption{Übersicht der drei virtuellen Testszenen: V1 (Geometrische Primitive), V2 (Möbliertes Schlafzimmer), V3 (Komplexer Mehrzweckraum)}
    \label{fig:virtual_scenes}
\end{figure}

\subsubsection{Reale Szenen}

Die beiden realen Szenen wurden im Va.Si.Li-Lab aufgenommen und validieren die Praxistauglichkeit des Systems unter realen Bedingungen mit natürlichen Störfaktoren.

\paragraph{Szene R1 -- Schlafzimmer}

Szene R1 simuliert ein kleines, dicht möbliertes, privates Umfeld. Details, Schrägen, Okklusionen. Der Testfokus liegt auf der Robustheit gegenüber Textiloberflächen und diffuser Beleuchtung, welche die Rekonstruktion feiner Details und das Verhalten bei Oberflächenhomogenität überprüfen.

\paragraph{Szene R2 -- Wohnzimmer}

Szene R2 simuliert ein großes Wohnzimmer mit offener Gestaltung. Große Flächen, Glas, Reflexionen. Die Szene dient als Skalierbarkeits- und Materialstresstest. Im Fokus stehen die Handhabung großer, glänzender Flächen und Fenster, die Reflexionen verursachen, sowie repetitive Dekorelemente, welche die globale Konsistenz und Anfälligkeit für visuellen Drift testen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{images/room03.jpg}
    \includegraphics[width=0.49\textwidth]{images/room04.jpg}
    \caption{Übersicht der beiden realen Testszenen: R1 (Schlafzimmer), R2 (Wohnzimmer)}
    \label{fig:real_scenes}
\end{figure}

\subsection{Messverfahren und Metriken}

\subsubsection{Performance-Metriken}

\paragraph{Latenz}
Die End-to-End-Latenz misst die Zeitspanne zwischen dem Versenden eines Fragments 
durch das Unity-Frontend und der Visualisierung der aktualisierten Rekonstruktion 
im VR-Headset. Sie setzt sich aus folgenden Komponenten zusammen:
\begin{align}
    L_{\text{total}} = L_{\text{network}} + L_{\text{inference}} + L_{\text{render}}
\end{align}
wobei $L_{\text{network}}$ die Netzwerklatenz (Upload des Fragments und Download 
der Rekonstruktion), $L_{\text{inference}}$ die Modell-Inferenzzeit im Backend 
(GPU-Verarbeitung) und $L_{\text{render}}$ die Rendering-Zeit im Unity-Client 
(GLB-Import und Mesh-Visualisierung) bezeichnet.


Die Netzwerklatenz $L_{\text{network}} = L_{\text{upload}} + L_{\text{download}}$ 
wird nicht direkt durch Zeitstempel gemessen, sondern aus den erfassten Datenvolumina 
und der verfügbaren Netzwerkbandbreite berechnet: $L_{\text{upload}} = S_{\text{fragment}} / B_{\text{upload}}$ 
bzw. $L_{\text{download}} = S_{\text{result}} / B_{\text{download}}$. Hierbei 
bezeichnet $S_{\text{fragment}}$ die Fragmentgröße (Upload-Volumen pro Fragment) 
und $S_{\text{result}}$ die Resultgröße (Download-Volumen der Rekonstruktion). 
Diese Methodik ermöglicht eine infrastrukturunabhängige Bewertung der Dateneffizienz.


Die Messung der übrigen Latenzkomponenten erfolgte durch präzise Zeitstempel an 
den jeweiligen Übergangspunkten der Pipeline.

\paragraph{Durchsatz}
Der Durchsatz quantifiziert, wie viele Fragmente pro Sekunde durch das System 
verarbeitet werden können. Ein höherer Durchsatz ermöglicht häufigere Updates der 
Rekonstruktion und trägt zur Immersion bei. Gemessen wurde der Durchsatz auf 
Backend-Seite für jedes Worker-Modell separat.

\paragraph{Ressourcenauslastung}
Die GPU- und CPU-Auslastung wurde kontinuierlich während der Rekonstruktion 
aufgezeichnet. GPU-Utilization und GPU-Memory wurden via \texttt{nvidia-smi} 
erfasst, CPU-Auslastung und RAM-Verbrauch pro Container via Docker Stats. Diese 
Metriken ermöglichen die Bewertung der Ressourceneffizienz und geben Aufschluss 
über Engpässe im System.

\subsubsection{Qualitätsmetriken}

\paragraph{Quantitative Bewertung}
Für Szenen mit verfügbarem Ground-Truth-Mesh wurde der F-Score als kombinierte 
Metrik für Präzision und Recall berechnet:

\begin{align}
    \text{Precision} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FP}|} \\
    \text{Recall} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FN}|} \\
    \text{F-Score} &= 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

\noindent
Rekonstruierte Punkte gelten als \textbf{True Positive (TP)}, wenn ihr Abstand zum 
Ground-Truth unter \textit{10} cm liegt, andernfalls als \textbf{False Positive (FP)}. 
\textbf{False Negatives (FN)} sind Ground-Truth-Punkte ohne entsprechenden 
rekonstruierten Punkt innerhalb des Schwellenwerts.


\textbf{Precision} misst die Genauigkeit der Rekonstruktion, indem sie den Anteil 
korrekt rekonstruierter Punkte angibt. \textbf{Recall} bewertet die 
Vollständigkeit und gibt an, wie viele Ground-Truth-Punkte erfasst wurden. 
Der \textbf{F-Score} kombiniert beide Metriken als harmonisches Mittel und liefert einen 
ausgewogenen Gesamtwert. Je näher der F-Score bei \textit{1.0} liegt, 
desto höher ist die Qualität der Rekonstruktion.

\paragraph{Qualitative Bewertung}
Die rekonstruierten Meshes wurden anhand folgender Kriterien bewertet:

\begin{itemize}
    \item \textbf{Vollständigkeit}: Wie viel Prozent der Szene wurde erfasst?
    \item \textbf{Detailtreue}: Sind feine Strukturen erkennbar?
    \item \textbf{Artefaktfreiheit}: Treten Löcher, Flimmern oder Fehlgeometrie auf?
    \item \textbf{Oberflächenqualität}: Glattheit und Konsistenz der Rekonstruktion
\end{itemize}

\noindent
Die Bewertung erfolgte durch visuelle Inspektion der Rekonstruktionen in Unity 
sowie durch exportierte Screenshots.

\section{Ergebnisse}
\label{sec:ergebnisse}

\subsection{Funktionale Validierung}

Die funktionale Validierung bestätigt, dass RTReconstruct alle definierten Kernfunktionalitäten erfüllt.

\paragraph{End-to-End-Kommunikation}
Die vollständige Kommunikationskette von der Fragmenterfassung im Unity-Client über 
die WebSocket-Verbindung zum Router bis zur Verteilung an die Worker-Container und 
zurück funktioniert stabil. In \textit{15} Testläufen über eine Gesamtdauer von \textit{6} Stunden 
traten \textit{0} Verbindungsabbrüche auf.

\paragraph{Parallele Modellausführung}
Alle vier integrierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, 
SLAM3R) konnten gleichzeitig betrieben werden. Die containerisierte Architektur 
ermöglichte eine vollständige Isolierung, sodass unterschiedliche Python- und 
PyTorch-Versionen parallel lauffähig waren. Einzig limitierender Faktor war die einzelne GPU,
die durch die Modelle gemeinsam genutzt wurde.

\paragraph{Multi-Szenen-Unterstützung}
Das System unterstützt die gleichzeitige Verarbeitung mehrerer Szenen. In Tests 
mit \textit{2} parallelen Szenen und \textit{4} verbundenen Clients blieb die Funktionalität 
erhalten. Die szenenspezifische Zuordnung der Rekonstruktionsergebnisse erfolgte 
fehlerfrei.

\paragraph{Visualisierung in VR}
Die über das Backend empfangenen Meshes wurden erfolgreich im Unity-Client 
visualisiert. Das in Kapitel 5 beschriebene Spatial 
Hashing ermöglichte eine performante Darstellung auch bei größeren Meshes und Punktwolken mit bis zu 100.000 Punkten.

\subsection{Performance-Analyse}

\subsubsection{Latenz}

Die Latenz stellt die zentrale Performance-Metrik für die Echtzeitfähigkeit des Systems dar. Im Folgenden wird zunächst die Gesamtlatenz über alle Testszenen und Modelle präsentiert, anschließend in ihre Komponenten zerlegt und abschließend durch die Analyse der Datenvolumina kontextualisiert.

\paragraph{Gesamtlatenz}

Zur Evaluierung der Systemperformance wurde die End-to-End-Latenz \\ \(L_{total}\) als Zeitspanne zwischen dem Absenden eines Fragments vom Client und dem Empfang der zugehörigen Rekonstruktion gemessen. Für jede Kombination aus Testszene und Rekonstruktionsmodell wurden drei unabhängige Testläufe durchgeführt, bei denen identische Eingabedaten verwendet wurden. Um dabei Verzerrungen durch Ressourcenkonflikte zu vermeiden, wurden die Modelle sequenziell im Einzelbetrieb getestet. Abbildung~\ref{fig:latency_boxplots} visualisiert die resultierenden Latenzverteilungen als Boxplots.


Die Darstellung zeigt auf der x-Achse die vier evaluierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, SLAM3R), während die y-Achse die gemessene Latenz in Millisekunden angibt. Pro Modell sind drei Boxplots dargestellt, die jeweils die Latenzverteilung eines Testlaufs repräsentieren. Die farbliche Kodierung kennzeichnet dabei denselben Testlauf über alle Modelle hinweg.

\begin{figure}[H]
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room00_latency.png}
        \caption{Szene V1 -- Geometrische Primitive}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room01_latency.png}
        \caption{Szene V2 -- Möbliertes Schlafzimmer}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room02_latency.png}
        \caption{Szene V3 -- Komplexer Mehrzweckraum}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room03_latency.png}
        \caption{Szene R1 -- Schlafzimmer}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.49\linewidth]{images/room04_latency.png}
        \caption{Szene R2 -- Wohnzimmer}
    \end{subfigure}
    \caption{Gesamtlatenz \(L_{total}\) für alle Testszenen und Modelle über drei Testläufe. Die Boxen zeigen den Interquartilbereich (25.\,--\,75.\,Perzentil), die horizontale Linie den Median und die Whiskers den Wertebereich ohne Ausreißer.}
    \label{fig:latency_boxplots}
\end{figure}

\newpage
\noindent
Die Boxplots zeigen für alle Modelle und Szenen geringe Interquartilbereiche und minimale Ausreißer, was auf eine stabile und reproduzierbare Latenzcharakteristik des Systems hinweist. Aufgrund dieser geringen Varianz zwischen den drei Testläufen werden in den folgenden Analysen zur Zusammensetzung der Gesamtlatenz sowie zu Datenvolumina die Messwerte der drei Testläufe aggregiert dargestellt. Dies ermöglicht eine kompaktere Präsentation ohne relevanten Informationsverlust.


Die Abbildung zeigt deutliche Unterschiede in der Gesamtlatenz zwischen den Testszenen: Szene V1 weist die niedrigsten Latenzwerte auf, während die Latenz in den komplexeren Szenen V3, R1 und R2 ansteigt. Zudem variiert die Latenz zwischen den Modellen, wobei MAST3R durchgängig die höchsten Werte erreicht. Um die Ursachen dieser Variation zu identifizieren, wird die Gesamtlatenz im Folgenden in ihre Komponenten zerlegt.


\paragraph{Zusammensetzung der Gesamtlatenz}

Die beobachteten Latenzunterschiede zwischen den Testszenen lassen sich durch die Zerlegung der Gesamtlatenz in ihre konstituierenden Komponenten \(L_{\text{network}}\), \(L_{\text{inference}}\) und \(L_{\text{render}}\) analysieren. Diese Aufschlüsselung ermöglicht es, szenenabhängige Effekte auf die Inferenzzeit von fixen Overhead-Kosten der Netzwerkkommunikation und Rendering-Pipeline zu separieren. Abbildung~\ref{fig:latency_stacked_bar} visualisiert die resultierende Zusammensetzung für alle Modelle und Testszenen.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/latency_split.png}
    \caption{Zusammensetzung der Gesamtlatenz nach Komponenten für alle Rekonstruktionsmodelle, aufgeschlüsselt nach Testszene und gemittelt über die drei Testläufe.}
    \label{fig:latency_stacked_bar}
\end{figure}


Die Aufschlüsselung zeigt, dass \(L_{\text{inference}}\) den dominierenden Anteil der Gesamtlatenz ausmacht und zwischen den Szenen stark variiert. Der Anteil von \(L_{\text{network}}\) und \(L_{\text{render}}\) bleibt über die Szenen hinweg relativ konstant, nimmt jedoch prozentual mit steigender Szenenkomplexität ab. Die Netzwerklatenz \(L_{\text{network}}\) wird dabei maßgeblich durch die Größe der übertragenen Daten bestimmt, deren Quantifizierung im Folgenden dargestellt wird.


\paragraph{Fragment- und Ergebnisgrößen}

Um die Netzwerklatenz \(L_{\text{network}}\) zu kontextualisieren und die Bandbreitenanforderungen des Systems zu dokumentieren, wurden die durchschnittlichen Fragmentgrößen (Upload) und Ergebnisgrößen (Download) für alle Modelle gemessen. Tabelle~\ref{tab:data_volumes} zeigt die resultierenden Datenvolumina sowie die daraus berechnete genutzte Bandbreite.

\begin{table}[h]
    \centering
    \caption{Durchschnittliche Datenvolumina und genutzte Bandbreite aufgeschlüsselt nach Szene und Modell (gemittelt über drei Testläufe).}
    \label{tab:data_volumes}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|c|c|c|c}
        \toprule
        \textbf{Szene} & \textbf{Modell} & \makecell{\textbf{\O~$\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{\O~$\text{Frag}_{\text{out}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{out}}$~[MB]}} \\
        \midrule
        \multirow{4}{*}{V1} 
            & NeuralRecon  & \multirow{4}{*}{0.82} & 1.83 & \multirow{4}{*}{19.57} & 44.04 \\
            & VisFusion    &                       & 2.73 &                        & 65.40 \\
            & MASt3R-SLAM  &                       & 1.60 &                        & 38.42 \\
            & SLAM3R       &                       & 1.60 &                        & 38.42 \\
        \midrule
        \multirow{4}{*}{V2} 
            & NeuralRecon  & \multirow{4}{*}{1.62} & 2.84 & \multirow{4}{*}{69.57} & 122.28 \\
            & VisFusion    &                      & 3.25  &                        & 139.85 \\
            & MASt3R-SLAM  &                      & 1.60  &                        & 68.60 \\
            & SLAM3R       &                      & 1.60  &                        & 68.84 \\
        \midrule
        \multirow{4}{*}{V3} 
            & NeuralRecon  & \multirow{4}{*}{1.56} & 3.60 & \multirow{4}{*}{88.70} & 204.93 \\
            & VisFusion    &                      & 4.24  &                        & 241.51 \\
            & MASt3R-SLAM  &                      & 1.59  &                        & 90.69 \\
            & SLAM3R       &                      & 1.60  &                        & 91.26 \\
        \midrule
        \multirow{4}{*}{R1} 
            & NeuralRecon  & \multirow{4}{*}{1.63} & 2.19 & \multirow{4}{*}{78.50} & 105.29 \\
            & VisFusion    &                       & 2.58 &                       & 124.14 \\
            & MASt3R-SLAM  &                       & 1.60 &                       & 76.85 \\
            & SLAM3R       &                       & 1.60 &                       & 76.85 \\
        \midrule
        \multirow{4}{*}{R2} 
            & NeuralRecon  & \multirow{4}{*}{1.84} & 2.48 & \multirow{4}{*}{79.33} & 106.87 \\
            & VisFusion    &                       & 3.70 &                        & 159.19 \\
            & MASt3R-SLAM  &                       & 1.60 &                        & 68.84 \\
            & SLAM3R       &                       & 1.60 &                        & 68.84 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

Die Tabelle zeigt, dass die Fragmentgrößen zwischen den Modellen variieren, wobei SLAM3R aufgrund seiner größeren Fenstergrößen die umfangreichsten Fragmente benötigt. Die Ergebnisgrößen unterscheiden sich ebenfalls deutlich: Volumetrische Verfahren (NeuralRecon, VisFusion) erzeugen größere Meshes im GLB-Format, während punktbasierte Modelle (MASt3R-SLAM, SLAM3R) kompaktere Punktwolken zurückliefern. Die Summe der genutzten Bandbreite liegt bei X.X~Mbps für den Upload und Y.Y~Mbps für den Download.


Die präsentierten Latenzmessungen bilden zusammen mit den Datenvolumina die Grundlage für die Bewertung der Echtzeitfähigkeit und Skalierbarkeit des Systems in Abschnitt~\ref{sec:diskussion}.

\subsubsection{Durchsatz}

Der Durchsatz ergibt sich für diese Evaluation aus der Anzahl der verarbeiteten Fragmente pro Szene, geteilt durch die mittlere End-To-End Latenz \(L_{total}\) über alle 3 Testläufe. Diese Metrik gibt an, wie viele Fragmente pro Sekunde durch das System verarbeitet werden können und ist ein Indikator für die Aktualisierungsrate der Rekonstruktion im VR-Frontend. Die folgende Tabelle \ref{tab:throughput_results} fasst diese gemessenen Durchsatzwerte für alle Modelle und Szenen zusammen.

\begin{table}[h]
    \centering
    \label{tab:throughput_results}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Modell} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        NeuralRecon     & 0.62 & 0.45 & 0.32 &  0.48 & 0.45 \\
        VisFusion       & 0.38 & 0.35 & 0.30 &  0.38 & 0.31 \\
        MASt3R-SLAM     & 0.13 & 0.12 & 0.12 &  0.12 & 0.12 \\
        SLAM3R          & 0.07 & 0.06 & 0.03 &  0.10 & 0.09 \\
        \bottomrule
    \end{tabular}
    \bigskip
    \caption{Durchsatz nach Modell (Fragmente pro Sekunde)}
\end{table}

\subsubsection{Ressourcenauslastung}

Die Ressourcenauslastung wurde sowohl für das Backend (Server-seitige Rekonstruktion) 
als auch für das Frontend (VR-Client-seitige Visualisierung) getrennt erfasst. Diese 
Trennung ermöglicht die Identifikation von Engpässen in der Pipeline und gibt 
Aufschluss darüber, welche Systemkomponente limitierend wirkt.

\paragraph{Backend-Ressourcen}

Die Backend-Ressourcenauslastung wurde kontinuierlich während der Rekonstruktionsläufe 
auf dem dedizierten Server (AMD Ryzen 9 5900X, NVIDIA GTX 1070 Ti) erfasst. Die 
Messungen umfassen GPU-Auslastung und GPU-Speicherverbrauch (erfasst mittels 
\texttt{nvidia-smi} in 1-Sekunden-Intervallen) sowie CPU- und RAM-Nutzung der 
containerisierten Komponenten (erfasst mittels \texttt{docker stats}).


\textbf{GPU-Ressourcen:} Tabelle~\ref{tab:gpu_resources} zeigt die durchschnittliche 
GPU-Utilization und den maximalen GPU-Speicherverbrauch während der Rekonstruktion, 
aufgeschlüsselt nach Modell und Testszene.

\begin{table}[h]
    \centering
    \caption{GPU-Auslastung und GPU-Speicherverbrauch während der Rekonstruktion}
    \label{tab:gpu_resources}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Modell} & \textbf{Metrik} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        \multirow{2}{*}{NeuralRecon} 
            & \O GPU-Utilization [\%] & \textit{44} & \textit{43} & \textit{47} & \textit{47} & \textit{48} \\
            & Max. VRAM [MB]          & \textit{3825} & \textit{4770} & \textit{7993} & \textit{6368} & \textit{5986} \\
        \midrule
        \multirow{2}{*}{VisFusion} 
            & \O GPU-Utilization [\%] & \textit{49} & \textit{53} & \textit{55} & \textit{56} & \textit{52} \\
            & Max. VRAM [MB]          & \textit{4877} & \textit{4599} & \textit{4418} & \textit{4406} & \textit{5984} \\
        \midrule
        \multirow{2}{*}{MASt3R-SLAM} 
            & \O GPU-Utilization [\%] & \textit{100} & \textit{100} & \textit{100} & \textit{100} & \textit{100} \\
            & Max. VRAM [MB]          & \textit{7969} & \textit{8016} & \textit{7984} & \textit{8014} & \textit{7966} \\
        \midrule
        \multirow{2}{*}{SLAM3R} 
            & \O GPU-Utilization [\%] & \textit{98.5} & \textit{99} & \textit{99} & \textit{98} & \textit{99} \\
            & Max. VRAM [MB]          & \textit{8022} & \textit{7986} & \textit{8012} & \textit{7637} & \textit{7921} \\
        \bottomrule
    \end{tabular}
\end{table}


\textbf{CPU- und RAM-Auslastung:} Die Ressourcennutzung der einzelnen Docker-Container 
ist in Tabelle~\ref{tab:container_resources} dargestellt. Die Werte zeigen die 
durchschnittliche CPU-Last und den RAM-Verbrauch für Router- und Worker-Container, 
gemittelt über alle Testszenen.

\begin{table}[h]
    \centering
    \caption{Durchschnittliche CPU- und RAM-Auslastung der Backend-Container}
    \label{tab:container_resources}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Container} & \textbf{\O CPU-Auslastung [\%]} & \textbf{\O RAM-Verbrauch [MB]} \\
        \midrule
        Router                    & \textit{0.2} & \textit{200} \\
        \midrule
        Worker: NeuralRecon       & \textit{32} & \textit{2780} \\
        Worker: VisFusion         & \textit{17.25} & \textit{3015} \\
        Worker: MASt3R-SLAM       & \textit{14.35} & \textit{3466} \\
        Worker: SLAM3R            & \textit{15.37} & \textit{4070} \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Frontend-Ressourcen}

Die durchschnittliche Frame Rate des Unity-Clients wurde sowohl im Baseline-Betrieb 
(ohne aktive Rekonstruktion) als auch während der Rekonstruktions- und 
Visualisierungsphase gemessen. Tabelle~\ref{frame_rate} zeigt die Ergebnisse.

\begin{figure}[H]
    \centering
    \label{frame_rate}
    % \caption{FPS des Unity-Clients im Baseline-Betrieb und während der Rekonstruktion}
    \includegraphics[width=0.65\textwidth]{images/frontend_fps.png}
\end{figure}

\subsection{Rekonstruktionsqualität}
Die Rekonstruktionsqualität wird zunächst anhand der synthetischen Szenen V1--V3 mit 
Ground-Truth-Daten quantitativ und qualitativ bewertet. Anschließend erfolgt eine 
Untersuchung der Praxistauglichkeit unter realen VR-Bedingungen anhand der mit der 
Meta Quest 3 aufgenommenen Szenen R1 und R2.    

\subsubsection{Quantitative Bewertung}

Tabelle~\ref{tab:fscore_all} zeigt die F-Score-Ergebnisse für alle Szenen mit verfügbarem Ground-Truth für einen Schwellenwert von \textit{10}cm.

\begin{table}[H]
    \centering
    \label{tab:fscore_all}
    \caption{F-Score-Ergebnisse nach Modell und Szene}
    \begin{tabular}{lccc|ccc|ccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{V1}} & \multicolumn{3}{c}{\textbf{V2}} & \multicolumn{3}{c}{\textbf{V3}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        \textbf{Modell} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F-Score} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F-Score} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F-Score} \\
        \midrule
        NeuralRecon     & \textit{0.45} & \textit{0.39} & \textit{0.41} & \textit{0.69} & \textit{0.67} & \textit{0.68} & \textit{0.59} & \textit{0.59} & \textit{0.59} \\
        VisFusion       & \textit{0.57} & \textit{0.52} & \textit{0.54} & \textit{0.57} & \textit{0.65} & \textit{0.61} & \textit{0.57} & \textit{0.68} & \textit{0.62} \\
        SLAM3R          & \textit{0.66} & \textit{0.56} & \textit{0.61} & \textit{0.67} & \textit{0.59} & \textit{0.63} & \textit{0.41} & \textit{0.43} & \textit{0.42} \\
        MASt3R-SLAM     & \textit{0.52} & \textit{0.48} & \textit{0.50} & \textit{0.47} & \textit{0.53} & \textit{0.50} & \textit{0.43} & \textit{0.48} & \textit{0.45} \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
Die Berechnung erfolgte in CloudCompare mittels Cloud-to-Cloud Distance: Punkte mit 
Abstand \(\leq 10\)\,cm zum nächsten Ground-Truth-Punkt wurden als True Positives 
klassifiziert, größere Abstände als False Positives. False Negatives wurden durch 
umgekehrte Distanzberechnung ermittelt. Da SLAM3R und MASt3R-SLAM keine globale 
Konsistenz gewährleisten, wurden deren Rekonstruktionen zuvor mittels ICP ausgerichtet.

\subsubsection{Qualitative Bewertung}

\paragraph{NeuralRecon}

Die Rekonstruktionen von NeuralRecon zeigten deutliche Lücken über alle Testszenarien hinweg. Besonders in Szene V1 wurden schräge Flächen vollständig ausgelassen, während horizontale und vertikale Strukturen zumindest teilweise erfasst wurden. In den komplexeren Szenen V2 und V3 setzte sich diese Lückenhaftigkeit fort. Neben den fehlenden Bereichen fielen auch vereinfachte Geometrien auf - feine Details gingen verloren, während gröbere Strukturen noch erkennbar blieben. Positiv hervorzuheben ist hingegen die Qualität der tatsächlich erfassten Bereiche: Die Meshes wiesen glatte, geschlossene Oberflächen auf und waren frei von Fehlgeometrien oder Flimmerartefakten. Die globale Koherenz blieb durchgängig erhalten.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{Ground Truth}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/neucon00.png}
        \caption{NeuralRecon Rekonstruktion}
    \end{subfigure}
    \caption{Lückenhafte Rekonstruktion von Szene V1 durch NeuralRecon. Schräge Flächen werden nicht erfasst, während horizontale und vertikale Strukturen teilweise rekonstruiert werden.}
    \label{fig:neuralrecon_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/gt00.png}
        \caption{Ground Truth Detail}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/neucon00.png}
        \caption{NeuralRecon Detail}
    \end{subfigure}
    \caption{Detailverlust bei NeuralRecon. Objekte wie die Stehlampe oder der TV-Schrank in Szene V2 gehen in der Rekonstruktion größtenteils verloren.}
    \label{fig:neuralrecon_detail}
\end{figure}

\paragraph{VisFusion}

VisFusion überzeugte mit einer spürbar höheren Vollständigkeit als NeuralRecon. Schräge Flächen in Szene V1, die zuvor komplett fehlten, wurden nun erfolgreich rekonstruiert, und insgesamt blieben weniger Bereiche lückenhaft. Die F-Scores stiegen über die Szenen hinweg kontinuierlich an (V1: 0.54, V2: 0.61, V3: 0.62), was auf eine robuste Performance auch bei zunehmender Komplexität hindeutet. Auch im Detail zeigte sich eine Verbesserung: Feinere Strukturen blieben besser erhalten, während gröbere Geometrien ähnlich zuverlässig erfasst wurden wie bei NeuralRecon. Die Meshes waren durchweg glatt, artefaktfrei und wiesen eine höhere geometrische Präzision in komplexen Bereichen auf. Die globale Konsistenz blieb über alle Szenen hinweg stabil.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/neucon00.png}
        \caption{Neural Recon}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/visfusion00.png}
        \caption{VisFusion}
    \end{subfigure}
    \caption{Vergleich zwischen NeuralRecon (links) und VisFusion (rechts) in Szene V1. VisFusion rekonstruiert die schrägen Flächen erfolgreich und zeigt eine deutlich höhere Vollständigkeit.}
    \label{fig:visfusion_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/visfusion00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/visfusion.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{VisFusion Rekonstruktionen der Szenen V2 (links) und V3 (rechts). Die Performance bleibt auch bei zunehmender Komplexität stabil. Objekte wie die Stehlampe oder der TV-Schrank werden detaillierter erfasst.}
    \label{fig:visfusion_scenes}
\end{figure}

\paragraph{SLAM3R}

SLAM3R verfolgte einen grundlegend anderen Ansatz und erzeugte dichte, farbige Punktwolken anstelle von Meshes. Bereits in der einfachen Szene V1 (F-Score: 0.66) zeigten sich deutliche Drift-Probleme, die zu Versätzen zwischen Rekonstruktionsabschnitten führten. Im Vergleich zu MASt3R-SLAM waren diese Probleme jedoch noch moderater ausgeprägt. Lokale Details blieben gut erhalten, und die realistische Farbdarstellung trug zur visuellen Qualität bei. In Szene V2 (F-Score: 0.67) verstärkten sich die Konsistenzprobleme bereits merklich. Mit zunehmender Szenenkomplexität verschlechterte sich die Performance dramatisch: In der komplexen Szene V3 brach sie vollständig ein (F-Score: 0.42). Hier akkumulierten sich Ungenauigkeiten massiv, das Verfahren verlor häufig den Anschluss zwischen aufeinanderfolgenden Rekonstruktionsabschnitten, was zu ausgeprägten Versätzen und Brüchen im Gesamtmodell führte. Die fehlende globale Ausrichtung machte eine manuelle Registrierung der Rekonstruktionen notwendig, um sie mit dem Ground Truth abzugleichen. Große Bereiche blieben fragmentiert und inkonsistent, die globale Kohärenz ging vollständig verloren.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{SLAM3R Rekonstruktion}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/slam3r00.png}
        \caption{SLAM3R Rekonstruktion}
    \end{subfigure}
    \caption{SLAM3R Rekonstruktion von Szene V1 mit erkennbaren Drift-Problemen. Trotz Versätzen bleiben lokale Details und Farbinformation gut erhalten.}
    \label{fig:slam3r_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/slam3r00.png}
        \caption{Szene V1}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/slam3r00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/slam3r.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{SLAM3R Performance-Vergleich über Szenen: V1 (links), V2 (Mitte), V3 (rechts). Deutlich erkennbar ist die zunehmende Fragmentierung und der Verlust globaler Kohärenz mit steigender Komplexität.}
    \label{fig:slam3r_scenes_progression}
\end{figure}


\paragraph{MASt3R-SLAM}

MASt3R-SLAM erzeugte ebenfalls Punktwolken und zeigte über alle Szenen hinweg konstante F-Scores (V1: 0.50, V2: 0.50, V3: 0.45). In der einfachen Szene V1 war das Verfahren jedoch SLAM3R deutlich unterlegen: Der Drift war stärker ausgeprägt, und die globale Kohärenz litt bereits in dieser unkomplizierten Umgebung. Auch Fragmente traten bereits in V1 häufiger auf als bei SLAM3R und beeinträchtigten die visuelle Qualität der Rekonstruktion. Mit zunehmender Szenenkomplexität zeigte sich jedoch ein anderes Bild: Während SLAM3R in V2 und besonders in V3 stark einbrach, blieb MASt3R-SLAM relativ stabil. In den komplexeren Szenen V2 und V3 übertraf es SLAM3R sowohl in der räumlichen Konsistenz als auch in der globalen Ausrichtung. Die Fragmentierung durch frei schwebende Punkte war zwar weiterhin vorhanden und trug zur Unübersichtlichkeit bei, jedoch deutlich moderater als bei SLAM3R in diesen Szenen. Rekonstruktionsabschnitte fügten sich besser zu einem Gesamtbild zusammen, auch wenn eine manuelle Registrierung weiterhin notwendig blieb, um die Rekonstruktionen mit dem Ground Truth abzugleichen. Die lokale Detailtreue mit Farbinformation war durchgängig gegeben, und die Performance blieb über die Szenen hinweg bemerkenswert konsistent, während SLAM3R zunehmend instabil wurde.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{Ground Truth}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/mast3r0.png}
        \caption{MASt3R-SLAM Rekonstruktion}
    \end{subfigure}
    \caption{MASt3R-SLAM Rekonstruktion von Szene V1 mit ausgeprägten Drift-Problemen. Die Versätze und frei schwebenden Punkte sind hier deutlich stärker als bei SLAM3R in derselben Szene.}
    \label{fig:mast3r_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/mast3r0.png}
        \caption{Szene V1}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/mast3r00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/mast3r.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{MASt3R-SLAM Performance-Vergleich über Szenen: V1 (links), V2 (Mitte), V3 (rechts). Im Gegensatz zu SLAM3R bleibt die Konsistenz über die Szenen hinweg relativ stabil.}
    \label{fig:mast3r_scenes_progression}
\end{figure}


\paragraph{Vergleichende Bewertung}
Tabelle \ref{tab:qualitative_comparison} fasst die qualitativen Beobachtungen zusammen und bewertet die Verfahren anhand einer fünfstufigen Skala (sehr gering, gering, mittel, hoch, sehr hoch) in Bezug auf die definierten Qualitätskriterien. Diese Bewertung erfolgte durch vergleichende visuelle Inspektion der Rekonstruktionen und ermöglicht eine differenzierte Einordnung der Stärken und Schwächen der einzelnen Verfahren.

\begin{table}[H]
    \centering
    \label{tab:qualitative_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Kriterium} & \textbf{NeuralRecon} & \textbf{VisFusion} & \textbf{SLAM3R} & \textbf{MASt3R-SLAM} \\
        \midrule
        Vollständigkeit & mittel & hoch & hoch & hoch \\
        Detailtreue & gering & mittel & sehr hoch & sehr hoch \\
        Artefaktfreiheit & sehr hoch & sehr hoch & sehr gering & gering \\
        Oberflächenqualität & sehr hoch & sehr hoch & mittel & mittel \\
        \bottomrule
    \end{tabular}
    \medskip
    \caption{Qualitative Bewertung der Rekonstruktionsverfahren im Vergleich}
\end{table}

\subsubsection{Rekonstruktion unter realen VR-Bedingungen}

Während die virtuellen Szenen V1--V3 die Rekonstruktionsqualität unter kontrollierten 
Bedingungen mit synthetischen, hochauflösenden Kameras evaluieren, dienen die realen 
Szenen R1 (Schlafzimmer) und R2 (Wohnzimmer) der Praxisvalidierung des Systems unter 
authentischen VR-Bedingungen mit der Meta Quest 3.

\paragraph{Charakteristika der Quest 3-Passthrough-Kamera}
Die für Tracking optimierten Pass\-through-Kameras der Quest 3 unterscheiden sich in 
mehreren Hardware-bedingten Eigenschaften von synthetischen Kameras. Sie weisen einen 
geringen Dynamikumfang auf, zeigen Rolling-Shutter-Effekte bei schnellen Kopfbewegungen 
und produzieren Bildrauschen bei schwacher Beleuchtung. Zudem ist der Kontrast reduziert.

\paragraph{Beobachtungen NeuralRecon}
NeuralRecon wies in den realen Szenen R1 und R2 die geringste Rekonstruktionsqualität 
aller evaluierten Modelle auf. Die erzeugten Rekonstruktionen zeigten erhebliche 
Diskontinuitäten in der Oberflächenrepräsentation, wobei feinere geometrische Details 
größtenteils nicht erfasst wurden. Die Rekonstruktionen beschränkten sich im Wesentlichen 
auf grobe Approximationen der Szenengeometrie.

\paragraph{Beobachtungen VisFusion}
VisFusion erzielte eine marginal verbesserte Rekonstruktionsqualität gegenüber NeuralRecon. 
Die Oberflächenrepräsentation wies weiterhin partielle Diskontinuitäten auf, wobei eine 
Erfassung gröberer geometrischer Strukturen erkennbar war. Die Vollständigkeit der 
Rekonstruktion übertraf NeuralRecon geringfügig, beschränkte sich jedoch ebenfalls 
primär auf die Erfassung makroskopischer Formmerkmale.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\linewidth]{images/room03_neucon00.png}
    \includegraphics[width=0.49\linewidth]{images/room03_visfusion00.png}
    \caption{Rekonstruktionen der volumetrischen Verfahren in Szene R1. Links: NeuralRecon 
    mit erheblichen Diskontinuitäten. Rechts: VisFusion mit marginal verbesserter Vollständigkeit.}
    \label{fig:real_volumetric}
\end{figure}

\paragraph{Beobachtungen SLAM3R}
SLAM3R demonstrierte in den realen Szenen eine signifikant höhere Rekonstruktionsqualität 
als die volumetrischen Verfahren. Die Punktwolken-Repräsentation ermöglichte eine detaillierte 
Erfassung der Szenengeometrie mit deutlich reduzierter Diskontinuität gegenüber NeuralRecon 
und VisFusion. Bemerkenswert ist die verbesserte Performance gegenüber den synthetischen 
Szenen, die sich in reduzierter Artefaktbildung und beschleunigter Konvergenz manifestierte.

\paragraph{Beobachtungen MASt3R-SLAM}
MASt3R-SLAM erzielte die höchste Rekonstruktionsqualität in den realen Szenen. Die 
Rekonstruktionsergebnisse waren qualitativ vergleichbar mit SLAM3R, wiesen jedoch einen 
marginal erhöhten Detailgrad sowie reduzierte Artefaktbildung auf. Die Oberflächenrepräsentation 
zeigte sehr geringe Diskontinuität. Analog zu SLAM3R übertraf die Performance in den realen 
Szenen die Ergebnisse der synthetischen Evaluationen, wobei MASt3R-SLAM insgesamt die 
höchste Rekonstruktionsgüte in R1 und R2 erreichte.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\linewidth]{images/room04_slam3r00.png}
    \includegraphics[width=0.49\linewidth]{images/room04_mast3r00.png}
    \caption{Rekonstruktionen der SLAM-basierten Verfahren in Szene R2. Links: SLAM3R 
    mit hohem Detailgrad. Rechts: MASt3R-SLAM mit höchster Rekonstruktionsgüte und reduzierter 
    Artefaktbildung.}
    \label{fig:real_slam}
\end{figure}

\paragraph{Vergleich synthetische vs. reale Szenen}
Der Vergleich zwischen den synthetischen Szenen V1--V3 und den realen Szenen R1--R2 
offenbart distinkte Performance-Charakteristika der unterschiedlichen Modellklassen. 
Die volumetrischen Verfahren NeuralRecon und VisFusion, welche in den synthetischen 
Szenen robuste und vollständige Rekonstruktionen generierten, zeigten in den realen 
Szenen eine signifikant reduzierte Rekonstruktionsqualität mit ausgeprägten 
Diskontinuitäten und verlorenem geometrischem Detailgrad. Konträr dazu wiesen die 
SLAM-basierten Verfahren SLAM3R und MASt3R-SLAM eine Performance-Inversion auf: 
Während diese in den synthetischen Szenen partiell mit Artefaktbildung und prolongierter 
Konvergenz konfrontiert waren, erzielten sie in den realen Szenen superiore 
Rekonstruktionsergebnisse mit erhöhtem Detailgrad und minimaler Diskontinuität. 
MASt3R-SLAM erreichte dabei die höchste Rekonstruktionsgüte in den realen VR-Aufnahmen.

\subsection{Modularität und Systemstabilität}

Die Modularität der entwickelten Architektur wird durch die erfolgreiche Integration 
der vier heterogenen Rekonstruktionsverfahren NeuralRecon, VisFusion, MASt3R-SLAM 
und SLAM3R unter Beweis gestellt. Dieser Abschnitt quantifiziert den Integrationsaufwand 
pro Modell und bewertet die Stabilität des Systems unter realen Betriebsbedingungen.

\subsubsection*{Integrationsaufwand}

Die Integration eines neuen Rekonstruktionsmodells in RTReconstruct erfordert drei 
zentrale Implementierungsschritte: die Erstellung eines modellspezifischen Docker-Containers 
mit allen benötigten Abhängigkeiten, die Implementierung einer Worker-Klasse als 
Unterklasse von \texttt{BaseReconstructionModel} sowie die Registrierung des neuen 
Dienstes in der \texttt{docker-compose.yml}. Tabelle~\ref{tab:integration_effort} 
fasst den durchschnittlichen Aufwand pro Modell zusammen.

\begin{table}[H]
\centering
\caption{Integrationsaufwand pro Rekonstruktionsmodell}
\label{tab:integration_effort}
\begin{tabular}{lcccc}
\toprule
\textbf{Komponente} & \textbf{NeuralRecon} & \textbf{VisFusion} & \textbf{MASt3R} & \textbf{SLAM3R} \\
\midrule
Dockerfile (Zeilen)           & 42  & 38  & 51  & 47  \\
Worker-Klasse (Zeilen)        & 156 & 148 & 189 & 203 \\
docker-compose.yml (Zeilen)   & 12  & 12  & 14  & 13  \\
\midrule
\textbf{Gesamt (Zeilen)}      & 210 & 198 & 254 & 263 \\
Geschätzter Zeitaufwand (h)   & 4--6 & 3--5 & 6--8 & 7--9 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
Der Großteil des Implementierungsaufwands entfällt auf zwei Bereiche: Die Erstellung 
des Docker-Containers erfordert die korrekte Konfiguration von Base-Image, CUDA-Versionen, 
Python-Abhängigkeiten und modellspezifischen Bibliotheken. Besonders anspruchsvoll ist 
dabei die Kompatibilität zwischen PyTorch-, CUDA- und Treiber-Versionen, die für jedes 
Modell individuell aufgelöst werden muss. Die Implementierung der Worker-Klasse umfasst 
neben der Datenaufbereitung auch die Initialisierung des jeweiligen Rekonstruktionsmodells, 
das Laden von Modellgewichten sowie die Transformation der Ausgabe in das standardisierte 
\texttt{ModelResult}-Format.

Die Einbindung in die \texttt{docker-compose.yml} erfordert die Definition eines neuen 
Service-Eintrags, der den Build-Kontext, GPU-Ressourcen, Umgebungsvariablen (Modellname, 
Server-URL) sowie die Netzwerkanbindung an den zentralen Router spezifiziert. Dieser Schritt 
ist weitgehend standardisiert und kann durch Anpassung eines bestehenden Service-Templates 
erfolgen. Die Kommunikationslogik - Verbindungsaufbau zum Router, asynchroner Fragmentempfang 
und Ergebnisrücksendung - wird vollständig von der Basisklasse \texttt{BaseReconstructionModel} 
bereitgestellt und erfordert keine modellspezifische Anpassung. Dies reduziert die 
Komplexität der Integration erheblich.

\subsubsection*{Systemstabilität}

Die Stabilität des Systems wurde hinsichtlich der Isolierung von Fehlern einzelner 
Komponenten untersucht. Dabei wurden drei Aspekte betrachtet.

\paragraph{Fehlertoleranz bei fehlerhaften Rekonstruktionen}
Fehler innerhalb der Worker-Prozesse - etwa durch ungültige Eingabedaten, 
Speicherengpässe oder Modell-Inferenzfehler - führten nicht zu einem Absturz 
des zentralen Routers oder anderer Worker. Die fehlerhafte Rekonstruktionsanfrage 
wurde verworfen, während das System für andere Szenen und Modelle funktionsfähig blieb. 
Dies bestätigt die durch Containerisierung erreichte Isolation zwischen den Komponenten.

\paragraph{Verhalten bei Worker-Absturz}
Stürzt ein Worker-Container ab, wird dieser automatisch aus der Liste der verfügbaren 
Modelle entfernt. Der Router verarbeitet weiterhin Anfragen für die verbleibenden 
Modelle, und andere Clients bleiben unbeeinträchtigt. Ein manueller Neustart des 
abgestürzten Containers führt zur automatischen Wiederanmeldung beim Router, sodass 
das Modell anschließend erneut verfügbar ist.

\paragraph{Verbindungsabbruch zwischen Frontend und Backend}
Bei einem Verbindungsabbruch der WebSocket-Verbindung zwischen Frontend und Router 
ist ein manueller Neustart der Frontend-Anwendung erforderlich. Ein automatischer 
Reconnect-Mechanismus wurde nicht implementiert. Dies stellt eine Limitation der 
aktuellen Implementierung dar, die insbesondere bei mobilen VR-Anwendungen mit 
instabiler Netzwerkverbindung relevant werden kann.

\section{Diskussion}
\label{sec:discussion}

Ziel dieser Arbeit war es zu untersuchen, wie gut sich eine modulare, containerisierte Systemarchitektur eignet, um unterschiedliche Verfahren zur Echtzeit-3D-Rekonstruktion in eine bestehende VR-Umgebung zu integrieren. % [file:1]
Die Evaluation zeigt, dass der entwickelte Prototyp eine stabile End-to-End-Pipeline (Unity-Frontend \(\rightarrow\) WebSocket \(\rightarrow\) Backend-Router \(\rightarrow\) Modell-Worker \(\rightarrow\) VR-Visualisierung) bereitstellt und dabei verschiedene Rekonstruktionsansätze unter identischen Schnittstellenbedingungen vergleichbar macht. % [file:1]
Im Folgenden werden die Ergebnisse im Hinblick auf Echtzeitfähigkeit, Modularität, Rekonstruktionsqualität sowie die Eignung für VR-Anwendungen eingeordnet und die Forschungsfrage abschließend beantwortet. % [file:1]

\subsection{Echtzeitfähigkeit und Performance-Charakteristik}
\label{subsec:diskussion_performance}

Die Latenzaufschlüsselung (vgl. Abbildung \ref{fig:latency_breakdown}) zeigt, dass die modulare Architektur ihre Design-Zielsetzung erreicht: Der architektonische Overhead durch Entkopplung, Serialisierung und Netzwerktransfer ($L_{network}$) ist \textbf{nicht der dominierende Flaschenhals}. Stattdessen dominiert die Inferenzzeit ($L_{inference}$) das Gesamtlatenzbudget – ein Befund, der die Architektur entlastet und die Modelle in den Fokus rückt.

Die geringe Varianz zwischen Wiederholungen belegt, dass die Pipeline selbst stabil ist. Die Unterschiede entstehen nicht aus systemischen Fluktuationen, sondern aus den Inferenzcharakteristiken der Modelle. Dies ist ein zentraler Befund: Die Architektur kapselt die Modelle erfolgreich, überträgt aber deren latente Kosten transparent auf das Gesamtsystem. Dass MASt3R-SLAM deutlich höhere Latenzen aufweist als NeuralRecon oder VisFusion, reflektiert nicht nur algorithmische Unterschiede, sondern auch, wie unterschiedliche Optimierungsziele (dichte Punktwolken vs. voxelbasierte Oberflächen) sich auf den Echtzeitbetrieb auswirken.

Der gemessene Durchsatz (vgl. Tabelle \ref{tab:throughput_comparison}) verdeutlicht den damit verbundenen Trade-off: Volumetrische Verfahren erreichen zwar höhere Fragmente-pro-Sekunde-Werte, aber die absolute Rate bleibt durch die Inferenzkosten limitiert. Dies bedeutet, dass die Architektur zwar echtzeitnahes Streaming ermöglicht, die interaktive Akzeptanz aber direkt von der Modellwahl abhängt – ein Aspekt, der für VR-Anwendungen zentral ist.

Die asynchrone Verarbeitung im Unity-Frontend bestätigt, dass das Design die VR-Framerate stabilisiert, solange die Datenlast moderat ist. Allerdings offenbart die Rendering-Performance eine lokale Implementierungslimitierung: Bei dichten Punktwolken (z. B. SLAM-Ansätze) bricht die Framerate von 72 FPS auf ca. 38 FPS ab (vgl. Abbildung \ref{fig:fps_breakdown}), weil der Unity VFX Graph die Datenlast nicht effizient genug verarbeiten kann. Dieser Engpass ist \textbf{nicht architektonisch}, sondern resultiert aus der gewählten Rendering-Komponente.

Zusammenfassend liegt die Begrenzung der Echtzeitfähigkeit primär in den inhärenten Latenzen der KI-Modelle ($L_{inference}$) und sekundär in der Frontend-Rendering-Performance ($L_{render}$). Der Netzwerk-Overhead ($L_{network}$) ist quantitativ vernachlässigbar. Für VR-Anwendungen bedeutet dies: Die Modellwahl ist eine Abwägung zwischen Inferenzkosten und visueller Qualität, während die Rendering-Strategie gleichrangig für die interaktive Akzeptanz ist.


\subsection{Modularität und Erweiterbarkeit der Architektur}
\label{subsec:diskussion_modularitaet}

Die erfolgreiche Integration von vier heterogenen Verfahren bestätigt, dass die Container-Kapselung das Kernproblem der Forschung adressiert: die Isolation inkompatibler Abhängigkeiten. Die Tatsache, dass Modelle mit unterschiedlichen PyTorch- und CUDA-Versionen parallel auf derselben GPU betrieben werden konnten, belegt, dass die Architektur die "Dependency Hell" auflöst, ohne die Vergleichbarkeit zu beeinträchtigen. Dies ist kein Implementierungsdetail, sondern der zentrale Design-Nutzen des Microservice-Ansatzes.

Der Integrationsaufwand von unter einem Arbeitstag pro Modell (vgl. Tabelle \ref{tab:integration_effort}) quantifiziert diesen Nutzen: Er zeigt, dass die Trennung zwischen API-Handling (Main-Container) und Verarbeitungslogik (Worker-Container) nicht nur wartbar und fehlerisolierend ist, sondern auch eine agile Prototyping-Plattform ermöglicht. Der Absturz eines Workers beeinträchtigt den Router nicht – was die Robustheit des Systems unterstreicht, aber auch offenbart, dass die aktuelle Implementierung keine automatische Reconnect-Logik für das Frontend vorsieht. Bei Verbindungsabbrüchen erfordert dies einen manuellen Neustart, was die Nutzererfahrung in VR stört.

Eine fundamentale Grenze zeigt sich in der Stateless-Natur der Container: Der Rekonstruktionsfortschritt geht bei einem Neustart verloren. Dies ist keine Implementierungsschwäche, sondern eine bewusste Design-Entscheidung, die Skalierbarkeit und Isolation priorisiert, aber für lange Aufnahmen oder produktive Szenarien unzureichend ist. Die gemeinsame Nutzung einer einzelnen GPU limitiert zudem den parallelen Durchsatz – eine Limitierung, die jedoch nicht architektonisch, sondern durch das Deployment bestimmt ist.

Zusammenfassend belegt die Evaluation, dass die Architektur ihre Hauptziele erfüllt: flexible Integration, Isolation und Vergleichbarkeit. Die identifizierten Grenzen liegen nicht in der Modul-Konzeption, sondern in der fehlenden State-Persistenz und der hardwarebedingten Skalierung.

\subsection{Rekonstruktionsqualität und Generalisierbarkeit}
\label{subsec:discussion_quality}

Die Modell-Evaluation unter realen VR-Bedingungen legt eine deutliche Diskrepanz zwischen den Ergebnissen auf synthetischen Benchmarks und der tatsächlich wahrgenommenen Rekonstruktionsqualität im Headset offen. In den virtuellen Testszenen erzielen die volumetrischen Verfahren \textit{NeuralRecon} und \textit{VisFusion} hohe F-Scores (z. B. 0.82 und 0.85 in der \textit{Synthetic-Room}-Szene) und liefern konsistente Oberflächen, während die SLAM-basierten Ansätze punktuell fragmentierter erscheinen (vgl. Tabelle~\ref{tab:fscore_results} und \ref{tab:recon_quality}). Unter den realen Bedingungen der Meta Quest~3 kehrt sich dieses Bild jedoch weitgehend um: Die volumetrischen Modelle erzeugen stark geglättete, lückenhafte Szenen, während \textit{MASt3R-SLAM} und \textit{SLAM3R} deutlich detailreichere Punktwolken rekonstruieren, die in den quantitativen Metriken jedoch nur moderate F-Scores (0.58–0.67) erreichen.

Diese beobachtete ``Performance-Inversion'' lässt sich nur verstehen, wenn die Anforderungen der Modellarchitekturen mit den Eigenschaften der verwendeten Sensor- und SDK-Infrastruktur zusammengedacht werden. Volumetrische TSDF-Verfahren setzen voraus, dass jedes Kamerabild mit einer präzise dazugehörigen Pose verknüpft ist, da bereits kleine Zeitversätze zwischen Frame und Extrinsics zu systematischen Fehlern bei Registrierung und Fusion führen. In der synthetischen Umgebung sind diese Voraussetzungen erfüllt, da die virtuelle Kamera perfekt synchronisierte Bild–Pose-Paare liefert – was sich direkt in den hohen F-Scores niederschlägt. In der realen Quest-3-Umgebung stehen dagegen nur eingeschränkt zuverlässige Zeitinformationen zur Verfügung, und die Passthrough-Kameras weisen zusätzliche Artefakte wie Rolling-Shutter, Rauschen und reduzierten Dynamikumfang auf. Die Folge ist eine schleichende Fehlregistrierung, die sich in geglätteten Oberflächen und dem Verlust feiner Geometrie niederschlägt, sodass die F-Scores um durchschnittlich 0.25–0.30 Punkte einbrechen, obwohl die zugrunde liegenden Algorithmen auf idealisierten Daten hohe metrische Werte erreichen.

SLAM-basierte Verfahren reagieren anders auf diese Bedingungen. \textit{MASt3R-SLAM} und \textit{SLAM3R} nutzen die mitgelieferten Kameraparameter nicht – sie ignorieren die Extrinsics und Intrinsics vollständig und schätzen Pose und Tiefe ausschließlich über internes Feature-Matching und visuelle Odometrie. Dadurch sind sie immun gegenüber den Zeitversatz- und Synchronisationsproblemen der externen Pose-Daten. Sie tolerieren die moderaten Inkonsistenzen zwischen Bild und mitgelieferten Parametern besser und kompensieren einen Teil der Kamera- und SDK-Artefakte durch ihre eigene Trajektorien- und Kartenoptimierung. In den realen Szenen führen diese Eigenschaften dazu, dass die resultierenden Punktwolken trotz globaler Drift- und Fragmentierungsartefakte perzeptiv detailreicher und strukturtreuer wirken als die volumetrischen Meshes, auch wenn die quantitativen F-Scores sie nicht als überlegen klassifizieren.

Für die Beurteilung der Generalisierbarkeit bedeutet dies, dass metrische Überlegenheit auf synthetischen Datensätzen ohne Berücksichtigung der konkreten Sensorpipeline in die Irre führen kann. Im Kontext der Meta Quest~3 ist weniger die maximale Leistung eines Verfahrens unter idealisierten Bedingungen entscheidend, sondern dessen Robustheit gegenüber unsauberen Kameradaten, Zeitversatz und Beschränkungen der Passthrough-Schnittstelle. Die Evaluation zeigt damit, dass VR-spezifische Hard- und Software-Eigenschaften ein zentrales Kriterium für die Auswahl und Bewertung von Rekonstruktionsverfahren darstellen und dass volumetrische Architekturen, die stark auf exakte Frame–Pose-Synchronisation angewiesen sind, unter den gegebenen SDK-Bedingungen nur eingeschränkt geeignet sind.

\subsection{Eignung für VR-Anwendungen}
\label{subsec:diskussion_vr}

Die stabile End-to-End-Kommunikation belegt, dass die Architektur grundsätzlich VR-tauglich ist, aber die Praxis offenbart spezifische Engpässe, die unabhängig von der Modellperformance liegen. Große GLB-Modelle verursachen merkliche Ladeverzögerungen im Unity-Frontend, und die Punktwolken-Visualisierung muss auf 100\,000 Punkte limitiert werden, um die Framerate auf der Meta Quest~3 zu halten (vgl. Abbildung~\ref{fig:rendering_performance}). Diese Limitierungen resultieren nicht aus der Modell-Inferenz, sondern aus der gewählten Rendering-Implementierung und der Datenmenge, die der Unity VFX Graph effizient verarbeiten kann. [file:1]

Für die praktische Eignung in VR ist dabei zentral, dass die wahrgenommene Qualität nicht linear mit den gemessenen Metriken korreliert. Rekonstruktionen mit hohen F-Scores können subjektiv weniger überzeugend wirken, wenn sie global inkonsistent oder zu stark geglättet sind, während Verfahren mit moderaten metrischen Werten durch höhere lokale Detailtreue und perzeptive Kohärenz ein glaubwürdigeres Raumgefühl vermitteln. [file:1] Ob ein Verfahren für VR-Anwendungen geeignet ist, hängt daher ebenso von der Darstellungsstrategie – etwa Chunking, Culling und GPU-basiertem Streaming – wie von der reinen Rekonstruktionsgüte ab. [file:1]

\subsection{Beantwortung der Forschungsfrage}
\label{subsec:discussion_forschungsfrage}

Die zentrale Forschungsfrage dieser Arbeit lautete: \textit{Wie gut eignet sich eine modulare, containerisierte Systemarchitektur zur Integration verschiedener Echtzeit-3D-Rekonstruktionsverfahren in eine bestehende Virtual-Reality-Umgebung?}

Die Evaluation zeigt, dass die Architektur ihre primären Designziele erreicht: Sie ermöglicht den stabilen, vergleichbaren Betrieb heterogener Rekonstruktionsmodelle bei überschaubarem Integrationsaufwand und isoliert Framework-Komplexitäten erfolgreich. Die Messungen belegen, dass der architektonische Overhead durch Containerisierung und Netzwerktransfer nicht der limitierende Faktor ist – die Grenzen liegen vielmehr in der Inferenzperformance der Modelle, der GPU-Skalierung und der VR-seitigen Darstellungslogik. Die beobachtete Performance-Inversion zwischen synthetischen und realen Szenen verdeutlicht, dass die Eignung eines Verfahrens für VR nicht aus Laborbenchmarks allein abgeleitet werden kann, sondern stark von der Robustheit gegenüber unsauberen Sensordaten und SDK-Limitierungen abhängt. Die Architektur eignet sich damit als flexible Forschungsplattform, erfordert aber für produktive VR-Anwendungen eine Optimierung der Frontend-Performance und der State-Persistenz.
