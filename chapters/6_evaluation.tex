\chapter{Evaluation}
\label{chap:evaluation} % Hinzugefügt, falls andere Kapitel darauf verweisen

In diesem Kapitel wird das entwickelte RTReconstruct-System hinsichtlich seiner Echtzeitfähigkeit, Modularität und Rekonstruktionsqualität evaluiert. Zunächst werden die Evaluationsziele definiert und die Testumgebung beschrieben. Anschließend werden die gemessenen Performance-Metriken sowie die Rekonstruktionsqualität der integrierten Modelle dargestellt. Das Kapitel schließt mit einer Diskussion der Ergebnisse im Kontext der definierten Anforderungen.

\section{Evaluationsziele}
\label{sec:eval_ziele}

Die Evaluation verfolgt drei zentrale Ziele, die direkt aus der in Kapitel~\ref{ch:introduction} formulierten Forschungsfrage abgeleitet sind:

\begin{enumerate}
    \item \textbf{Funktionale Validierung}: \\
    Es wird nachgewiesen, dass die modulare Architektur die definierten funktionalen Anforderungen erfüllt. Insbesondere wird geprüft, ob verschiedene Rekonstruktionsmodelle parallel betrieben werden können und ob die End-to-End-Kommunikation zwischen VR-Frontend und Backend stabil funktioniert.
    
    \item \textbf{Echtzeitfähigkeit}: \\
    Es wird bewertet, ob das System die für VR-Anwendungen erforderlichen Performance-Anforderungen erfüllt. Dabei werden die kritischen Metriken Latenz, Durchsatz und Ressourcenauslastung untersucht.
    
    \item \textbf{Rekonstruktionsqualität}: \\
    Es erfolgt eine qualitative und, soweit möglich, quantitative Bewertung der von den integrierten Modellen erzeugten 3D-Rekonstruktionen unter identischen Bedingungen.
\end{enumerate}

\section{Evaluationsmethodik}
Die Evaluationsmethodik beschreibt die Testumgebung, die verwendeten Testszenarien und die Messmethoden, die zur Erreichung der Evaluationsziele eingesetzt wurden. 

\subsection{Test- und Evaluationsumgebung}
\label{sec:testumgebung}

Um die Reproduzierbarkeit der Performance-Messungen und die Vergleichbarkeit der erzielten Ergebnisse zu gewährleisten, wurde die gesamte Evaluation in einer dedizierten, kontrollierten Hardware- und Software-Umgebung durchgeführt. Die zentralen Komponenten und Spezifikationen dieser Umgebung sind in Tabelle ~\ref{tab:hardware_and_software} zusammengefasst.

\begin{table}[H]
    \centering
    \caption{Spezifikationen der Hard- und Software-Umgebung}
    \label{tab:hardware_and_software}
    \begin{tabularx}{\textwidth}{l X}
        \toprule
        \textbf{Kategorie} & \textbf{Details und Spezifikationen} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Hardware-Umgebung (Backend/Server)}} \\
        \midrule
        Backend-Server CPU & AMD Ryzen 5 2600 (6 Kerne, 12 Threads) \\
        GPU & NVIDIA GeForce GTX 1070 Ti, 8 GB VRAM \\
        VR-System (Frontend) & Meta Quest 3 \\
        Netzwerk & WiFi 5 Heimnetzwerk \\
        \midrule
        \multicolumn{2}{l}{\textbf{Software-Umgebung und Frameworks}} \\
        \midrule
        Betriebssystem & Windows 11, Build 25H2 (Host) \\
        Containerisierung & Docker (\textit{29.0.1}) \\
        GPU-Unterstützung & NVIDIA Container Toolkit (\textit{580.97}), CUDA \textit{13} \\
        VR-Frontend & Unity (\textit{6000.2.12f1}) \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Testszenarien und Datensätze}

Für die Evaluation wurden fünf Testszenen mit unterschiedlichen Komplexitätsstufen 
konzipiert: drei virtuelle Szenen mit verfügbarem Ground-Truth zur quantitativen 
Bewertung sowie zwei reale Szenen aus einem typischen Alltagsumfeld zur Validierung der
Praxistauglichkeit. Eine kurze Übersicht über alle Testszenen findet sich in Tabelle~\ref{tab:test_scenes_overview}.

\begin{table}[H]
    \centering
    \caption{Übersicht und Klassifikation der Testszenen}
    \label{tab:test_scenes_overview}
    \begin{tabularx}{\textwidth}{lcclX}
        \toprule
        \textbf{Szene} & \textbf{Größe (m)} & \textbf{Fragments} & \textbf{Frames} & \textbf{Merkmale} \\
        \midrule
        \multicolumn{5}{l}{\textbf{Virtuelle Szenen}} \\
        V1 -- Primitive & $5\times 5\times 5$ & 24 & 216 & Geometrische Grundformen, unifarbige Oberflächen \\
        V2 -- Schlafzimmer & $6\times 5\times 3$ & 44 & 396 & moderate Größe, Okklusionen \\
        V3 -- Mehrzweckraum & $10\times 5\times 3$ & 58 & 522 & ähnlich V2, große freie Flächen \\
        \midrule
        \multicolumn{5}{l}{\textbf{Reale Szenen}} \\
        R1 & $6\times 4\times 3$ & 48 & 432 & \textit{Schlafzimmer}, Details, Schrägen, Okklusionen \\
        R2 & $7.5\times 5\times 3$ & 43 & 387 & \textit{Wohnzimmer}, Glas, Reflexionen, große Flächen \\
        \bottomrule
    \end{tabularx}
    \medskip
\end{table}

\subsubsection{Virtuelle Szenen}

Die drei virtuellen Szenen wurden in Unity erstellt und ermöglichen durch verfügbare \textit{Ground-Truth-Meshes} eine quantitative Evaluation mittels F-Score. Die Szenen sind so konzipiert, dass sie eine progressive Komplexitätssteigerung durchlaufen, um verschiedene Aspekte der Rekonstruktionsverfahren isoliert zu testen. Abbildung ~\ref{fig:virtual_scenes} zeigt eine Übersicht aller drei Szenen.

\paragraph{Szene V1 -- Geometrische Primitive}
Szene V1 dient als Baseline-Test und enthält ausschließlich einfache geometrische Primitive (Würfel, Torus, Pyramide) in einem quadratischen Raum mit farbigen Wänden (blau, cyan, gelb, rot). Die unifarbigen, matten Oberflächen ohne Texturen ermöglichen die isolierte Bewertung fundamentaler Rekonstruktionsfähigkeiten: scharfe Kanten, gekrümmte Oberflächen und feature-arme Flächen.

\paragraph{Szene V2 -- Möbliertes Schlafzimmer}
Szene V2 repräsentiert einen möblierten Innenraum mittlerer Komplexität mit Doppelbett, Sessel, Couch, Sideboard, Wandbildern und Stehlampe. Diese Szene testet die Rekonstruktion komplexer Möbelgeometrie, das Verhalten bei Okklusionen, die Texturverarbeitung sowie die Detailerfassung verschiedener Materialien und Dekorationsobjekte.

\paragraph{Szene V3 -- Erweiterter Mehrzweckraum}
Szene V3 erweitert das Konzept von V2 durch einen größeren Raumzuschnitt mit deutlich mehr freier Bodenfläche. Der multifunktionale Raum kombiniert Schlaf-, Wohn- und Arbeitsbereich mit Doppelbett, Sessel, Couch, Sideboard, Schreibtisch, Stuhl und diversen Kleinobjekten. Die größere Raumausdehnung und zusätzlichen Möbelelemente testen die Skalierbarkeit der Rekonstruktionsverfahren sowie deren Fähigkeit, sowohl detailreiche Objektbereiche als auch ausgedehnte leere Flächen konsistent zu erfassen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\textwidth]{images/room00.png}
    \includegraphics[width=0.32\textwidth]{images/room01.png}
    \includegraphics[width=0.32\textwidth]{images/Room02.png}
    \caption{Übersicht der drei virtuellen Testszenen: V1 (Geometrische Primitive), V2 (Möbliertes Schlafzimmer), V3 (Komplexer Mehrzweckraum)}
    \label{fig:virtual_scenes}
\end{figure}

\subsubsection{Reale Szenen}

Zur Evaluierung der praktischen Anwendbarkeit des Systems wurden realitätsnahe Alltagsräume ausgewählt. Diese ermöglichen es, das System unter natürlichen Lichtverhältnissen und realistischen Raumgeometrien zu testen und somit die Eignung für den alltäglichen Einsatz zu beurteilen.

\paragraph{Szene R1 -- Schlafzimmer}

Das Schlafzimmer ist ein schmaler Dachraum mit Arbeits- und Ruhebereich. Am einen Ende befindet sich ein Schreibtisch mit Bürostuhl und Monitor, am anderen Ende ein graues Sofa und ein TV-Board. Ein großes Dachflächenfenster sowie ein seitliches Fenster sorgen für viel Tageslicht, unterstützt durch Arbeitsleuchten am Schreibtisch. Der dunklere Bodenbelag und die hellen Wände erzeugen deutliche Kontraste, die für die Rekonstruktion von Kanten und Strukturen vorteilhaft sind.

\paragraph{Szene R2 -- Wohnzimmer}

Das Wohnzimmer ist ein helles, großzügiges Zimmer mit Dachschräge und breiter Fensterfront mit Zugang zum Balkon. Die Einrichtung umfasst eine große Sofalandschaft mit einem Couchtisch aus Glas, TV-Bereich und mehrere Teppiche, ergänzt durch Pflanzen und Deko-Elemente. Der helle Holzboden und die diffuse Beleuchtung durch Fenster und Deckenleuchten erzeugen eine gleichmäßige Ausleuchtung ohne starke Schatten. Die Kombination aus Glasflächen, Stoffoberflächen und Möbelkanten bietet vielfältige Strukturen für die 3D-Rekonstruktion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{images/room03.jpg}
    \includegraphics[width=0.49\textwidth]{images/room04.jpg}
    \caption{Übersicht der beiden realen Testszenen: R1 (Schlafzimmer), R2 (Wohnzimmer)}
    \label{fig:real_scenes}
\end{figure}

\subsection{Messverfahren und Metriken}

\subsubsection{Performance-Metriken}

\paragraph{Latenz}
Die End-to-End-Latenz misst die Zeitspanne zwischen dem Versenden eines Fragments 
durch das Unity-Frontend und der Visualisierung der aktualisierten Rekonstruktion 
im VR-Headset. Sie setzt sich aus folgenden Komponenten zusammen:
\begin{align}
    L_{\text{total}} = L_{\text{network}} + L_{\text{inference}} + L_{\text{render}}
\end{align}
wobei $L_{\text{network}}$ die Netzwerklatenz (Upload des Fragments und Download 
der Rekonstruktion), $L_{\text{inference}}$ die Modell-Inferenzzeit im Backend 
(GPU-Verarbeitung) und $L_{\text{render}}$ die Rendering-Zeit im Unity-Client 
(GLB-Import und Mesh-Visualisierung) bezeichnet.


Die Netzwerklatenz $L_{\text{network}} = L_{\text{upload}} + L_{\text{download}}$ 
wird nicht direkt durch Zeitstempel gemessen, sondern aus den erfassten Datenvolumina 
und der verfügbaren Netzwerkbandbreite berechnet: $L_{\text{upload}} = S_{\text{fragment}} / B_{\text{upload}}$ 
bzw. $L_{\text{download}} = S_{\text{result}} / B_{\text{download}}$. Hierbei 
bezeichnet $S_{\text{fragment}}$ die Fragmentgröße (Upload-Volumen pro Fragment) 
und $S_{\text{result}}$ die Resultgröße (Download-Volumen der Rekonstruktion). 
Diese Methodik ermöglicht eine infrastrukturunabhängige Bewertung der Dateneffizienz.


Die Messung der übrigen Latenzkomponenten erfolgte durch präzise Zeitstempel an 
den jeweiligen Übergangspunkten der Pipeline.

\paragraph{Durchsatz}
Der Durchsatz quantifiziert, wie viele Fragmente pro Sekunde durch das System 
verarbeitet werden können. Ein höherer Durchsatz ermöglicht häufigere Updates der 
Rekonstruktion und trägt zur Immersion bei. Gemessen wurde der Durchsatz auf 
Backend-Seite für jedes Worker-Modell separat.

\paragraph{Ressourcenauslastung}
Die GPU- und CPU-Auslastung wurde kontinuierlich während der Rekonstruktion 
aufgezeichnet. GPU-Utilization und GPU-Memory wurden via \texttt{nvidia-smi} 
erfasst, CPU-Auslastung und RAM-Verbrauch pro Container via Docker Stats. Diese 
Metriken ermöglichen die Bewertung der Ressourceneffizienz und geben Aufschluss 
über Engpässe im System.

\subsubsection{Qualitätsmetriken}

\paragraph{Quantitative Bewertung}
Für Szenen mit verfügbarem Ground-Truth-Mesh wurde der F-Score als kombinierte 
Metrik für Präzision und Recall berechnet:

\begin{align}
    \text{Precision} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FP}|} \\
    \text{Recall} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FN}|} \\
    \text{F-Score} &= 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

\noindent
Rekonstruierte Punkte gelten als \textbf{True Positive (TP)}, wenn ihr Abstand zum 
Ground-Truth unter \textit{10}cm liegt, andernfalls als \textbf{False Positive (FP)}. 
\textbf{False Negatives (FN)} sind Ground-Truth-Punkte ohne entsprechenden 
rekonstruierten Punkt innerhalb des Schwellenwerts. 

\textbf{Precision} misst die Genauigkeit der Rekonstruktion, indem sie den Anteil 
korrekt rekonstruierter Punkte angibt. \textbf{Recall} bewertet die 
Vollständigkeit und gibt an, wie viele Ground-Truth-Punkte erfasst wurden. 
Der \textbf{F-Score} kombiniert beide Metriken als harmonisches Mittel und liefert einen 
ausgewogenen Gesamtwert. Je näher der F-Score bei \textit{1.0} liegt, 
desto höher ist die Qualität der Rekonstruktion.

Der explizite Schwellwert von 10cm wurde gewählt, um eine für VR-Anwendungen praxisnahe Toleranzgrenze zu definieren, die Tracking-Ungenauigkeiten der Hardware berücksichtigt und gleichzeitig grobe Geometriefehler zuverlässig als solche klassifiziert.

\paragraph{Qualitative Bewertung}
Die rekonstruierten Meshes wurden anhand folgender Kriterien bewertet:

\begin{itemize}
    \item \textbf{Vollständigkeit}: Wie viel Prozent der Szene wurde erfasst?
    \item \textbf{Detailtreue}: Sind feine Strukturen erkennbar?
    \item \textbf{Artefaktfreiheit}: Treten Löcher, Flimmern oder Fehlgeometrie auf?
    \item \textbf{Oberflächenqualität}: Glattheit und Konsistenz der Rekonstruktion
\end{itemize}

\noindent
Die Bewertung erfolgte durch visuelle Inspektion der Rekonstruktionen in Unity 
sowie durch exportierte Screenshots.

\section{Ergebnisse}
\label{sec:ergebnisse}

\subsection{Funktionale Validierung}

Die funktionale Validierung bestätigt, dass RTReconstruct alle definierten Kernfunktionalitäten erfüllt.

\paragraph{End-to-End-Kommunikation}
\label{par:e2e_functional}
Die vollständige Kommunikationskette von der Fragmenterfassung im Unity-Client über 
die WebSocket-Verbindung zum Router bis zur Verteilung an die Worker-Container und 
zurück funktioniert stabil. In \textit{60} Testläufen über eine Gesamtdauer von \textit{6} Stunden 
traten \textit{0} Verbindungsabbrüche auf.

\paragraph{Parallele Modellausführung}
\label{par:parallel_execution}
Alle vier integrierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, SLAM3R) konnten erfolgreich parallel betrieben werden. Die containerisierte Architektur gewährleistete dabei eine vollständige Isolierung der Laufzeitumgebungen, sodass divergierende Python- und PyTorch-Versionen konfliktfrei koexistierten. Als limitierender Faktor im Parallelbetrieb erwies sich die verfügbare VRAM-Kapazität der verwendeten NVIDIA GeForce GTX 1070 Ti (8 GB).

\paragraph{Multi-Szenen-Unterstützung}
Das System unterstützt die gleichzeitige Verarbeitung mehrerer Szenen. In Tests 
mit \textit{2} parallelen Szenen und \textit{4} verbundenen Clients blieb die Funktionalität 
erhalten. Die szenenspezifische Zuordnung der Rekonstruktionsergebnisse erfolgte 
fehlerfrei.

\paragraph{Visualisierung in VR}
Die über das Backend empfangenen Meshes wurden erfolgreich im Unity-Client 
visualisiert. Das in Kapitel 5 beschriebene Spatial 
Hashing ermöglichte eine performante Darstellung auch bei größeren Meshes und Punktwolken mit bis zu 100.000 Punkten.

\subsection{Performance-Analyse}

\subsubsection{Latenz}

Die Latenz stellt die zentrale Performance-Metrik für die Echtzeitfähigkeit des Systems dar. Im Folgenden wird zunächst die Gesamtlatenz über alle Testszenen und Modelle präsentiert, anschließend in ihre Komponenten zerlegt und abschließend durch die Analyse der Datenvolumina kontextualisiert.

\paragraph{Gesamtlatenz}

Zur Evaluierung der Systemperformance wurde die End-to-End-Latenz \\ \(L_{total}\) als Zeitspanne zwischen dem Absenden eines Fragments vom Client und der Visualisierung der entstandenen Rekonstruktion im Frontend gemessen. Für jede Kombination aus Testszene und Rekonstruktionsmodell wurden drei unabhängige Testläufe durchgeführt, bei denen identische Eingabedaten verwendet wurden. Um dabei Verzerrungen durch Ressourcenkonflikte zu vermeiden, wurden die Modelle sequenziell im Einzelbetrieb getestet. Abbildung~\ref{fig:latency_boxplots} visualisiert die resultierenden Latenzverteilungen als Boxplots.

Die Darstellung zeigt auf der x-Achse die vier evaluierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, SLAM3R), während die y-Achse die gemessene Latenz in Millisekunden angibt. Pro Modell sind drei Boxplots dargestellt, die jeweils die Latenzverteilung eines Testlaufs repräsentieren. Die farbliche Kodierung kennzeichnet dabei denselben Testlauf über alle Modelle hinweg.

\begin{figure}[H]
    \phantomcaption
    \label{fig:latency_boxplots}

    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room00_latency.png}
        \caption{Szene V1 -- Geometrische Primitive}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room01_latency.png}
        \caption{Szene V2 -- Möbliertes Schlafzimmer}
    \end{subfigure}
\end{figure}

\clearpage

\begin{figure}[H]\ContinuedFloat
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room02_latency.png}
        \caption{Szene V3 -- Komplexer Mehrzweckraum}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room03_latency.png}
        \caption{Szene R1 -- Schlafzimmer}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.49\linewidth]{images/room04_latency.png}
        \caption{Szene R2 -- Wohnzimmer}
    \end{subfigure}

    \caption{Gesamtlatenz \(L_{total}\) für alle Testszenen und Modelle über drei Testläufe.}
\end{figure}


Die Boxplots zeigen für alle Modelle und Szenen geringe Interquartilbereiche und minimale Ausreißer, was auf eine stabile und reproduzierbare Latenzcharakteristik des Systems hinweist. Aufgrund dieser geringen Varianz zwischen den drei Testläufen werden in den folgenden Analysen zur Zusammensetzung der Gesamtlatenz sowie zu Datenvolumina die Messwerte der drei Testläufe aggregiert dargestellt. Dies ermöglicht eine kompaktere Präsentation ohne relevanten Informationsverlust.


Die Abbildung zeigt deutliche Unterschiede in der Gesamtlatenz zwischen den Testszenen: Szene V1 weist die niedrigsten Latenzwerte auf, während die Latenz in den komplexeren Szenen V3, R1 und R2 ansteigt. Zudem variiert die Latenz zwischen den Modellen, wobei MAST3R durchgängig die höchsten Werte erreicht. Um die Ursachen dieser Variation zu identifizieren, wird die Gesamtlatenz im Folgenden in ihre Komponenten zerlegt.


\paragraph{Zusammensetzung der Gesamtlatenz}

Die beobachteten Latenzunterschiede zwischen den Testszenen lassen sich durch die Zerlegung der Gesamtlatenz in ihre konstituierenden Komponenten \(L_{\text{network}}\), \(L_{\text{inference}}\) und \(L_{\text{render}}\) analysieren. Diese Aufschlüsselung ermöglicht es, szenenabhängige Effekte auf die Inferenzzeit von fixen Overhead-Kosten der Netzwerkkommunikation und Rendering-Pipeline zu separieren. Abbildung~\ref{fig:latency_stacked_bar} visualisiert die resultierende Zusammensetzung für alle Modelle und Testszenen.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/latency_split.png}
    \caption{Zusammensetzung der Gesamtlatenz nach Komponenten für alle Rekonstruktionsmodelle, aufgeschlüsselt nach Testszene und gemittelt über die drei Testläufe.}
    \label{fig:latency_stacked_bar}
\end{figure}


Die Aufschlüsselung zeigt, dass \(L_{\text{inference}}\) den dominierenden Anteil der Gesamtlatenz ausmacht und zwischen den Szenen stark variiert. Der Anteil von \(L_{\text{network}}\) und \(L_{\text{render}}\) bleibt über die Szenen hinweg relativ konstant, nimmt jedoch prozentual mit steigender Szenenkomplexität ab. Die Netzwerklatenz \(L_{\text{network}}\) wird dabei maßgeblich durch die Größe der übertragenen Daten bestimmt, deren Quantifizierung im Folgenden dargestellt wird.


\paragraph{Fragment- und Ergebnisgrößen}

Um die Netzwerklatenz \(L_{\text{network}}\) zu kontextualisieren und die Bandbreitenanforderungen des Systems zu dokumentieren, wurden die durchschnittlichen Fragmentgrößen (Upload) und Ergebnisgrößen (Download) für alle Modelle gemessen. Tabelle~\ref{tab:data_volumes} zeigt die resultierenden Datenvolumina sowie die daraus berechnete genutzte Bandbreite.

\begin{table}[h]
    \centering
    \caption{Durchschnittliche Datenvolumina und genutzte Bandbreite aufgeschlüsselt nach Szene und Modell (gemittelt über drei Testläufe).}
    \label{tab:data_volumes}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|c|c|c|c}
        \toprule
        \textbf{Szene} & \textbf{Modell} & \makecell{\textbf{\O~$\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{\O~$\text{Frag}_{\text{out}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{out}}$~[MB]}} \\
        \midrule
        \multirow{4}{*}{V1} 
            & NeuralRecon  & \multirow{4}{*}{1.60} & 1.83 & \multirow{4}{*}{19.57} & 44.04 \\
            & VisFusion    &                       & 2.73 &                        & 65.40 \\
            & MASt3R-SLAM  &                       & 1.60 &                        & 38.42 \\
            & SLAM3R       &                       & 1.60 &                        & 38.42 \\
        \midrule
        \multirow{4}{*}{V2} 
            & NeuralRecon  & \multirow{4}{*}{1.62} & 2.84 & \multirow{4}{*}{69.57} & 122.28 \\
            & VisFusion    &                      & 3.25  &                        & 139.85 \\
            & MASt3R-SLAM  &                      & 1.60  &                        & 68.60 \\
            & SLAM3R       &                      & 1.60  &                        & 68.84 \\
        \midrule
        \multirow{4}{*}{V3} 
            & NeuralRecon  & \multirow{4}{*}{1.56} & 3.60 & \multirow{4}{*}{88.70} & 204.93 \\
            & VisFusion    &                      & 4.24  &                        & 241.51 \\
            & MASt3R-SLAM  &                      & 1.59  &                        & 90.69 \\
            & SLAM3R       &                      & 1.60  &                        & 91.26 \\
        \midrule
        \multirow{4}{*}{R1} 
            & NeuralRecon  & \multirow{4}{*}{1.63} & 2.19 & \multirow{4}{*}{78.50} & 105.29 \\
            & VisFusion    &                       & 2.58 &                       & 124.14 \\
            & MASt3R-SLAM  &                       & 1.60 &                       & 76.85 \\
            & SLAM3R       &                       & 1.60 &                       & 76.85 \\
        \midrule
        \multirow{4}{*}{R2} 
            & NeuralRecon  & \multirow{4}{*}{1.84} & 2.48 & \multirow{4}{*}{79.33} & 106.87 \\
            & VisFusion    &                       & 3.70 &                        & 159.19 \\
            & MASt3R-SLAM  &                       & 1.60 &                        & 68.84 \\
            & SLAM3R       &                       & 1.60 &                        & 68.84 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

Die Tabelle zeigt, dass die durchschnittlichen Fragmentgrößen für alle Modelle nahezu identisch sind (1{,}56--1{,}84~MB), da alle Modelle dieselbe Fenstergröße verwenden. Die geringfügigen Unterschiede zwischen den Szenen resultieren aus der variierenden JPEG-Komprimierung in Abhängigkeit vom Bildinhalt.

Die durchschnittlichen Ergebnisgrößen pro Fragment unterscheiden sich hingegen deutlicher zwischen den Modellen: VisFusion erzeugt mit $2{,}58$--$4{,}24~\text{MB}$ die größten Meshes im GLB-Format, während NeuralRecon mit $1{,}83$--$3{,}60~\text{MB}$ ähnlich große Meshes produziert. Die punktbasierten Modelle (MASt3R-SLAM, SLAM3R) liefern mit ihren genau 100,000 Punkten konstant ca. $1{,}60~\text{MB}$ zurück. Die Summe der genutzten Bandbreite über alle Fragmente variiert entsprechend der unterschiedlichen Fragmentanzahl pro Szene, wobei beispielsweise in Szene V3 insgesamt 88{,}70~Mbps für den Upload und zwischen 90{,}69~Mbps (MASt3R-SLAM) und 241{,}51~Mbps (VisFusion) für den Download gemessen wurden.

Die präsentierten Latenzmessungen bilden zusammen mit den Datenvolumina die Grundlage für die Bewertung der Echtzeitfähigkeit und Skalierbarkeit des Systems in Abschnitt~\ref{subsec:realtime}.

\subsubsection{Durchsatz}

Der Durchsatz quantifiziert die Verarbeitungsgeschwindigkeit des Systems und wird als Quotient aus der Anzahl verarbeiteter Fragmente pro Szene und der mittleren End-to-End-Latenz \(L_{\text{total}}\) über alle drei Testläufe berechnet. Diese Metrik gibt an, wie viele Fragmente pro Sekunde durch das System verarbeitet werden können, und bestimmt damit unmittelbar die Aktualisierungsrate der Rekonstruktion im VR-Frontend. Tabelle~\ref{tab:throughput_results} fasst die gemessenen Durchsatzwerte für alle Modelle und Szenen zusammen.

\begin{table}[h]
    \centering
    \caption{Durchsatz nach Modell (Fragmente pro Sekunde)}
    \label{tab:throughput_results}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Modell} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        NeuralRecon     & 0.62 & 0.45 & 0.32 &  0.48 & 0.45 \\
        VisFusion       & 0.38 & 0.35 & 0.30 &  0.38 & 0.31 \\
        SLAM3R          & 0.13 & 0.12 & 0.12 &  0.12 & 0.12 \\
        MASt3R-SLAM     & 0.07 & 0.06 & 0.03 &  0.10 & 0.09 \\
        \bottomrule
    \end{tabular}
    \bigskip
\end{table}

Die volumetrischen Verfahren \textit{NeuralRecon} und \textit{VisFusion} erreichen in allen Szenen höhere Durchsatzraten als die SLAM-basierten Ansätze. Bei den virtuellen Szenen sinkt der Durchsatz von V1 zu V3 für alle Modelle: \textit{NeuralRecon} von 0{,}62 auf 0{,}32 fps, \textit{VisFusion} von 0{,}38 auf 0{,}30 fps, \textit{SLAM3R} konstant bei etwa 0{,}12-0{,}13 fps und \textit{MASt3R-SLAM} von 0{,}07 auf 0{,}03 fps.  

In den realen Szenen R1 und R2 liegen die Durchsätze bei allen Modellen höher als in der komplexesten virtuellen Szene V3. So erreicht \textit{NeuralRecon} 0{,}48-0{,}45 fps, \textit{VisFusion} 0{,}38-0{,}31 fps, \textit{SLAM3R} 0{,}12 fps in beiden Szenen und \textit{MASt3R-SLAM} 0{,}10 bzw. 0{,}09 fps.

\subsubsection{Ressourcenauslastung}

Die Ressourcenauslastung wurde sowohl für das Backend (Server-seitige Rekonstruktion) 
als auch für das Frontend (VR-Client-seitige Visualisierung) getrennt erfasst. Diese 
Trennung ermöglicht die Identifikation von Engpässen in der Pipeline und gibt 
Aufschluss darüber, welche Systemkomponente limitierend wirkt.

\paragraph{Backend-Ressourcen}

Die Backend-Ressourcenauslastung wurde kontinuierlich während der Rekonstruktionsläufe 
auf dem dedizierten Server (siehe~\ref{tab:hardware_and_software}) erfasst. Die 
Messungen umfassen GPU-Auslastung und GPU-Speicherverbrauch sowie CPU- und RAM-Nutzung der 
containerisierten Komponenten.

\textbf{GPU-Ressourcen:}  
Tabelle~\ref{tab:gpu_resources} zeigt die durchschnittliche GPU-Auslastung sowie den maximalen GPU-Speicherverbrauch während der Rekonstruktion für alle Modelle und Testszenen.

\begin{table}[H]
    \centering
    \caption{GPU-Auslastung und GPU-Speicherverbrauch während der Rekonstruktion}
    \label{tab:gpu_resources}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Modell} & \textbf{Metrik} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        \multirow{2}{*}{NeuralRecon} 
            & \O GPU-Utilization [\%] & \textit{44} & \textit{43} & \textit{47} & \textit{47} & \textit{48} \\
            & Max. VRAM [MB]          & \textit{3825} & \textit{4770} & \textit{7993} & \textit{6368} & \textit{5986} \\
        \midrule
        \multirow{2}{*}{VisFusion} 
            & \O GPU-Utilization [\%] & \textit{49} & \textit{53} & \textit{55} & \textit{56} & \textit{52} \\
            & Max. VRAM [MB]          & \textit{4877} & \textit{4599} & \textit{4418} & \textit{4406} & \textit{5984} \\
        \midrule
        \multirow{2}{*}{MASt3R-SLAM} 
            & \O GPU-Utilization [\%] & \textit{100} & \textit{100} & \textit{100} & \textit{100} & \textit{100} \\
            & Max. VRAM [MB]          & \textit{7969} & \textit{8016} & \textit{7984} & \textit{8014} & \textit{7966} \\
        \midrule
        \multirow{2}{*}{SLAM3R} 
            & \O GPU-Utilization [\%] & \textit{98.5} & \textit{99} & \textit{99} & \textit{98} & \textit{99} \\
            & Max. VRAM [MB]          & \textit{8022} & \textit{7986} & \textit{8012} & \textit{7637} & \textit{7921} \\
        \bottomrule
    \end{tabular}
\end{table}

Die GPU-Auslastung unterscheidet sich deutlich zwischen den Modellklassen. \textit{NeuralRecon} und \textit{VisFusion} erreichen in allen Szenen moderate Auslastungen im Bereich von 43\,\% bis 56\,\%, während \textit{MASt3R-SLAM} und \textit{SLAM3R} durchgehend sehr hohe Werte zwischen 98\,\% und 100\,\% aufweisen.  

Beim maximalen GPU-Speicherverbrauch zeigt \textit{NeuralRecon} einen deutlichen Anstieg in der virtuellen Szene V3 mit 7993\,MB, während die Werte in den realen Szenen zwischen 5986 und 6368\,MB liegen. \textit{VisFusion} weist insgesamt geringere Maximalwerte auf (4418-5984\,MB) und zeigt über die Szenen hinweg geringere Schwankungen. Die SLAM-basierten Verfahren \textit{MASt3R-SLAM} und \textit{SLAM3R} erreichen über alle Szenen hinweg konstant hohe Speicherverbräuche im Bereich von etwa 7{,}9-8{,}0\,GB.

\textbf{CPU- und RAM-Auslastung:}  
Tabelle~\ref{tab:container_resources} fasst die durchschnittliche CPU-Last und den RAM-Verbrauch der Router- und Worker-Container zusammen, gemittelt über alle Testszenen. Der maximale Wert der CPU Auslastung ist 800\%, da die Evaluation auf einem 8-Kern Prozessor durchgeführt wurde.

\begin{table}[H]
    \centering
    \caption{Durchschnittliche CPU- und RAM-Auslastung der Backend-Container}
    \label{tab:container_resources}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Container} & \textbf{\O CPU-Auslastung [\%]} & \textbf{\O RAM-Verbrauch [MB]} \\
        \midrule
        Router                    & \textit{20} & \textit{100} \\
        \midrule
        Worker: NeuralRecon       & \textit{250} & \textit{1841} \\
        Worker: VisFusion         & \textit{100} & \textit{1820} \\
        Worker: SLAM3R            & \textit{100} & \textit{1916} \\
        Worker: MASt3R-SLAM       & \textit{100} & \textit{2813} \\
        \bottomrule
    \end{tabular}
\end{table}

Der Router-Container weist eine geringe CPU-Auslastung von 20\% sowie einen niedrigen RAM-Verbrauch von 100~MB auf. Die Worker-Container zeigen im Vergleich dazu deutlich höhere Ressourcenverbräuche, wobei \textit{NeuralRecon} mit 250\% die höchste durchschnittliche CPU-Auslastung erreicht. Dies entspricht einer Nutzung von etwa drei CPU-Kernen. Die übrigen Worker-Container (\textit{VisFusion}, \textit{SLAM3R} und \textit{MASt3R-SLAM}) liegen mit jeweils 100\% CPU-Auslastung in einem ähnlichen Bereich, was etwa einem vollständig ausgelasteten Kern entspricht.

Der RAM-Verbrauch der Worker-Container unterscheidet sich ebenfalls. Während \textit{VisFusion} mit 1820,MB den geringsten Speicherverbrauch aufweist, liegen \textit{NeuralRecon} und \textit{SLAM3R} mit 1841,MB bzw. 1916,MB in einem ähnlichen Bereich. \textit{MASt3R-SLAM} zeigt mit 2813,MB den höchsten RAM-Verbrauch unter allen Worker-Containern.

\paragraph{Frontend-Ressourcen}

Die durchschnittliche Frame Rate des Unity-Clients wurde sowohl im Baseline-Betrieb ohne aktive Rekonstruktion als auch während der Rekonstruktions- und Visualisierungsphase gemessen (Abbildung~\ref{frame_rate}). Im Baseline-Betrieb erreicht der Client eine durchschnittliche Bildrate von 72\,fps.  

Während der Rekonstruktion bleibt die Frame Rate bei Verwendung von \textit{NeuralRecon} unverändert bei 72\,fps. Bei \textit{VisFusion} sinkt die durchschnittliche Bildrate auf 58\,fps. Die Verwendung der SLAM-basierten Verfahren \textit{SLAM3R} und \textit{MASt3R-SLAM} führt zu einem stärkeren Rückgang der Frame Rate auf jeweils 38\,fps aufgrund der Punktwolken Repräsentation.

\begin{figure}[H]
    \centering
    \caption{FPS des Unity-Clients im Baseline-Betrieb und während der Rekonstruktion}
    \label{frame_rate}
    \includegraphics[width=0.65\textwidth]{images/frontend_fps.png}
\end{figure}

\subsection{Rekonstruktionsqualität}
Die Rekonstruktionsqualität wird zunächst anhand der synthetischen Szenen V1--V3 mit 
Ground-Truth-Daten quantitativ und qualitativ bewertet. Anschließend erfolgt eine 
Untersuchung der Praxistauglichkeit unter realen VR-Bedingungen anhand der mit der 
Meta Quest 3 aufgenommenen Szenen R1 und R2.    

\subsubsection{Quantitative Bewertung}

Tabelle~\ref{tab:fscore_all} zeigt die F-Score-Ergebnisse für alle Szenen mit verfügbarem Ground-Truth für einen Schwellenwert von \textit{10}cm.

Die Berechnung erfolgte in CloudCompare mittels Cloud-to-Cloud Distance: Punkte mit 
Abstand \(\leq 10\)\,cm zum nächsten Ground-Truth-Punkt wurden als True Positives 
klassifiziert, größere Abstände als False Positives. False Negatives wurden durch 
umgekehrte Distanzberechnung ermittelt. Da SLAM3R und MASt3R-SLAM keine globale 
Konsistenz gewährleisten, wurden deren Rekonstruktionen zuvor mittels ICP~\cite{Besl1992AMF} ausgerichtet.

\begin{table}[H]
    \centering
    \caption{F-Score-Ergebnisse nach Modell und Szene}
    \label{tab:fscore_all}
    \begin{tabular}{lccc|ccc|ccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{V1}} & \multicolumn{3}{c}{\textbf{V2}} & \multicolumn{3}{c}{\textbf{V3}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        \textbf{Modell} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F} \\
        \midrule
        NeuralRecon     & \textit{0.45} & \textit{0.39} & \textit{0.41} & \textit{0.69} & \textit{0.67} & \textit{0.68} & \textit{0.59} & \textit{0.59} & \textit{0.59} \\
        VisFusion       & \textit{0.57} & \textit{0.52} & \textit{0.54} & \textit{0.57} & \textit{0.65} & \textit{0.61} & \textit{0.57} & \textit{0.68} & \textit{0.62} \\
        SLAM3R          & \textit{0.66} & \textit{0.56} & \textit{0.61} & \textit{0.67} & \textit{0.59} & \textit{0.63} & \textit{0.41} & \textit{0.43} & \textit{0.42} \\
        MASt3R-SLAM     & \textit{0.52} & \textit{0.48} & \textit{0.50} & \textit{0.47} & \textit{0.53} & \textit{0.50} & \textit{0.43} & \textit{0.48} & \textit{0.45} \\
        \bottomrule
    \end{tabular}
\end{table}

Die Ergebnisse zeigen deutliche Unterschiede zwischen den Modellen und Szenen. In Szene V1 erreichte SLAM3R mit einer Precision von \(0{,}66\), einer Recall von \(0{,}56\) und einem F-Score von \(0{,}61\) die höchsten Werte über alle Metriken hinweg. VisFusion folgte mit einem F-Score von \(0{,}54\), während NeuralRecon mit \(0{,}41\) das schwächste Ergebnis in dieser Szene erzielte. MASt3R-SLAM erreichte einen F-Score von \(0{,}50\) und positionierte sich im mittleren Bereich. 

Für Szene V2 verschob sich die Rangfolge deutlich. NeuralRecon erzielte hier mit einer Precision von \(0{,}69\), einer Recall von \(0{,}67\) und einem F-Score von \(0{,}68\) die besten Werte. SLAM3R folgte mit einem F-Score von \(0{,}63\), während VisFusion \(0{,}61\) erreichte. MASt3R-SLAM blieb mit einem F-Score von \(0{,}50\) konstant auf dem Niveau der ersten Szene. 

In der komplexesten Szene V3 dominierte VisFusion mit einem F-Score von \(0{,}62\) sowie der höchsten Recall von \(0{,}68\). NeuralRecon erreichte einen F-Score von \(0{,}59\) und die höchste Precision dieser Szene mit \(0{,}59\). SLAM3R fiel auf einen F-Score von \(0{,}42\) zurück, während MASt3R-SLAM mit \(0{,}45\) einen leichten Rückgang gegenüber den vorherigen Szenen aufwies. Über alle drei virtuellen Szenen hinweg betrachtet ergeben sich mittlere F-Scores von \(0{,}56\) für NeuralRecon, \(0{,}59\) für VisFusion, \(0{,}55\) für SLAM3R und \(0{,}48\) für MASt3R-SLAM. 

\subsubsection{Qualitative Bewertung}

\paragraph{NeuralRecon}

Die Rekonstruktionen von NeuralRecon zeigten deutliche Lücken über alle Testszenarien hinweg. Besonders in Szene V1 wurden schräge Flächen vollständig ausgelassen, während horizontale und vertikale Strukturen zumindest teilweise erfasst wurden. In den komplexeren Szenen V2 und V3 setzte sich diese Lückenhaftigkeit fort. Neben den fehlenden Bereichen fielen auch vereinfachte Geometrien auf - feine Details gingen verloren, während gröbere Strukturen noch erkennbar blieben. Positiv hervorzuheben ist hingegen die Qualität der tatsächlich erfassten Bereiche: Die Meshes wiesen glatte, geschlossene Oberflächen auf und waren frei von Fehlgeometrien oder Flimmerartefakten. Die globale Koherenz blieb durchgängig erhalten.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{Ground Truth}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/neucon00.png}
        \caption{NeuralRecon Rekonstruktion}
    \end{subfigure}
    \caption{Lückenhafte Rekonstruktion von Szene V1 durch NeuralRecon. Schräge Flächen werden nicht erfasst, während horizontale und vertikale Strukturen teilweise rekonstruiert werden.}
    \label{fig:neuralrecon_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/gt00.png}
        \caption{Ground Truth Detail}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/neucon00.png}
        \caption{NeuralRecon Detail}
    \end{subfigure}
    \caption{Detailverlust bei NeuralRecon. Objekte wie die Stehlampe oder der TV-Schrank in Szene V2 gehen in der Rekonstruktion größtenteils verloren.}
    \label{fig:neuralrecon_detail}
\end{figure}

\paragraph{VisFusion}

VisFusion überzeugte mit einer spürbar höheren Vollständigkeit als NeuralRecon. Schräge Flächen in Szene V1, die zuvor komplett fehlten, wurden nun erfolgreich rekonstruiert, und insgesamt blieben weniger Bereiche lückenhaft. Die F-Scores stiegen über die Szenen hinweg kontinuierlich an (V1: 0.54, V2: 0.61, V3: 0.62), was auf eine robuste Performance auch bei zunehmender Komplexität hindeutet. Auch im Detail zeigte sich eine Verbesserung: Feinere Strukturen blieben besser erhalten, während gröbere Geometrien ähnlich zuverlässig erfasst wurden wie bei NeuralRecon. Die Meshes waren durchweg glatt, artefaktfrei und wiesen eine höhere geometrische Präzision in komplexen Bereichen auf. Die globale Konsistenz blieb über alle Szenen hinweg ebenfalls stabil.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/neucon00.png}
        \caption{Neural Recon}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/visfusion00.png}
        \caption{VisFusion}
    \end{subfigure}
    \caption{Vergleich zwischen NeuralRecon und VisFusion in Szene V1. VisFusion rekonstruiert die schrägen Flächen erfolgreich und zeigt eine deutlich höhere Vollständigkeit.}
    \label{fig:visfusion_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/visfusion00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/visfusion.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{VisFusion Rekonstruktionen der Szenen V2 und V3. Die Performance bleibt auch bei zunehmender Komplexität stabil. Objekte wie die Stehlampe oder der TV-Schrank werden detaillierter erfasst.}
    \label{fig:visfusion_scenes}
\end{figure}

\paragraph{SLAM3R}

SLAM3R verfolgte einen grundlegend anderen Ansatz und erzeugte dichte, farbige Punktwolken anstelle von Meshes. Bereits in der einfachen Szene V1 (F-Score: 0.66) zeigten sich deutliche Drift-Probleme, die zu Versätzen zwischen Rekonstruktionsabschnitten führten. Im Vergleich zu MASt3R-SLAM waren diese Probleme jedoch noch moderater ausgeprägt. Lokale Details blieben gut erhalten, und die realistische Farbdarstellung trug zur visuellen Qualität bei. In Szene V2 (F-Score: 0.67) verstärkten sich die Konsistenzprobleme bereits merklich, während das Modell der komplexen Szene V3 vollständig einbrach (F-Score: 0.42). Hier akkumulierten sich Ungenauigkeiten massiv, das Verfahren verlor häufig den Anschluss zwischen aufeinanderfolgenden Rekonstruktionsabschnitten, was zu ausgeprägten Versätzen und Brüchen im Gesamtmodell führte. Die fehlende globale Ausrichtung machte eine manuelle Registrierung der Rekonstruktionen notwendig, um sie mit dem Ground Truth abzugleichen. Große Bereiche blieben fragmentiert und inkonsistent, die globale Kohärenz ging vollständig verloren.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{SLAM3R Rekonstruktion}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/slam3r00.png}
        \caption{SLAM3R Rekonstruktion}
    \end{subfigure}
    \caption{SLAM3R Rekonstruktion von Szene V1 mit erkennbaren Drift-Problemen. Trotz Versätzen bleiben lokale Details und Farbinformation gut erhalten.}
    \label{fig:slam3r_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/slam3r00.png}
        \caption{Szene V1}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/slam3r00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/slam3r.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{SLAM3R Performance-Vergleich über Szenen: V1, V2 und V3. Deutlich erkennbar ist die zunehmende Fragmentierung und der Verlust globaler Kohärenz mit steigender Komplexität.}
    \label{fig:slam3r_scenes_progression}
\end{figure}


\paragraph{MASt3R-SLAM}

MASt3R-SLAM erzeugte ebenfalls Punktwolken und zeigte über alle Szenen hinweg konstante F-Scores (V1: 0.50, V2: 0.50, V3: 0.45). In der einfachen Szene V1 war das Verfahren jedoch SLAM3R deutlich unterlegen: Der Drift war stärker ausgeprägt, und die globale Kohärenz litt bereits in dieser unkomplizierten Umgebung. Auch Fragmente traten bereits in V1 häufiger auf als bei SLAM3R und beeinträchtigten die visuelle Qualität der Rekonstruktion. Mit zunehmender Szenenkomplexität zeigte sich jedoch ein anderes Bild: Während SLAM3R in V2 und besonders in V3 stark einbrach, blieb MASt3R-SLAM relativ stabil. In den komplexeren Szenen V2 und V3 übertraf es SLAM3R sowohl in der räumlichen Konsistenz als auch in der globalen Ausrichtung. Die Fragmentierung durch frei schwebende Punkte war zwar weiterhin vorhanden und trug zur Unübersichtlichkeit bei, jedoch deutlich moderater als bei SLAM3R in diesen Szenen. Die rekonstruierten Abschnitte fügten sich besser zu einem Gesamtbild zusammen, auch wenn eine manuelle Registrierung weiterhin notwendig war, um die Rekonstruktionen mit dem Ground Truth abzugleichen. Die lokale Detailtreue mit Farbinformationen war durchgängig gegeben und die Performance blieb über die Szenen hinweg bemerkenswert konsistent, während SLAM3R zunehmend instabiler wurde.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/gt00.png}
        \caption{Ground Truth}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/mast3r0.png}
        \caption{MASt3R-SLAM Rekonstruktion}
    \end{subfigure}
    \caption{MASt3R-SLAM Rekonstruktion von Szene V1 mit ausgeprägten Drift-Problemen. Die Versätze und frei schwebenden Punkte sind hier deutlich stärker als bei SLAM3R in derselben Szene.}
    \label{fig:mast3r_v1}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room00/mast3r0.png}
        \caption{Szene V1}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room01/mast3r00.png}
        \caption{Szene V2}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\linewidth]{images/EvaluationScreenshots/room02/mast3r.png}
        \caption{Szene V3}
    \end{subfigure}
    \caption{MASt3R-SLAM Performance-Vergleich über Szenen: V1 (links), V2 (Mitte), V3 (rechts). Im Gegensatz zu SLAM3R bleibt die Konsistenz über die Szenen hinweg relativ stabil.}
    \label{fig:mast3r_scenes_progression}
\end{figure}


\paragraph{Vergleichende Bewertung}
Tabelle \ref{tab:qualitative_comparison} fasst die qualitativen Beobachtungen zusammen und bewertet die Verfahren anhand einer fünfstufigen Skala (sehr gering, gering, mittel, hoch, sehr hoch) in Bezug auf die definierten Qualitätskriterien. Diese Bewertung erfolgte durch vergleichende visuelle Inspektion der Rekonstruktionen und ermöglicht eine differenzierte Einordnung der Stärken und Schwächen der einzelnen Verfahren.

\begin{table}[H]
    \centering
    \caption{Qualitative Bewertung der Rekonstruktionsverfahren im Vergleich}
    \label{tab:qualitative_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Kriterium} & \textbf{NeuralRecon} & \textbf{VisFusion} & \textbf{SLAM3R} & \textbf{MASt3R-SLAM} \\
        \midrule
        Vollständigkeit & mittel & hoch & hoch & hoch \\
        Detailtreue & gering & mittel & sehr hoch & sehr hoch \\
        Artefaktfreiheit & sehr hoch & sehr hoch & gering & gering \\
        Oberflächenqualität & sehr hoch & sehr hoch & mittel & mittel \\
        \bottomrule
    \end{tabular}
    \medskip
\end{table}

\subsubsection{Rekonstruktion unter realen VR-Bedingungen}

Während die virtuellen Szenen V1--V3 die Rekonstruktionsqualität unter kontrollierten Bedingungen mit synthetischen, hochauflösenden Kameras evaluieren, dienen die realen Szenen R1 (Schlafzimmer) und R2 (Wohnzimmer) der praxisnahen Validierung des Systems unter authentischen VR-Bedingungen mit der Meta Quest 3.

\paragraph{Charakteristika der Quest 3-Passthrough-Kamera}
Bei der Evaluation des Systems zeigten die für das Tracking optimierten Pass-Through-Kameras der Quest 3 mehrere hardwarebedingte Eigenschaften, die sich von denen synthetischer Kameras unterscheiden. Auffällig war der geringe Dynamikumfang, der insbesondere bei stark variierenden Lichtverhältnissen zu Detailverlusten in hellen und dunklen Bildbereichen führte. Rolling-Shutter-Effekte traten bei schnellen Kopfbewegungen deutlich hervor und verursachten sichtbare Verzerrungen. Zudem wurde bei schwacher Beleuchtung verstärktes Bildrauschen beobachtet, während der insgesamt reduzierte Kontrast die Bildqualität zusätzlich beeinträchtigte.

\paragraph{Beobachtungen NeuralRecon}
In den realen Szenen R1 und R2 wies NeuralRecon die geringste Rekonstruktionsqualität aller evaluierten Modelle auf. Die erzeugten Rekonstruktionen wiesen erhebliche Lücken in der Oberfläche auf, und feinere geometrische Details wurden größtenteils nicht erfasst. Die Rekonstruktionen beschränkten sich im Wesentlichen auf grobe Approximationen der Szenengeometrie.

\paragraph{Beobachtungen VisFusion}
VisFusion erzielte eine geringfügig bessere Rekonstruktionsqualität als NeuralRecon. 
Die Oberfläche wies weiterhin partielle Lücken auf, es war jedoch eine Erfassung gröberer geometrischer Strukturen erkennbar. Die Vollständigkeit der Rekonstruktion übertraf die von NeuralRecon geringfügig, beschränkte sich jedoch ebenfalls primär auf die Erfassung grober Formmerkmale.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\linewidth]{images/room03_neucon00.png}
    \includegraphics[width=0.49\linewidth]{images/room03_visfusion00.png}
    \caption{Rekonstruktionen der volumetrischen Verfahren in Szene R1. Links: NeuralRecon 
    mit erheblichen Diskontinuitäten. Rechts: VisFusion mit marginal verbesserter Vollständigkeit.}
    \label{fig:real_volumetric}
\end{figure}

\paragraph{Beobachtungen SLAM3R}
In den realen Szenen zeigte SLAM3R eine deutlich höhere Rekonstruktionsqualität als die volumetrischen Verfahren. Die Punktwolken-Repräsentation ermöglichte eine detaillierte Erfassung der Szenengeometrie mit deutlich reduzierter Diskontinuität. Bemerkenswert ist die verbesserte Performance gegenüber den synthetischen Szenen, die sich in reduzierter Artefaktbildung und beschleunigter Konvergenz äußerte.

\paragraph{Beobachtungen MASt3R-SLAM}
MASt3R-SLAM erzielte die höchste Rekonstruktionsqualität in den realen Szenen. Die Rekonstruktionsergebnisse waren qualitativ mit denen von SLAM3R vergleichbar, wiesen jedoch einen höheren Detailgrad sowie eine geringere Artefaktbildung auf. Die Darstellung der Oberflächen zeigte eine sehr geringe Diskontinuität. Analog zu SLAM3R übertraf die Performance in den realen Szenen die Ergebnisse der synthetischen Evaluationen. Insgesamt erreichte MASt3R-SLAM somit die höchste Rekonstruktionsgüte in R1 und R2.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\linewidth]{images/room04_slam3r00.png}
    \includegraphics[width=0.49\linewidth]{images/room04_mast3r00.png}
    \caption{Rekonstruktionen der SLAM-basierten Verfahren in Szene R2. Obwohl beide Modelle einen hohen Detailgrad haben, liegt MASt3R-SLAM vorne (Esstisch, Sofa)}
    \label{fig:real_slam}
\end{figure}

\paragraph{Vergleich synthetische vs. reale Szenen}
Ein Vergleich der synthetischen Szenen V1--V3 mit den realen Szenen R1--R2 zeigt die unterschiedlichen Leistungsmerkmale der verschiedenen Modellklassen. 
Während die volumetrischen Verfahren NeuralRecon und VisFusion in den synthetischen Szenen robuste und vollständige Rekonstruktionen generierten, zeigten sie in den realen Szenen eine signifikant reduzierte Rekonstruktionsqualität mit ausgeprägten Diskontinuitäten und verlorenem geometrischem Detailgrad. Im Gegensatz dazu wiesen die SLAM-basierten Verfahren SLAM3R und MASt3R-SLAM eine umgekehrte Performance auf. 
Während sie in den synthetischen Szenen teilweise mit Artefaktbildung und verlängerter Inferrenz konfrontiert waren, erzielten sie in den realen Szenen hervorragende Rekonstruktionsergebnisse mit erhöhtem Detailgrad und minimalen Diskontinuitäten. 
MASt3R-SLAM erreichte dabei die höchste Rekonstruktionsgüte in den realen VR-Aufnahmen.

\subsection{Modularität und Systemstabilität}

Die Modularität der entwickelten Architektur wird durch die erfolgreiche Integration der vier heterogenen Rekonstruktionsverfahren NeuralRecon, VisFusion, MASt3R-SLAM und SLAM3R unter Beweis gestellt. In diesem Abschnitt wird der jeweilige Integrationsaufwand bestimmt und die Stabilität des Systems unter realen Bedingungen bewertet.

\subsubsection{Integrationsaufwand}

Die Integration eines neuen Rekonstruktionsmodells in RTReconstruct erfordert drei 
zentrale Implementierungsschritte: die Erstellung eines modellspezifischen Docker-Containers 
mit allen benötigten Abhängigkeiten, die Implementierung einer Worker-Klasse als 
Unterklasse von \texttt{BaseReconstructionModel} sowie die Registrierung des neuen 
Dienstes in der \texttt{docker-compose.yml}. Tabelle~\ref{tab:integration_effort} 
fasst den durchschnittlichen Aufwand pro Modell zusammen.

\begin{table}[H]
\centering
\caption{Integrationsaufwand pro Rekonstruktionsmodell}
\label{tab:integration_effort}
\begin{tabular}{lcccc}
\toprule
\textbf{Komponente} & \textbf{NeuralRecon} & \textbf{VisFusion} & \textbf{MASt3R} & \textbf{SLAM3R} \\
\midrule
Dockerfile (Zeilen)           & 47  & 47  & 37  & 66  \\
Worker-Klasse (Zeilen)        & 205 & 216 & 401 & 282 \\
docker-compose.yml (Zeilen)   & 20  & 21  & 17  & 20  \\
\midrule
\textbf{Gesamt (Zeilen)}      & 272 & 284 & 455 & 368 \\
Geschätzter Zeitaufwand (h)   & 6--8 & 2--3 & 6--8 & 6--8 \\
\bottomrule
\end{tabular}
\end{table}

Der Großteil des Aufwandes für die Implementierung entfällt auf zwei Bereiche: Die Erstellung des Docker-Containers erfordert die korrekte Konfiguration des Base-Images, der CUDA-Versionen, der Python-Abhängigkeiten und der modellspezifischen Bibliotheken. Besonders anspruchsvoll ist dabei die Gewährleistung der Kompatibilität zwischen PyTorch-, CUDA- und Treiber-Versionen, die für jedes Modell individuell erfolgen muss. Die Implementierung der Worker-Klasse umfasst neben der Datenaufbereitung auch die Initialisierung des jeweiligen Rekonstruktionsmodells, das Laden von Modellgewichten und die Transformation der Ausgabe in das standardisierte \texttt{ModelResult}-Format.

Um den Dienst in die \texttt{docker-compose.yml} einzubinden, muss ein neuer Service-Eintrag definiert werden, in dem der Build-Kontext, die GPU-Ressourcen, die Umgebungsvariablen (Modellname, Server-URL) sowie die Netzwerkanbindung an den zentralen Router spezifiziert werden. Dieser Schritt ist weitgehend standardisiert und kann durch Anpassung eines bestehenden Service-Templates erfolgen. Die Basisklasse \texttt{BaseReconstructionModel} stellt die Kommunikationslogik bereit, die den Verbindungsaufbau zum Router, den asynchronen Fragmentempfang und die Ergebnisrücksendung umfasst. Diese Logik erfordert keine modellspezifische Anpassung. Dies reduziert die Komplexität der Integration erheblich.

\subsubsection{Systemstabilität}

Die Stabilität des Systems wurde in Bezug auf die Isolierung von Fehlern einzelner Komponenten untersucht. Dabei wurden drei Aspekte berücksichtigt.

\paragraph{Fehlertoleranz bei fehlerhaften Rekonstruktionen}
Falls es innerhalb eines Worker-Containers Fehler bei der Inferrenz gibt, beispielsweise durch ein fehlerhaftes Eingabefragment oder durch einen Speicherengpass, führt das nicht zu einem vollständigen Absturz des Worker-Containers. Zuerst wird ein Fehlercode an den Router gesendet, damit dieser weiß, dass es zu einem Fehler kam und kein ungültiges Ergebnis an den Client sendet. Danach setzt der Worker-Container seinen internen Zustand zurück um weitere Fehler zu vermeiden und fährt anschließend mit der Rekonstruktion fort.

\paragraph{Verhalten bei Worker-Absturz}
Wenn ein Worker-Container doch abstürzt oder einfach manuell beendet wird, wird dieser automatisch aus der Liste der verfügbaren Modelle entfernt. Der Router verarbeitet weiterhin Anfragen für die verbleibenden Modelle und andere Clients sind nicht betroffen. Durch einen manuellen Neustart des Containers erfolgt eine automatische Wiederanmeldung beim Router, sodass das Modell anschließend erneut verfügbar ist.

\paragraph{Verbindungsabbruch zwischen Frontend und Backend}
Bei einem Verbindungsabbruch der WebSocket-Verbindung zwischen Frontend und Router ist ein Neustart der Unity-Anwendung erforderlich. Ein automatischer Reconnect-Mechanismus wurde nicht implementiert. Dies stellt eine Einschränkung der aktuellen Implementierung dar, die insbesondere bei mobilen VR-Anwendungen mit instabiler Netzwerkverbindung relevant werden kann.

\section{Diskussion}

Ziel dieser Arbeit war die Untersuchung, inwieweit sich eine modulare, containerisierte Systemarchitektur für die Integration verschiedener Echtzeit-3D-Rekonstruktionsverfahren in Virtual-Reality-Umgebungen eignet. Die in Kapitel~\ref{sec:ergebnisse} präsentierten Evaluationsergebnisse werden im Folgenden systematisch interpretiert und hinsichtlich ihrer praktischen Implikationen diskutiert. Die Diskussion gliedert sich in fünf Schwerpunkte: Echtzeitfähigkeit und Performance-Charakteristik, Modularität und Erweiterbarkeit der Architektur, Rekonstruktionsqualität und Generalisierbarkeit, Eignung für VR-Anwendungen sowie die abschließende Beantwortung der Forschungsfrage.

\subsection{Echtzeitfähigkeit und Performance-Charakteristik}
\label{subsec:realtime}

Die Echtzeitfähigkeit des Systems basiert auf der architektonischen Entkopplung von Datenerfassung, Inferenz und Visualisierung. Die Performance-Analyse zeigt, dass das entwickelte System diese Entkopplung erfolgreich umsetzt. Dabei treten deutliche Unterschiede zwischen den integrierten Rekonstruktionsverfahren auf, die hauptsächlich auf deren algorithmische Komplexität zurückzuführen sind.

Diese Unterschiede manifestieren sich zunächst in den gemessenen Durchsatzraten. Die volumetrischen Verfahren NeuralRecon und VisFusion erreichten mit 0,32--0,62 bzw. 0,30--0,38 Fragmenten pro Sekunde (vgl.~\ref{tab:throughput_results}) akzeptable Durchsatzraten. Die SLAM-basierten Verfahren lagen mit 0,03--0,13 Fragmenten pro Sekunde deutlich darunter. Diese Diskrepanz resultiert aus unterschiedlichen Rekonstruktionsparadigmen: Volumetrische Ansätze nutzen effiziente TSDF-Fusion in vordefinierten Voxelgittern~\cite{Curless1996AVM}. SLAM-basierte Verfahren erfordern hingegen rechenintensive Feature-Matching- und Bundle-Adjustment-Operationen über \\ wachsende Keyframe-Mengen~\cite{liu2017robustkeyframebaseddenseslam}.

Trotz dieser Performance-Unterschiede zwischen den Verfahren zeigen die gemessenen Latenzen eine bemerkenswerte Stabilität über alle Testläufe hinweg. Die Boxplot-Darstellungen (vgl.~\ref{fig:latency_boxplots}) offenbaren minimale Interquartilbereiche mit nahezu identischen Medianwerten über drei unabhängige Messungen. Es traten lediglich vereinzelte Ausreißer auf, die auf Garbage-Collection-Events zurückzuführen sind. Diese Konsistenz belegt die robuste Datenstromverarbeitung der asynchronen WebSocket-Architektur. Die Entkopplung verhindert, dass langsamere Modelle die Datenerfassung im Frontend blockieren oder schnellere Modelle auf Eingabedaten warten müssen.

Um die Ursachen der unterschiedlichen Gesamtlatenzen genauer zu verstehen, erfolgte eine Zerlegung der End-to-End-Latenz in ihre Komponenten. Diese Analyse zeigt die Inferenzzeit als limitierenden Faktor (vgl.~\ref{fig:latency_stacked_bar}). Bei NeuralRecon und VisFusion machte $L_{\text{inference}}$ etwa 63--78\% der Gesamtlatenz aus und blieb über alle Szenen hinweg konstant. Bei den SLAM-basierten Verfahren MASt3R-SLAM und SLAM3R dominierte die Inferenz mit ca. 90\% die Gesamtlatenz, während $L_{\text{network}}$ und $L_{\text{render}}$ jeweils knapp unter 10\% lagen.

Im Gegensatz zur variablen Inferenzzeit blieben die Komponenten $L_{\text{network}}$ und $L_{\text{render}}$ über alle Szenen hinweg konstant. Ihre absolute Dauer von typischerweise unter 1000~ms für die gesamte Kommunikations- und Visualisierungspipeline ist bemerkenswert: Sie demonstriert, dass die gewählte Architektur mit binärem Protokoll, WebSocket-Streaming und GPU-beschleunigter Visualisierung einen vernachlässigbaren Overhead verursacht. Die Datenvolumina von 1,60--1,84~MB für Uploads und 1,60--4,24~MB für Downloads pro Fragment (vgl.~\ref{tab:data_volumes}) werden effizient übertragen. Das Chunking-basierte Rendering verzögert die Darstellung von Meshes nicht wahrnehmbar. Die Gesamtperformance des Systems skaliert somit nahezu linear mit der Inferenzgeschwindigkeit der integrierten Modelle -- ein zentrales Designziel modularer Architekturen.

Die inferenzdominierte Performance spiegelt sich auch in der GPU-Ressourcenauslastung wider. MASt3R-SLAM und SLAM3R erreichten eine nahezu vollständige GPU-Auslastung von 98--100\%, während die volumetrischen Verfahren mit 44--56\% deutlich moderater ausgelastet waren (vgl.~\ref{tab:gpu_resources}). Die verwendete NVIDIA GTX 1070 Ti aus dem Jahr 2017 stellt einen Engpass für moderne Transformer-basierte Rekonstruktionsverfahren dar, die für deutlich leistungsfähigere Hardware konzipiert wurden. Entscheidend ist jedoch, dass die geringe Architektur-Latenz eine direkte Skalierbarkeit auf aktuelle GPU-Generationen ermöglicht: Bei Verwendung modernerer Hardware wie einer RTX 4090 würde sich die Inferenzzeit proportional reduzieren, während der System-Overhead konstant niedrig bliebe. Die containerisierte Architektur erlaubt zudem den transparenten Austausch der Backend-Hardware ohne Anpassungen an Frontend oder Kommunikationsprotokoll, was ein wesentlicher Vorteil für den langfristigen Betrieb in sich entwickelnden VR-Infrastrukturen ist.

\subsection{Modularität und Erweiterbarkeit der Architektur}

Die erfolgreiche Integration von vier heterogenen Rekonstruktionsverfahren (vgl.~\ref{tab:model_comparison}) validiert den modularen Architekturansatz als praktikabel. Besonders relevant ist dabei, dass diese Verfahren auf unterschiedlichen PyTorch-Versionen basieren und teils konfliktäre Abhängigkeiten aufweisen, die ohne Containerisierung einen gemeinsamen Betrieb unmöglich machen würden. Die Isolation durch Docker-Container ermöglicht hier eine technische Koexistenz, die in monolithischen Architekturen nicht realisierbar wäre.

Der gemessene Integrationsaufwand von durchschnittlich 47--66 Zeilen Dockerfile und 205--401 Zeilen Worker-Code pro Modell (vgl.\ref{tab:integration_effort}) demonstriert die Effizienz der entwickelten Abstraktionsschicht. Diese Werte sind insofern aussagekräftig, als das sie ausschließlich den integrationsspezifischen Code umfassen, wobei die eigentlichen Rekonstruktionsmodelle unverändert bleiben und lediglich durch eine einheitliche Kommunikationsschicht gekapselt werden. Die geringe Varianz zwischen den Modellen (Dockerfile: $\pm 19$ LoC, Worker: $\pm 196$ LoC) deutet darauf hin, dass die Abstraktion stabil über unterschiedliche Modellarchitekturen hinweg funktioniert. Ein wesentlicher Teil des Worker-Codes entfällt auf modellspezifische Datenvorverarbeitung, wie etwa die Transformation von Unity-Koordinatensystemen in die jeweiligen Modell-Konventionen, oder Inferrenz-Prozesse was als unvermeidlicher Aufwand einzustufen ist der in jeder Integrationsarchitektur anfallen würde.

Die Effizienz der Kommunikationsschicht manifestiert sich in den kompakten Datenvolumina und der geringen Backend-Ressourcenauslastung. Das binäre Protokoll vermeidet den Overhead von ca. 33\%, der durch Base64-Kodierung entstehen würde und erreicht durch JPEG-Komprimierung und GLB-Serialisierung Uploadgrößen von 1,60--1,84~MB und Downloadgrößen von 1,60--4,24~MB pro Fragment (vgl.~\ref{tab:data_volumes}). Die Unterschiede zwischen Modellen reflektieren deren Repräsentationsformen: Volumetrische Verfahren generieren dichte Meshes (2,19--4,24~MB), während SLAM-basierte Ansätze konstant kompaktere Punktwolken (1,60~MB) erzeugen. Der zentrale Router benötigt während der Ausführung lediglich 20\% CPU und 100~MB RAM (vgl.~\ref{tab:container_resources}), was die Effizienz der asynchronen Event-Loop-Architektur unterstreicht. Die Worker-Container weisen moderate CPU-Auslastungen von 100--250\% und einen RAM-Verbrauch von 1,8--2,8~GB auf, der primär durch die Modellgewichte und deren Aktivierungen während der Inferenz bestimmt wird.

Eine zentrale, architekturbedingte Einschränkung ist die fehlende persistente Zustandsverwaltung. Das System wurde für zustandslose Inferenz konzipiert. Jedes Fragment wird unabhängig verarbeitet und bei einem Container-Neustart gehen alle aktuellen Rekonstruktionsdaten verloren. Für produktive VR-Anwendungen, in denen Nutzer zu früheren Rekonstruktionen zurückkehren oder diese inkrementell erweitern möchten, wäre eine Anbindung an persistente Speicherschichten erforderlich. Diese Erweiterung würde die grundlegende Modularität jedoch nicht beeinträchtigen, sondern ließe sich als zusätzliche Backend-Komponente implementieren, die transparent zwischen Router und Workern agiert.

\subsection{Rekonstruktionsqualität und Generalisierbarkeit}

Die vergleichende Evaluation der vier integrierten Rekonstruktionsverfahren demonstriert einen wesentlichen Vorteil der modularen Architektur: Sie ermöglicht die systematische Identifikation von Anwendungsszenarien, für die bestimmte Verfahrensklassen besser geeignet sind. Dabei offenbaren die gemessenen Qualitätsunterschiede zwischen synthetischen und realen VR-Umgebungen weniger fundamentale Limitationen der Algorithmen, sondern primär die Auswirkungen von Hardware- und SDK-Restriktionen auf die Rekonstruktionsqualität. Diese Erkenntnis ist für das Design produktiver VR-Rekonstruktionssysteme sehr relevant.

In den synthetischen Unity-Szenen V1--V3 zeigten volumetrische Verfahren eine konsistente Performance mit F-Scores zwischen 0,41--0,68 (NeuralRecon) und 0,54--0,62 (VisFusion), während SLAM-basierte Ansätze bei zunehmender Szenenkomplexität von 0,50--0,61 auf 0,42--0,45 abfielen (vgl.~\ref{tab:fscore_all}). Die qualitativen Bewertungen ergänzen dieses Bild: Volumetrische Verfahren erzielten hohe Werte bei Artefaktfreiheit und Oberflächenqualität, während SLAM-Verfahren bei Detailtreue überlegen waren, aber unter Konsistenzproblemen litten (vgl. Abbildung~\ref{fig:slam3r_scenes_progression}). Diese Ergebnisse entsprechen den theoretischen Erwartungen und bestätigen die grundsätzliche Funktionsfähigkeit der integrierten Modelle unter idealen Sensorbedingungen mit präzisen Kameraparametern.

Bei der Evaluation unter realen VR-Bedingungen mit der Meta Quest 3 zeigte sich jedoch eine ausgeprägte Leistungsumkehr. Während volumetrische Verfahren fragmentierte Rekonstruktionen mit großflächigen Lücken und erheblichen Diskontinuitäten zeigten (vgl. Abbildung ~\ref{fig:real_volumetric}), erreichten SLAM-basierte Ansätze deutlich höhere Vollständigkeit und Detailgrad (vgl. Abbildung ~\ref{fig:real_slam}). Diese Umkehrung lässt sich auf eine fundamentale Limitation der Quest 3 Passthrough API zurückführen, denn diese liefert keine präzisen Timestamps für die aufgenommenen Kamerabilder. Dadurch ist eine exakte zeitliche Zuordnung zwischen Bildern und den vom Tracking-System bereitgestellten Extrinsiken nicht möglich. Volumetrische Verfahren wie NeuralRecon und VisFusion setzen jedoch voraus, dass die Kamerapose-Daten exakt zum Aufnahmezeitpunkt des jeweiligen Frames gehören. Selbst geringe zeitliche Versätze von wenigen Millisekunden führen bei Kopfbewegungen zu räumlichen Inkonsistenzen, die sich über die gesamte Rekonstruktion ausbreiten und großflächige Artefakte erzeugen. Zusätzlich weisen die für Tracking optimierten Kameras eine signifikant verminderte Bildqualität auf, durch geringer Dynamikumfang, ausgeprägte Rolling-Shutter-Effekte und erhöhtes Bildrauschen, was die die Rekonstruktion weiter erschwert.

SLAM-basierte Verfahren umgehen diese Problematik durch eine fundamental andere Architektur: SLAM3R und MASt3R-SLAM nutzen die externen Extrinsiken überhaupt nicht, sondern schätzen Kameraposen vollständig selbst durch explizite Feature-Matching- und Optimierungsschritte. Diese interne Pose-Estimation ist unabhängig von SDK-Timestamps und kann zeitliche Inkonsistenzen kompensieren, da sie ausschließlich auf der Bildfolge selbst basiert. Die höhere Robustheit gegenüber Sensor-Artefakten erklärt auch die beobachteten längeren Inferenzzeiten (vgl. \ref{tab:throughput_results}), da die iterative Pose-Optimierung rechenintensiver ist als die direkte Nutzung externer Tracking-Daten. Aus Systemperspektive manifestiert sich hier ein klassischer Trade-off: Volumetrische Verfahren sind schneller, benötigen aber präzise Eingabedaten, während SLAM-Verfahren langsamer, aber robuster gegenüber SDK-Limitationen sind.

Diese Beobachtung ist aus Systemperspektive in zweifacher Hinsicht aufschlussreich: Erstens ermöglicht die modulare Architektur des entwickelten Systems überhaupt erst die systematische Identifikation solcher Hardware-Abhängigkeiten, indem verschiedene Rekonstruktionsverfahren unter identischen Bedingungen evaluiert werden können. Die einheitliche Schnittstelle und das gemeinsame Evaluationsframework machen Unterschiede in der Sensor-Sensitivität transparent, die bei isolierter Betrachtung einzelner Algorithmen verborgen blieben.
Zweitens zeigt sich, dass die beobachteten Trade-offs zwischen Inferenzgeschwindigkeit und Robustheit gegenüber Sensor-Artefakten (vgl. Abbildung~\ref{fig:real_volumetric},~\ref{fig:real_slam}) nicht nur algorithmische Eigenschaften widerspiegeln, sondern stark vom Deployment-Kontext abhängen. Die modulare Struktur erlaubt es, diese kontextspezifischen Unterschiede zu quantifizieren: Während volumetrische Verfahren bei präzisen Kamera-Extrinsiken ihre Stärken ausspielen, kompensieren SLAM-basierte Ansätze Ungenauigkeiten in der Pose-Schätzung durch interne Optimierung. Ein Aspekt, der ohne vergleichende Evaluation unter realen Hardware-Bedingungen schwer zu bewerten wäre.

\subsection{Eignung für VR-Anwendungen}

Die Integration von RTReconstruct in das Va.Si.Li-Lab demonstriert die praktische Einsetzbarkeit der entwickelten Architektur in produktiven VR-Umgebungen. Die gewählte Strategie der additiven Integration erwies sich dabei als tragfähig: RTReconstruct wurde als optionaler, zuschaltbarer Dienst in die bestehende Mehrbenutzerinfrastruktur eingebettet, ohne etablierte Szenenlogik oder Interaktionsmechanismen zu verändern. Das implementierte Rollen- und Szenenkonzept (Host/Visitor) ermöglichte eine klare Aufgabentrennung zwischen aktiver Rekonstruktion und passivem Konsum der Ergebnisse, wobei die Mehrbenutzerumgebung auch bei deaktivierter Rekonstruktion vollständig funktionsfähig blieb.

Die Systemstabilität unter realen Nutzungsbedingungen wurde durch umfangreiche Testläufe validiert. In 60 Testdurchgängen über eine Gesamtdauer von 6 Stunden traten 0 Verbindungsabbrüche auf (vgl.~\ref{par:e2e_functional}). Die Multi-Szenen-Unterstützung funktionierte mit 2 parallelen Szenen und 4 verbundenen Clients ohne Funktionseinbußen (vlg.~\ref{par:parallel_execution}), was die Robustheit der asynchronen WebSocket-Architektur und des Fan-Out-Mechanismus auch unter Mehrbenutzer-Last bestätigt. Diese Zuverlässigkeit bildet die Grundlage für den produktiven Einsatz in kollaborativen VR-Szenarien.

Die Visualisierungs-Performance variiert jedoch deutlich zwischen den Repräsentationsformen. Bei der volumetrischen Mesh-Rekonstruktion von \textit{NeuralRecon} blieb die Frame-Rate mit 72 FPS auf Baseline-Niveau. Die implementierte Spatial-Hashing-Visualisierung mit Chunk-Größen von $5 \times 5 \times 5$ Metern ermöglichte hier effizientes Frustum-Culling und sicherte eine flüssige Darstellung auch bei größeren Geometrien. \textit{VisFusion} erreichte hingegen nur 58 FPS, was auf die größere Mesh-Komplexität und höhere Polygon-Anzahl zurückzuführen ist. SLAM-basierte Punktwolken-Rekonstruktionen (\textit{SLAM3R}, \textit{MASt3R-SLAM}) erreichten bei 100.000 dargestellten Punkten konstant 37 FPS. Eine weitere Reduktion der Punktzahl zur Performance-Steigerung erwies sich als nicht praktikabel, da dies zu erheblichem Detailverlust führte und die primäre Stärke der SLAM-Verfahren, ihre hohe geometrische Auflösung, untergraben würde. Die Begrenzung auf 100.000 Punkte stellt damit eine Kompromisslösung dar, die ausreichende Darstellungsqualität bei akzeptabler Performance auf mobiler VR-Hardware gewährleistet.

Eine weitere Herausforderung für den produktiven Einsatz betrifft die globale Registrierung der SLAM-Rekonstruktionen. SLAM3R und MASt3R-SLAM erzeugen Rekonstruktionen in einem modellinternen Koordinatensystem mit lokaler Skalierung, da sie Kameraposen vollständig selbst schätzen und keine externen Referenzen nutzen. Volumetrische Verfahren hingegen verwenden die vom Meta SDK bereitgestellten Extrinsiken direkt für die TSDF-Fusion und rekonstruieren dadurch automatisch im korrekten Maßstab des VR-Tracking-Systems. Um SLAM-Outputs in die Unity-Szene zu integrieren, ist derzeit manuelles Skalieren und Ausrichten durch den Nutzer erforderlich. Dieser Arbeitsschritt unterbricht den automatisierten Workflow und erschwert spontane Rekonstruktionsszenarien, in denen Nutzer unmittelbar mit den erfassten Geometrien interagieren möchten. Die Problematik illustriert eine grundlegende Herausforderung bei der Integration von Rekonstruktionsverfahren, die ihre Posen selbst intern optimieren.

\subsection{Beantwortung der Forschungsfrage}

Die zentrale Forschungsfrage \glqq Wie gut eignet sich eine modulare, containerisierte Systemarchitektur zur Integration verschiedener Echtzeit-3D-Rekonstruktionsverfahren in eine bestehende Virtual-Reality-Umgebung?\grqq kann auf Basis der durchgeführten Evaluation nun differenziert beantwortet werden. Die Architektur erweist sich als gut bis sehr gut geeignet für Forschungs- und Evaluationskontexte, zeigt jedoch auch spezifische Stärken und Limitationen, die von den jeweiligen Deployment-Anforderungen abhängen.

Die modulare Architektur erfüllt ihre Kernziele auf technischer Ebene umfassend. Die erfolgreiche Integration von vier methodisch divergenten Rekonstruktionsverfahren mit unterschiedlichen Framework-Abhängigkeiten und Repräsentationsformen validiert das Containerisierungskonzept als praktikabel (vlg.~\ref{tab:integration_effort}). Die asynchrone WebSocket-Architektur gewährleistet eine robuste, latenzarme Datenstromverarbeitung mit 0 Verbindungsabbrüchen in 6 Stunden Testbetrieb (vgl.~\ref{par:e2e_functional}). Die niedrige Basis-Latenz von typischerweise unter 1000~ms für Netzwerk- und Rendering-Komponenten demonstriert, dass die Gesamtperformance nahezu linear mit der Modell-Inferenzgeschwindigkeit skaliert und erfüllt somit ein zentrales Designziel modularer Architekturen (vgl.~\ref{fig:latency_stacked_bar}). Die Integration in das Va.Si.Li-Lab gelang additiv ohne disruptive Eingriffe, was die Wiederverwendbarkeit für andere VR-Plattformen nahelegt.

Aus wissenschaftlicher Perspektive ermöglichte die Architektur systematische Erkenntnisse, die über isolierte Modell-Benchmarks hinausgehen. Die identifizierte Leistungsumkehr zwischen synthetischen und realen VR-Umgebungen (vgl. Tabelle~\ref{tab:fscore_all}, Abbildung~\ref{fig:real_volumetric} und~\ref{fig:real_slam}) wäre ohne vergleichende Evaluation unter identischen Bedingungen kaum systematisch zu erfassen gewesen. Die Rückführung auf SDK-Limitationen der Meta Quest 3 (fehlende Frame-Timestamps) statt auf inhärente Algorithmen-Schwächen zeigt den Mehrwert kontrollierter Vergleiche. Ebenso offenbarte die Evaluation deployment-spezifische Trade-offs: Volumetrische Verfahren sind schneller (0,30--0,62 Fragmente/s), benötigen aber präzise Extrinsiken; SLAM-Verfahren sind langsamer (0,03--0,13 Fragmente/s), aber robuster gegenüber Sensor-Artefakten (vgl.~\ref{tab:throughput_results}). Die Architektur erlaubt es, solche Entscheidungen kontextspezifisch zu treffen, ohne Code-Anpassungen vornehmen zu müssen.
\clearpage

Limitationen bestehen primär in drei Bereichen. Erstens erreichen SLAM-Verfahren auf der verwendeten NVIDIA GTX 1070 Ti eine GPU-Auslastung von 98--100\% bei vollständiger VRAM-Auslastung (8~GB), was den parallelen Betrieb mehrerer Modelle einschränkt (vgl.~\ref{tab:gpu_resources}). Dieser Engpass ist hardware-spezifisch und würde sich mit moderneren GPUs deutlich reduzieren, unterstreicht aber die Relevanz der Architektur. Diese ermöglicht nämlich die direkte Skalierung auf leistungsfähigere Hardware ohne Code-Anpassungen. Zweitens fehlt eine persistente Zustandsverwaltung, sodass Container-Neustarts zum Datenverlust führen. Drittens erfordert die Integration von SLAM-Rekonstruktionen manuelle Skalierung durch Nutzer, da diese in modellinternen Koordinatensystemen vorliegen. Diese Limitationen sind nicht architekturinhärent, sondern reflektieren Hardware-Constraints der Evaluationsumgebung, Implementierungsentscheidungen und eine fehlende Standardisierung zwischen Rekonstruktionsverfahren und VR-Tracking-Systemen.

Zusammenfassend zeigt sich, dass sich die entwickelte Architektur sehr gut als flexible Integrationsplattform für Forschungs- und Evaluationszwecke eignet. Sie ermöglicht den transparenten Austausch von Rekonstruktionsverfahren, die Identifikation deployment-spezifischer Eignung und die Generierung vergleichender Erkenntnisse unter kontrollierten Bedingungen. Für produktive VR-Anwendungen mit Echtzeitanforderungen empfiehlt sich eine kontextabhängige Verfahrenswahl: volumetrische Ansätze für synthetische Umgebungen oder präzise Kamera-Hardware bzw. SLAM-basierte Ansätze für Consumer-VR mit SDK-Limitationen. Die Modularität des Systems stellt sicher, dass solche Entscheidungen deployment-spezifisch getroffen werden können, ohne dass eine grundlegende Anpassung der Frontend- oder Backend-Implementierung erforderlich ist. Dies ist ein zentrales Erfolgskriterium für langfristig wartbare VR-Forschungsinfrastrukturen.