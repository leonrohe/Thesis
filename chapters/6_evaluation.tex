\chapter{Evaluation}
\label{chap:evaluation} % Hinzugefügt, falls andere Kapitel darauf verweisen

Dieses Kapitel evaluiert das entwickelte RTReconstruct-System hinsichtlich seiner 
Echtzeitfähigkeit, Modularität und Rekonstruktionsqualität. Zunächst werden die 
Evaluationsziele definiert und die Testumgebung beschrieben. Anschließend erfolgt 
die Darstellung der gemessenen Performance-Metriken sowie der Rekonstruktionsqualität 
der integrierten Modelle. Das Kapitel schließt mit einer Diskussion der Ergebnisse 
im Kontext der definierten Anforderungen.

\section{Evaluationsziele}
\label{sec:eval_ziele}

Die Evaluation verfolgt drei zentrale Ziele, die direkt aus der in Abschnitt formulierten Forschungsfrage abgeleitet sind:

\begin{enumerate}
    \item \textbf{Funktionale Validierung}: Nachweis, dass die modulare Architektur 
    die definierten funktionalen Anforderungen erfüllt. Insbesondere wird geprüft, 
    ob verschiedene Rekonstruktionsmodelle parallel betrieben werden können und ob 
    die End-to-End Kommunikation zwischen VR-Frontend und Backend stabil funktioniert.
    
    \item \textbf{Echtzeitfähigkeit}: Bewertung, ob das System die für VR-Anwendungen 
    erforderlichen Performance-Anforderungen erfüllt. Dabei werden Latenz, Durchsatz 
    und Ressourcenauslastung als kritische Metriken untersucht.
    
    \item \textbf{Rekonstruktionsqualität}: Qualitative und -- soweit möglich -- 
    quantitative Bewertung der von den integrierten Modellen erzeugten 
    3D-Rekonstruktionen unter identischen Bedingungen.
\end{enumerate}

\section{Evaluationsmethodik}
Die Evaluationsmethodik beschreibt die Testumgebung, die verwendeten Testszenarien und die Messmethoden, die zur Erreichung der Evaluationsziele eingesetzt wurden. 

\subsection{Test- und Evaluationsumgebung}
\label{sec:testumgebung}

Um die \textbf{Reproduzierbarkeit} der Performance-Messungen und die \textbf{Vergleichbarkeit} der erzielten Ergebnisse zu gewährleisten, wurde die gesamte Evaluation in einer dedizierten und \textbf{kontrollierten Hard- und Software-Umgebung} durchgeführt. Die zentralen Komponenten und Spezifikationen dieser Umgebung sind in Tabelle~\ref{tab:hardware_and_software} zusammengefasst.

\begin{table}[H]
    \centering
    \label{tab:hardware_and_software}
    \caption{Spezifikationen der Hard- und Software-Umgebung}
    \begin{tabularx}{\textwidth}{l X}
        \toprule
        \textbf{Kategorie} & \textbf{Details und Spezifikationen} \\
        \midrule
        \multicolumn{2}{l}{\textbf{Hardware-Umgebung (Backend/Server)}} \\
        \midrule
        Backend-Server CPU & AMD Ryzen 9 5900X (12 Kerne, 24 Threads) \\
        GPU & NVIDIA GeForce GTX 1070 Ti, 8 GB VRAM \\
        VR-System (Frontend) & Meta Quest 3 \\
        Netzwerk & WiFi 6 Heimnetzwerk \\
        \midrule
        \multicolumn{2}{l}{\textbf{Software-Umgebung und Frameworks}} \\
        \midrule
        Betriebssystem & Ubuntu 22.04 LTS (Host) \\
        Containerisierung & Docker (\textit{[Version einfügen, z.B. 24.0.7]}) \\
        GPU-Unterstützung & NVIDIA Container Toolkit \\
        VR-Frontend & Unity (\textit{2022.3 LTS}) \\
        \bottomrule
    \end{tabularx}
\end{table}

\noindent
Alle Messungen erfolgten unter \textbf{kontrollierten Bedingungen}. Es wurde strikt darauf geachtet, dass während der Performance-Tests \textbf{keine weiteren rechenintensiven Hintergrundprozesse} liefen, um Verzerrungen zu minimieren.

\subsection{Testszenarien und Datensätze}

Für die Evaluation wurden fünf Testszenen mit unterschiedlichen Komplexitätsstufen 
konzipiert: drei virtuelle Szenen mit verfügbarem Ground-Truth zur quantitativen 
Bewertung sowie zwei reale Szenen aus einem typischen Alltagsumfeld zur Validierung der
Praxistauglichkeit. Eine kurze Übersicht über alle Testszenen findet sich in Tabelle~\ref{tab:test_scenes_overview}.

\begin{table}[H]
    \centering
    \label{tab:test_scenes_overview}
    \begin{tabularx}{\textwidth}{lcclX}
        \toprule
        \textbf{Szene} & \textbf{Größe (m)} & \textbf{Fragments} & \textbf{Frames} & \textbf{Merkmale} \\
        \midrule
        \multicolumn{5}{l}{\textbf{Virtuelle Szenen}} \\
        V1 -- Primitive & $5\times 5\times 5$ & 24 & 216 & Geometrische Grundformen, unifarbige Oberflächen \\
        V2 -- Schlafzimmer & $6\times 5\times 3$ & 44 & 396 & moderate Komplexität, Okklusionen \\
        V3 -- Mehrzweckraum & $10\times 5\times 3$ & 58 & 522 & Hohe Dichte, komplexe Geometrie \\
        \midrule
        \multicolumn{5}{l}{\textbf{Reale Szenen}} \\
        R1 & $4\times 4\times 3$ & [150] & [40] & \textit{Schlafzimmer}, Details, Schrägen, Okklusionen \\
        R2 & $6\times 5\times 3$ & [200] & [50] & \textit{Wohnzimmer}, Glas, Reflexionen, große Flächen \\
        \bottomrule
    \end{tabularx}
    \caption{Übersicht und Klassifikation der Testszenen}
\end{table}

\subsubsection{Virtuelle Szenen}

Die drei virtuellen Szenen wurden in Unity erstellt und ermöglichen durch verfügbare \textit{Ground-Truth-Meshes} eine quantitative Evaluation mittels F-Score. Die Szenen folgen einer progressiven Komplexitätssteigerung, um verschiedene Aspekte der Rekonstruktionsverfahren isoliert zu testen. Abbildung~\ref{fig:virtual_scenes} zeigt eine Übersicht aller drei Szenen.

\paragraph{Szene V1 -- Geometrische Primitive}
Szene V1 dient als Baseline-Test und enthält ausschließlich einfache geometrische Primitive (Quader, Pyramide, Zylinder, Kapsel) in einem hexagonalen Raum mit farbigen Wänden. Die unifarbigen, matten Oberflächen ohne Texturen ermöglichen die isolierte Bewertung fundamentaler Rekonstruktionsfähigkeiten: scharfe Kanten, gekrümmte Oberflächen und feature-arme Flächen.

\paragraph{Szene V2 -- Möbliertes Schlafzimmer}
Szene V2 repräsentiert einen möblierten Innenraum mittlerer Komplexität mit Doppelbett, Sessel, Sideboard, Wandbildern und Stehlampe. Diese Szene testet die Rekonstruktion komplexer Möbelgeometrie, das Verhalten bei Okklusionen, die Texturverarbeitung sowie die Detailerfassung kleiner Dekorationsobjekte.

\paragraph{Szene V3 -- Komplexer Mehrzweckraum}
Szene V3 stellt einen Stresstest für Skalierbarkeit und Detailtreue dar und simuliert einen multifunktionalen Raum mit Schlaf-, Wohn- und Arbeitsbereich. Die hohe Objektdichte mit zwei Betten, Esstisch, Stühlen und diversen Kleinobjekten erzeugt multiple Okklusionsebenen. Erwartet werden längere Inferenzzeiten, höhere GPU-Auslastung und potenzielle Artefakte bei geometrisch komplexen Strukturen und teilweise verdeckten Bereichen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.32\textwidth]{images/room00.png}
    \includegraphics[width=0.32\textwidth]{images/room01.png}
    \includegraphics[width=0.32\textwidth]{images/Room02.png}
    \caption{Übersicht der drei virtuellen Testszenen: V1 (Geometrische Primitive), V2 (Möbliertes Schlafzimmer), V3 (Komplexer Mehrzweckraum)}
    \label{fig:virtual_scenes}
\end{figure}

\subsubsection{Reale Szenen}

Die beiden realen Szenen wurden im Va.Si.Li-Lab aufgenommen und validieren die Praxistauglichkeit des Systems unter realen Bedingungen mit natürlichen Störfaktoren.

\paragraph{Szene R1 -- Schlafzimmer}

Szene R1 simuliert ein kleines, dicht möbliertes, privates Umfeld. Details, Schrägen, Okklusionen. Der Testfokus liegt auf der Robustheit gegenüber Textiloberflächen und diffuser Beleuchtung, welche die Rekonstruktion feiner Details und das Verhalten bei Oberflächenhomogenität überprüfen.

\paragraph{Szene R2 -- Wohnzimmer}

Szene R2 simuliert ein großes Wohnzimmer mit offener Gestaltung. Große Flächen, Glas, Reflexionen. Die Szene dient als Skalierbarkeits- und Materialstresstest. Im Fokus stehen die Handhabung großer, glänzender Flächen und Fenster, die Reflexionen verursachen, sowie repetitive Dekorelemente, welche die globale Konsistenz und Anfälligkeit für visuellen Drift testen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{images/room01.png}
    \includegraphics[width=0.49\textwidth]{images/room01.png}
    \caption{Übersicht der beiden realen Testszenen: R1 (Schlafzimmer), R2 (Wohnzimmer)}
    \label{fig:real_scenes}
\end{figure}

\subsection{Messverfahren und Metriken}

\subsubsection{Performance-Metriken}

\paragraph{Latenz}
Die End-to-End-Latenz misst die Zeitspanne zwischen dem Versenden eines Fragments 
durch das Unity-Frontend und der Visualisierung der aktualisierten Rekonstruktion 
im VR-Headset. Sie setzt sich aus folgenden Komponenten zusammen:
\begin{align}
    L_{\text{total}} = L_{\text{network}} + L_{\text{inference}} + L_{\text{render}}
\end{align}
wobei $L_{\text{network}}$ die Netzwerklatenz (Upload des Fragments und Download 
der Rekonstruktion), $L_{\text{inference}}$ die Modell-Inferenzzeit im Backend 
(GPU-Verarbeitung) und $L_{\text{render}}$ die Rendering-Zeit im Unity-Client 
(GLB-Import und Mesh-Visualisierung) bezeichnet.

\medskip
\noindent
Die Netzwerklatenz $L_{\text{network}} = L_{\text{upload}} + L_{\text{download}}$ 
wird nicht direkt durch Zeitstempel gemessen, sondern aus den erfassten Datenvolumina 
und der verfügbaren Netzwerkbandbreite berechnet: $L_{\text{upload}} = S_{\text{fragment}} / B_{\text{upload}}$ 
bzw. $L_{\text{download}} = S_{\text{result}} / B_{\text{download}}$. Hierbei 
bezeichnet $S_{\text{fragment}}$ die Fragmentgröße (Upload-Volumen pro Fragment) 
und $S_{\text{result}}$ die Resultgröße (Download-Volumen der Rekonstruktion). 
Diese Methodik ermöglicht eine infrastrukturunabhängige Bewertung der Dateneffizienz.

\medskip
\noindent
Die Messung der übrigen Latenzkomponenten erfolgte durch präzise Zeitstempel an 
den jeweiligen Übergangspunkten der Pipeline.

\paragraph{Durchsatz}
Der Durchsatz quantifiziert, wie viele Fragmente pro Sekunde durch das System 
verarbeitet werden können. Ein höherer Durchsatz ermöglicht häufigere Updates der 
Rekonstruktion und trägt zur Immersion bei. Gemessen wurde der Durchsatz auf 
Backend-Seite für jedes Worker-Modell separat.

\paragraph{Ressourcenauslastung}
Die GPU- und CPU-Auslastung wurde kontinuierlich während der Rekonstruktion 
aufgezeichnet. GPU-Utilization und GPU-Memory wurden via \texttt{nvidia-smi} 
erfasst, CPU-Auslastung und RAM-Verbrauch pro Container via Docker Stats. Diese 
Metriken ermöglichen die Bewertung der Ressourceneffizienz und geben Aufschluss 
über Engpässe im System.

\subsubsection{Qualitätsmetriken}

\paragraph{Quantitative Bewertung}
Für Szenen mit verfügbarem Ground-Truth-Mesh wurde der F-Score als kombinierte 
Metrik für Präzision und Recall berechnet:

\begin{align}
    \text{Precision} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FP}|} \\
    \text{Recall} &= \frac{|\text{TP}|}{|\text{TP}| + |\text{FN}|} \\
    \text{F-Score} &= 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{align}

\noindent
Rekonstruierte Punkte werden als True Positive (TP) gezählt, wenn ihr Abstand zum 
Ground-Truth-Mesh unter einem Schwellenwert von \textit{[X]} cm liegt.

\paragraph{Qualitative Bewertung}
Die rekonstruierten Meshes wurden anhand folgender Kriterien bewertet:

\begin{itemize}
    \item \textbf{Vollständigkeit}: Wie viel Prozent der Szene wurde erfasst?
    \item \textbf{Detailtreue}: Sind feine Strukturen erkennbar?
    \item \textbf{Artefaktfreiheit}: Treten Löcher, Flimmern oder Fehlgeometrie auf?
    \item \textbf{Oberflächenqualität}: Glattheit und Konsistenz der Rekonstruktion
\end{itemize}

\noindent
Die Bewertung erfolgte durch visuelle Inspektion der Rekonstruktionen in Unity 
sowie durch exportierte Screenshots.

\section{Ergebnisse}
\label{sec:ergebnisse}

\subsection{Funktionale Validierung}

Die funktionale Validierung bestätigt, dass RTReconstruct alle definierten Kernfunktionalitäten erfüllt.

\paragraph{End-to-End-Kommunikation}
Die vollständige Kommunikationskette von der Fragmenterfassung im Unity-Client über 
die WebSocket-Verbindung zum Router bis zur Verteilung an die Worker-Container und 
zurück funktioniert stabil. In \textit{15} Testläufen über eine Gesamtdauer von \textit{6} Stunden 
traten \textit{0} Verbindungsabbrüche auf.

\paragraph{Parallele Modellausführung}
Alle vier integrierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, 
SLAM3R) konnten gleichzeitig betrieben werden. Die containerisierte Architektur 
ermöglichte eine vollständige Isolierung, sodass unterschiedliche Python- und 
PyTorch-Versionen parallel lauffähig waren. Einzig limitierender Faktor war die einzelne GPU,
die durch die Modelle gemeinsam genutzt wurde.

\paragraph{Multi-Szenen-Unterstützung}
Das System unterstützt die gleichzeitige Verarbeitung mehrerer Szenen. In Tests 
mit \textit{2} parallelen Szenen und \textit{4} verbundenen Clients blieb die Funktionalität 
erhalten. Die szenenspezifische Zuordnung der Rekonstruktionsergebnisse erfolgte 
fehlerfrei.

\paragraph{Visualisierung in VR}
Die über das Backend empfangenen Meshes wurden erfolgreich im Unity-Client 
visualisiert. Das in Kapitel 5 beschriebene Spatial 
Hashing ermöglichte eine performante Darstellung auch bei größeren Meshes und Punktwolken mit bis zu 100.000 Punkten.

\subsection{Performance-Analyse}

\subsubsection{Latenz}

Die Latenz stellt die zentrale Performance-Metrik für die Echtzeitfähigkeit des Systems dar. Im Folgenden wird zunächst die Gesamtlatenz über alle Testszenen und Modelle präsentiert, anschließend in ihre Komponenten zerlegt und abschließend durch die Analyse der Datenvolumina kontextualisiert.

\paragraph{Gesamtlatenz}

Zur Evaluierung der Systemperformance wurde die End-to-End-Latenz \\ \(L_{total}\) als Zeitspanne zwischen dem Absenden eines Fragments vom Client und dem Empfang der zugehörigen Rekonstruktion gemessen. Für jede Kombination aus Testszene und Rekonstruktionsmodell wurden drei unabhängige Testläufe durchgeführt, bei denen identische Eingabedaten verwendet wurden. Um dabei Verzerrungen durch Ressourcenkonflikte zu vermeiden, wurden die Modelle sequenziell im Einzelbetrieb getestet. Abbildung~\ref{fig:latency_boxplots} visualisiert die resultierenden Latenzverteilungen als Boxplots.

\medskip
\noindent
Die Darstellung zeigt auf der x-Achse die vier evaluierten Rekonstruktionsmodelle (NeuralRecon, VisFusion, MASt3R-SLAM, SLAM3R), während die y-Achse die gemessene Latenz in Millisekunden angibt. Pro Modell sind drei Boxplots dargestellt, die jeweils die Latenzverteilung eines Testlaufs repräsentieren. Die farbliche Kodierung kennzeichnet dabei denselben Testlauf über alle Modelle hinweg.

\medskip
\noindent
Die Boxplots zeigen für alle Modelle und Szenen geringe Interquartilbereiche und minimale Ausreißer, was auf eine stabile und reproduzierbare Latenzcharakteristik des Systems hinweist. Aufgrund dieser geringen Varianz zwischen den drei Testläufen werden in den folgenden Analysen zur Zusammensetzung der Gesamtlatenz sowie zu Datenvolumina die Messwerte der drei Testläufe aggregiert dargestellt. Dies ermöglicht eine kompaktere Präsentation ohne relevanten Informationsverlust.

\begin{figure}[H]
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room00_latency.png}
        \caption{Szene V1 -- Geometrische Primitive}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room01_latency.png}
        \caption{Szene V2 -- Möbliertes Schlafzimmer}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room02_latency.png}
        \caption{Szene V3 -- Komplexer Mehrzweckraum}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{images/room00_latency.png}
        \caption{Szene R1 -- Schlafzimmer}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.49\linewidth]{images/room00_latency.png}
        \caption{Szene R2 -- Wohnzimmer}
    \end{subfigure}
    \caption{Gesamtlatenz \(L_{total}\) für alle Testszenen und Modelle über drei Testläufe. Die Boxen zeigen den Interquartilbereich (25.\,--\,75.\,Perzentil), die horizontale Linie den Median und die Whiskers den Wertebereich ohne Ausreißer.}
    \label{fig:latency_boxplots}
\end{figure}

\medskip
\noindent
Die Abbildung zeigt deutliche Unterschiede in der Gesamtlatenz zwischen den Testszenen: Szene V1 weist die niedrigsten Latenzwerte auf, während die Latenz in den komplexeren Szenen V3, R1 und R2 ansteigt. Zudem variiert die Latenz zwischen den Modellen, wobei MAST3R durchgängig die höchsten Werte erreicht. Um die Ursachen dieser Variation zu identifizieren, wird die Gesamtlatenz im Folgenden in ihre Komponenten zerlegt.


\paragraph{Zusammensetzung der Gesamtlatenz}

Die beobachteten Latenzunterschiede zwischen den Testszenen lassen sich durch die Zerlegung der Gesamtlatenz in ihre konstituierenden Komponenten \(L_{\text{network}}\), \(L_{\text{inference}}\) und \(L_{\text{render}}\) analysieren. Diese Aufschlüsselung ermöglicht es, szenenabhängige Effekte auf die Inferenzzeit von fixen Overhead-Kosten der Netzwerkkommunikation und Rendering-Pipeline zu separieren. Abbildung~\ref{fig:latency_stacked_bar} visualisiert die resultierende Zusammensetzung für alle Modelle und Testszenen.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/latency_split.png}
    \caption{Zusammensetzung der Gesamtlatenz nach Komponenten für alle Rekonstruktionsmodelle, aufgeschlüsselt nach Testszene und gemittelt über die drei Testläufe.}
    \label{fig:latency_stacked_bar}
\end{figure}

\medskip
\noindent
Die Aufschlüsselung zeigt, dass \(L_{\text{inference}}\) den dominierenden Anteil der Gesamtlatenz ausmacht und zwischen den Szenen stark variiert. Der Anteil von \(L_{\text{network}}\) und \(L_{\text{render}}\) bleibt über die Szenen hinweg relativ konstant, nimmt jedoch prozentual mit steigender Szenenkomplexität ab. Die Netzwerklatenz \(L_{\text{network}}\) wird dabei maßgeblich durch die Größe der übertragenen Daten bestimmt, deren Quantifizierung im Folgenden dargestellt wird.


\paragraph{Fragment- und Ergebnisgrößen}

Um die Netzwerklatenz \(L_{\text{network}}\) zu kontextualisieren und die Bandbreitenanforderungen des Systems zu dokumentieren, wurden die durchschnittlichen Fragmentgrößen (Upload) und Ergebnisgrößen (Download) für alle Modelle gemessen. Tabelle~\ref{tab:data_volumes} zeigt die resultierenden Datenvolumina sowie die daraus berechnete genutzte Bandbreite.

\begin{table}[h]
    \centering
    \caption{Durchschnittliche Datenvolumina und genutzte Bandbreite aufgeschlüsselt nach Szene und Modell (gemittelt über drei Testläufe).}
    \label{tab:data_volumes}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|l|c|c|c|c}
        \toprule
        \textbf{Szene} & \textbf{Modell} & \makecell{\textbf{\O~$\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{\O~$\text{Frag}_{\text{out}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{in}}$~[MB]}} & \makecell{\textbf{$\sum\text{Frag}_{\text{out}}$~[MB]}} \\
        \midrule
        \multirow{4}{*}{V1} 
            & NeuralRecon  & \multirow{4}{*}{0.82} & 1.83 & \multirow{4}{*}{19.57} & 44.04 \\
            & VisFusion    &                       & 2.73 &                        & 65.40 \\
            & MASt3R-SLAM  &                       & 1.60 &                        & 38.42 \\
            & SLAM3R       &                       & 1.60 &                        & 38.42 \\
        \midrule
        \multirow{4}{*}{V2} 
            & NeuralRecon  & \multirow{4}{*}{1.62} & 2.84 & \multirow{4}{*}{69.57} & 122.28 \\
            & VisFusion    &                      & 3.25  &                        & 139.85 \\
            & MASt3R-SLAM  &                      & 1.60  &                        & 68.60 \\
            & SLAM3R       &                      & 1.60  &                        & 68.84 \\
        \midrule
        \multirow{4}{*}{V3} 
            & NeuralRecon  & \multirow{4}{*}{1.56} & 3.60 & \multirow{4}{*}{88.70} & 204.93 \\
            & VisFusion    &                      & 4.24  &                        & 241.51 \\
            & MASt3R-SLAM  &                      & 1.59  &                        & 90.69 \\
            & SLAM3R       &                      & 1.60  &                        & 91.26 \\
        \midrule
        \multirow{4}{*}{R1} 
            & NeuralRecon  & \multirow{4}{*}{X.X} & Y.Y & \multirow{4}{*}{Z.Z} & A.A \\
            & VisFusion    &                      & Y.Y &                      & A.A \\
            & MASt3R-SLAM  &                      & Y.Y &                      & A.A \\
            & SLAM3R       &                      & Y.Y &                      & A.A \\
        \midrule
        \multirow{4}{*}{R2} 
            & NeuralRecon  & \multirow{4}{*}{X.X} & Y.Y & \multirow{4}{*}{Z.Z} & A.A \\
            & VisFusion    &                      & Y.Y &                      & A.A \\
            & MASt3R-SLAM  &                      & Y.Y &                      & A.A \\
            & SLAM3R       &                      & Y.Y &                      & A.A \\
        \bottomrule
    \end{tabular}%
    }
\end{table}


\medskip
\noindent
Die Tabelle zeigt, dass die Fragmentgrößen zwischen den Modellen variieren, wobei SLAM3R aufgrund seiner größeren Fenstergrößen die umfangreichsten Fragmente benötigt. Die Ergebnisgrößen unterscheiden sich ebenfalls deutlich: Volumetrische Verfahren (NeuralRecon, VisFusion) erzeugen größere Meshes im GLB-Format, während punktbasierte Modelle (MASt3R-SLAM, SLAM3R) kompaktere Punktwolken zurückliefern. Die Summe der genutzten Bandbreite liegt bei X.X~Mbps für den Upload und Y.Y~Mbps für den Download.

\medskip
\noindent
Die präsentierten Latenzmessungen bilden zusammen mit den Datenvolumina die Grundlage für die Bewertung der Echtzeitfähigkeit und Skalierbarkeit des Systems in Abschnitt~\ref{sec:diskussion}.

\subsubsection{Durchsatz}

Der Durchsatz ergibt sich für diese Evaluation aus der Anzahl der verarbeiteten Fragmente pro Szene, geteilt durch die mittlere End-To-End Latenz \(L_{total}\) über alle 3 Testläufe. Diese Metrik gibt an, wie viele Fragmente pro Sekunde durch das System verarbeitet werden können und ist ein Indikator für die Aktualisierungsrate der Rekonstruktion im VR-Frontend. Die folgende Tabelle \ref{tab:throughput_results} fasst diese gemessenen Durchsatzwerte für alle Modelle und Szenen zusammen.

\medskip
\begin{table}[h]
    \centering
    \label{tab:throughput_results}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Modell} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        NeuralRecon     & 0.62 & 0.45 & 0.32 &  &  \\
        VisFusion       & 0.38 & 0.35 & 0.30 &  &  \\
        MASt3R-SLAM     & 0.13 & 0.12 & 0.12 &  &  \\
        SLAM3R          & 0.07 & 0.06 & 0.03 &  &  \\
        \bottomrule
    \end{tabular}
    \bigskip
    \caption{Durchsatz nach Modell (Fragmente pro Sekunde)}
\end{table}
\newpage

\subsubsection{Ressourcenauslastung}

Die Ressourcenauslastung wurde sowohl für das Backend (Server-seitige Rekonstruktion) 
als auch für das Frontend (VR-Client-seitige Visualisierung) getrennt erfasst. Diese 
Trennung ermöglicht die Identifikation von Engpässen in der Pipeline und gibt 
Aufschluss darüber, welche Systemkomponente limitierend wirkt.

\paragraph{Backend-Ressourcen}

Die Backend-Ressourcenauslastung wurde kontinuierlich während der Rekonstruktionsläufe 
auf dem dedizierten Server (AMD Ryzen 9 5900X, NVIDIA GTX 1070 Ti) erfasst. Die 
Messungen umfassen GPU-Auslastung und GPU-Speicherverbrauch (erfasst mittels 
\texttt{nvidia-smi} in 1-Sekunden-Intervallen) sowie CPU- und RAM-Nutzung der 
containerisierten Komponenten (erfasst mittels \texttt{docker stats}).

\medskip
\noindent
\textbf{GPU-Ressourcen:} Tabelle~\ref{tab:gpu_resources} zeigt die durchschnittliche 
GPU-Utilization und den maximalen GPU-Speicherverbrauch während der Rekonstruktion, 
aufgeschlüsselt nach Modell und Testszene.

\begin{table}[h]
    \centering
    \caption{GPU-Auslastung und GPU-Speicherverbrauch während der Rekonstruktion}
    \label{tab:gpu_resources}
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Modell} & \textbf{Metrik} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2}\\
        \midrule
        \multirow{2}{*}{NeuralRecon} 
            & \O GPU-Utilization [\%] & \textit{44} & \textit{43} & \textit{47} & \textit{[X]} & \textit{[X]} \\
            & Max. VRAM [MB]          & \textit{3825} & \textit{4770} & \textit{7993} & \textit{[X.X]} & \textit{[X.X]} \\
        \midrule
        \multirow{2}{*}{VisFusion} 
            & \O GPU-Utilization [\%] & \textit{49} & \textit{53} & \textit{55} & \textit{[X]} & \textit{[X]} \\
            & Max. VRAM [MB]          & \textit{4877} & \textit{4599} & \textit{4418} & \textit{[X.X]} & \textit{[X.X]} \\
        \midrule
        \multirow{2}{*}{MASt3R-SLAM} 
            & \O GPU-Utilization [\%] & \textit{100} & \textit{100} & \textit{100} & \textit{[X]} & \textit{[X]} \\
            & Max. VRAM [MB]          & \textit{7969} & \textit{8016} & \textit{7984} & \textit{[X.X]} & \textit{[X.X]} \\
        \midrule
        \multirow{2}{*}{SLAM3R} 
            & \O GPU-Utilization [\%] & \textit{98.5} & \textit{99} & \textit{99} & \textit{[X]} & \textit{[X]} \\
            & Max. VRAM [MB]          & \textit{8022} & \textit{7986} & \textit{8012} & \textit{[X.X]} & \textit{[X.X]} \\
        \bottomrule
    \end{tabular}
\end{table}

\medskip
\noindent
\textbf{CPU- und RAM-Auslastung:} Die Ressourcennutzung der einzelnen Docker-Container 
ist in Tabelle~\ref{tab:container_resources} dargestellt. Die Werte zeigen die 
durchschnittliche CPU-Last und den RAM-Verbrauch für Router- und Worker-Container, 
gemittelt über alle Testszenen.

\begin{table}[h]
    \centering
    \caption{Durchschnittliche CPU- und RAM-Auslastung der Backend-Container}
    \label{tab:container_resources}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Container} & \textbf{\O CPU-Auslastung [\%]} & \textbf{\O RAM-Verbrauch [MB]} \\
        \midrule
        Router                    & \textit{0.2} & \textit{200} \\
        \midrule
        Worker: NeuralRecon       & \textit{32} & \textit{2780} \\
        Worker: VisFusion         & \textit{17.25} & \textit{3015} \\
        Worker: MASt3R-SLAM       & \textit{14.35} & \textit{3466} \\
        Worker: SLAM3R            & \textit{15.37} & \textit{4070} \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Frontend-Ressourcen}

Die Frontend-Performance wurde auf dem Meta Quest 3 VR-Headset gemessen. Die 
durchschnittliche Frame Rate des Unity-Clients wurde sowohl im Baseline-Betrieb 
(ohne aktive Rekonstruktion) als auch während der Rekonstruktions- und 
Visualisierungsphase gemessen. Tabelle~\ref{tab:frame_rate} zeigt die Ergebnisse 
sowie die Differenz zwischen beiden Modi.

\begin{table}[h]
    \centering
    \caption{Frame Rate-Einbußen durch Rekonstruktion nach Modell [fps]}
    \label{tab:frame_rate}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Modell} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{R1} & \textbf{R2} \\
        \midrule
        NeuralRecon    & \textit{72}~fps & \textit{72}~fps & \textit{72}~fps & \textit{72}~fps & \textit{72}~fps \\
        VisFusion      & \textit{68}~fps & \textit{68}~fps & \textit{68}~fps & \textit{68}~fps & \textit{68}~fps \\
        SLAM3R         & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps \\
        MASt3R-SLAM    & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps & \textit{37}~fps \\
        \midrule
        \textit{Baseline (alle Modelle)} & \multicolumn{5}{c}{72 fps} \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Rekonstruktionsqualität}

\subsubsection{Quantitative Bewertung}

Für Szene \textit{[Name]} mit verfügbarem Ground-Truth wurde der F-Score für alle Modelle 
berechnet. Tabelle \ref{tab:fscore_results} zeigt die Ergebnisse.

\begin{table}[h]
    \centering
    \caption{F-Score-Ergebnisse für Szene [Name] (Schwellenwert: [X] cm)}
    \label{tab:fscore_results}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Modell} & \textbf{Precision} & \textbf{Recall} & \textbf{F-Score} \\
        \midrule
        NeuralRecon     & \textit{[X]} & \textit{[Y]} & \textit{[Z]} \\
        VisFusion       & \textit{[X]} & \textit{[Y]} & \textit{[Z]} \\
        MASt3R-SLAM     & \textit{[X]} & \textit{[Y]} & \textit{[Z]} \\
        SLAM3R          & \textit{[X]} & \textit{[Y]} & \textit{[Z]} \\
        \bottomrule
    \end{tabular}
\end{table}

\noindent
\textit{[Modellname]} erreichte den höchsten F-Score von \textit{[X]}, was auf \textit{[Erklärung]} 
zurückzuführen ist. Im Vergleich dazu zeigte \textit{[anderes Modell]} eine höhere 
Precision bei niedrigerem Recall, was für \textit{[Interpretation]} spricht.

\subsubsection{Qualitative Bewertung}

Die visuelle Inspektion der Rekonstruktionen ergab folgende Beobachtungen:

\paragraph{NeuralRecon}
Die Rekonstruktion zeigte \textit{[Beschreibung: z.B. robuste Gesamtmodelle mit glatter 
Oberflächenqualität]}. Abbildung \textbf{[REF: fig:neuralrecon\_result]} zeigt exemplarisch 
die Rekonstruktion von Szene \textit{[X]}. \textit{[Stärken und Schwächen beschreiben]}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/neuralrecon_scene1.png}
%     \caption{Rekonstruktion von Szene [Name] mit NeuralRecon}
%     \label{fig:neuralrecon_result}
% \end{figure}

\paragraph{VisFusion}
Im Vergleich zu NeuralRecon zeigte VisFusion \textit{[Unterschiede: z.B. feinere Details 
in texturierten Bereichen]}. Besonders in Bereichen mit \textit{[Merkmal wie Okklusionen]} 
war \textit{[Beobachtung wie bessere Oberflächenkonsistenz]} erkennbar.

\paragraph{MASt3R-SLAM}
Das SLAM-basierte Verfahren lieferte \textit{[Charakteristik: z.B. detaillierte 
Punktwolken mit hoher lokaler Genauigkeit]}. Während \textit{[positive Aspekte wie 
Echtzeitfähigkeit]}, traten vereinzelt \textit{[negative Aspekte wie Drift]} auf.

\paragraph{SLAM3R}
SLAM3R zeichnete sich durch \textit{[Merkmale wie dichte Punktwolken]} aus. Die 
Rekonstruktion war \textit{[Bewertung: z.B. konsistent, aber mit geringerer Vollständigkeit 
als volumetrische Verfahren]}.

\paragraph{Vergleichende Bewertung}
Tabelle \ref{tab:qualitative_comparison} fasst die qualitativen Beobachtungen 
zusammen.

\begin{table}[h]
    \centering
    \caption{Qualitative Bewertung der Rekonstruktionsmodelle}
    \label{tab:qualitative_comparison}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Kriterium} & \textbf{NeuralRecon} & \textbf{VisFusion} & \textbf{MASt3R} & \textbf{SLAM3R} \\
        \midrule
        Vollständigkeit    & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} \\
        Detailtreue        & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} \\
        Artefaktfreiheit   & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} \\
        Oberflächenqualität & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} & \textit{[+/o/-]} \\
        \bottomrule
    \end{tabular}
    \begin{flushleft}
        \footnotesize Legende: + = gut, o = mittel, - = schwach
    \end{flushleft}
\end{table}

\subsection{Modularität und Systemstabilität}

\subsubsection{Integrationsaufwand neuer Modelle}

Um die Modularität der Architektur zu bewerten, wurde ein \textit{[fünftes Modell/Mock-Modell]} 
in das System integriert. Der Integrationsaufwand umfasste:

\begin{itemize}
    \item Erstellung eines Dockerfile: \textit{[X]} Zeilen Code
    \item Implementierung des Worker-Interfaces: \textit{[Y]} Zeilen Code
    \item Anpassung der docker-compose.yml: \textit{[Z]} Zeilen
    \item Gesamtdauer: \textit{[A]} Stunden
\end{itemize}

\noindent
Dies bestätigt, dass die einheitliche Schnittstelle den Integrationsaufwand 
erheblich reduziert.

\subsubsection{Stabilitätstests}

Das System wurde unter folgenden Fehlerszenarien getestet:

\paragraph{Container-Neustart während Rekonstruktion}
Ein Worker-Container wurde manuell gestoppt und nach \textit{[X]} Sekunden neu gestartet. 
Das System erkannte den Ausfall und \textit{[Beschreibung des Verhaltens]}. Die 
Wiederherstellungszeit betrug \textit{[Y]} Sekunden.

\paragraph{Netzwerkunterbrechung}
Die WiFi-Verbindung wurde für \textit{[X]} Sekunden unterbrochen. Das Frontend behielt 
das letzte Mesh \textit{[bei/zeigte Fehlermeldung]}. Nach Wiederherstellung der Verbindung 
konnte die Rekonstruktion \textit{[fortgesetzt/musste neu gestartet]} werden.

\paragraph{Gleichzeitige Last}
Bei parallelem Betrieb von \textit{[X]} Clients und \textit{[Y]} Szenen blieb die Performance 
\textit{[stabil/verschlechterte sich um Z\%]}. Die Latenz stieg auf durchschnittlich \textit{[A]} ms.

\section{Diskussion}
\label{sec:diskussion}

\subsection{Echtzeitfähigkeit}

Die gemessenen Latenzen von \textit{[Bereich]} zeigen, dass der Hauptanteil der Verzögerung 
auf die Modell-Inferenz entfällt (\textit{[X]}\%). Die WebSocket-Kommunikation und 
Container-Architektur verursachen nur einen geringen Overhead von \textit{[Y]}\%. Dies 
bestätigt, dass die gewählte Architektur keine signifikanten Performance-Einbußen 
gegenüber monolithischen Systemen mit sich bringt.

Die Unterschiede zwischen den Modellen lassen sich auf ihre unterschiedlichen 
Rekonstruktionsparadigmen zurückführen: Volumetrische Verfahren wie 
\textit{[NeuralRecon/VisFusion]} erzielen \textit{[schnellere/langsamere]} Inferenzzeiten aufgrund 
\textit{[Begründung: z.B. ihrer TSDF-basierten Fusion]}, während SLAM-basierte Ansätze 
\textit{[Charakteristik: z.B. durch Feature-Matching höhere Latenzen]} aufweisen.

Der gemessene Durchsatz von \textit{[X]} Fragmenten pro Sekunde ermöglicht \textit{[häufige/seltene]} 
Updates der Rekonstruktion. Die Frame Rate im Unity-Client von durchschnittlich 
\textit{[Y]} fps liegt \textit{[im akzeptablen/grenzwertigen]} Bereich für VR-Anwendungen.

\subsection{Modularität der Architektur}

Die erfolgreiche parallele Ausführung aller vier Modelle sowie der geringe 
Integrationsaufwand für neue Modelle (\textit{[X]} Stunden) belegen die hohe Modularität 
des Systems. Die containerisierte Architektur ermöglicht die Isolierung 
unterschiedlicher Laufzeitumgebungen, was die Erweiterbarkeit erheblich vereinfacht.

Die Stabilitätstests zeigen, dass das System \textit{[Szenario A wie Container-Neustarts]} 
robust handhabt. Bei \textit{[Szenario B wie Netzwerkunterbrechungen]} zeigten sich jedoch 
\textit{[Schwächen wie Datenverlust]}, die für produktive Einsätze adressiert werden müssten.

\subsection{Rekonstruktionsqualität im Vergleich}

Der Vergleich der Modelle zeigt erwartungsgemäß unterschiedliche Stärken: 
Volumetrische Verfahren lieferten \textit{[Charakteristik wie robuste Gesamtmodelle mit 
hoher Vollständigkeit]}, während SLAM-basierte Ansätze \textit{[Charakteristik wie feinere 
lokale Details bei geringerer globaler Konsistenz]} aufwiesen. Dies entspricht den 
in Kapitel \textbf{[REF: chap:stand\_der\_technik]} beschriebenen methodischen Unterschieden.

Die quantitativen Ergebnisse zeigen, dass \textit{[Modellname]} mit einem F-Score von \textit{[X]} 
die beste geometrische Genauigkeit erreicht, während \textit{[anderes Modell]} bei \textit{[Kriterium]} 
überzeugt. Die qualitative Bewertung unterstreicht, dass \textit{[Modellname]} für Szenarien 
mit \textit{[Merkmal]} besonders geeignet ist, während \textit{[anderes Modell]} in \textit{[anderen Szenarien]} 
Vorteile bietet.

\subsection{Eignung für VR-Anwendungen}

Die Evaluationsergebnisse zeigen, dass RTReconstruct die definierten funktionalen 
Anforderungen erfüllt und für interaktive VR-Anwendungen \textit{[grundsätzlich geeignet/
mit Einschränkungen nutzbar]} ist. Die Latenzwerte von \textit{[Bereich]} liegen \textit{[im/knapp 
über dem]} für VR akzeptablen Bereich. Die stabile Kommunikation und die erfolgreiche 
Multi-Szenen-Unterstützung bestätigen die Praxistauglichkeit der Architektur für 
das Va.Si.Li-Lab.

Die modulare Struktur ermöglicht es, je nach Anwendungsfall das optimale 
Rekonstruktionsverfahren auszuwählen: \textit{[Szenario A]} profitiert von \textit{[Modellname]}, 
während \textit{[Szenario B]} mit \textit{[anderem Modell]} bessere Ergebnisse erzielt.