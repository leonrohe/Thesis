\chapter{Stand der Technik}

Dieses Kapitel verortet die Arbeit im Forschungsstand der echtzeitfähigen 3D-Rekonstruktion
und untersucht aktuelle Entwicklungen sowohl auf System- als auch auf Modellebene.
Es werden zunächst bestehende End-to-End-Pipelines betrachtet, die eine vollständige Rekonstruktion
vom Kameraeingang bis zur Visualisierung in Echtzeit umsetzen. 
Anschließend werden die in dieser Arbeit integrierten Modelle — NeuralRecon, VisFusion, 
MASt3R-SLAM und SLAM3R — im Detail beschrieben, begründet und in die entwickelte Systemarchitektur eingeordnet.

\section{Einleitung und Abgrenzung}

Während Kapitel~2 die theoretischen Grundlagen der 3D-Rekonstruktion erläuterte,
konzentriert sich dieses Kapitel auf den aktuellen Stand der Forschung und Entwicklung.
Der Fokus liegt dabei auf Verfahren, die monokulare oder RGB-D-Eingaben in Echtzeit
verarbeiten können und sich für den Einsatz in interaktiven VR-Umgebungen eignen.
Besonderes Augenmerk gilt Arbeiten, die eine kontinuierliche Verarbeitung von Videoströmen
ermöglichen und Ansätze zur modularen oder containerisierten Integration verfolgen.
Damit bildet dieses Kapitel die Brücke zwischen den methodischen Grundlagen und 
der in Kapitel~4 vorgestellten Systemkonzeption.

\section{End-to-End-Rekonstruktionspipelines}

Frühe Echtzeit-Rekonstruktionssysteme wie \textbf{KinectFusion} und \textbf{ElasticFusion}
zeigten erstmals, dass sich Tiefendaten fortlaufend in einem volumetrischen Speicher
akkumulieren lassen, wodurch eine dichte, kamerageführte 3D-Rekonstruktion möglich wird.
Diese Systeme basieren auf klassischen SLAM-Prinzipien, bei denen die Kamerapose in
Echtzeit geschätzt und neue Tiefeninformationen in ein globales TSDF-Volumen integriert werden.
Trotz ihrer Robustheit bleiben sie auf spezielle Sensoren (z.~B.~RGB-D-Kameras) beschränkt
und sind nur eingeschränkt auf monokulare Eingangsdaten übertragbar.

Mit dem Aufkommen neuronaler Verfahren entstanden in den letzten Jahren mehrere
\textbf{End-to-End-Systeme}, die Bildaufnahme, Kameratracking und volumetrische oder
strahlbasierte Rekonstruktion in einem gemeinsamen Netzwerk integrieren.
Beispiele hierfür sind \textbf{NeuralFusion}, \textbf{NICE-SLAM} und \textbf{Co-SLAM}.
Diese Systeme kombinieren dichte Feature-Extraktion, Lern-basierte Tiefenschätzung
und SLAM-Optimierung innerhalb einer einheitlichen Pipeline und erreichen
eine bislang nicht gekannte Konsistenz zwischen lokalen und globalen Karten.
Sie verarbeiten kontinuierliche Kamerastreams und erzeugen fortlaufend aktualisierte
3D-Darstellungen, die sich bereits während der Bewegung des Nutzers rekonstruieren lassen.

Ein früher praxisorientierter Ansatz zur direkten Integration von Rekonstruktionssoftware
in Game-Engines wurde 2017 von \textit{NVIDIA} und \textit{Capturing Reality} vorgestellt~\cite{gdc-photogrammetry-2017}.
Dabei wurde gezeigt, wie sich das Photogrammetrie-Tool \textit{RealityCapture} über ein C++/C\#-SDK
direkt in den Unity-Editor einbinden lässt, um aus gewöhnlichen Kamerabildern in wenigen Minuten
texturierte 3D-Modelle zu erzeugen.
Dieses Konzept einer \emph{Image-to-Unity}-Pipeline markierte einen wichtigen Zwischenschritt
zwischen offline-orientierter Photogrammetrie und modernen, echtzeitfähigen Rekonstruktionssystemen,
die Bildaufnahme, GPU-Verarbeitung und Visualisierung zunehmend nahtlos miteinander verbinden.

Gleichzeitig weisen solche Systeme strukturelle Einschränkungen auf:
Sie sind \emph{monolithisch} konzipiert, das heißt Erfassung, Verarbeitung und Ausgabe
sind eng miteinander gekoppelt und kaum austauschbar.
Dies erschwert die Integration in bestehende VR- oder Multi-Modell-Umgebungen,
da Kommunikation und Laufzeitumgebung fest in die jeweilige Architektur eingebettet sind.

\section{Ausgewählte Rekonstruktionsverfahren}

\subsection{Volumetrische Verfahren}

\textbf{NeuralRecon} gilt als Meilenstein für die kohärente, echtzeitfähige Rekonstruktion
aus monokularen Videosequenzen. Es nutzt ein rekurrentes, voxelbasiertes Netz,
das lokale Tiefenschätzungen in einem TSDF-Volumen zusammenführt.
Durch die sequentielle Feature-Fusion entlang von Videofragmenten wird die zeitliche
Konsistenz der Rekonstruktion deutlich verbessert.
Das Modell verarbeitet Fenster von Bildern mit bekannten Kameraparametern und
konzentriert die Rechenleistung auf aktive Volumenregionen, wodurch es sich besonders
für Streaming-Pipelines eignet.

\textbf{VisFusion} erweitert diesen Ansatz um eine explizite Modellierung der Sichtbarkeiten
zwischen mehreren Ansichten. Dadurch werden Okklusionen und feingranulare Oberflächen
besser berücksichtigt, während die Echtzeitfähigkeit erhalten bleibt.
Durch eine residuale TSDF-Vorhersage über mehrere Skalen und strahlbasierte Fokussierung
aktiver Regionen erzielt VisFusion eine höhere Detailtreue und Konvergenzstabilität,
was es zu einem geeigneten Vergleichsmodell für NeuralRecon macht.

\subsection{SLAM-basierte Verfahren}

\textbf{MASt3R-SLAM} kombiniert klassische dichte SLAM-Verfahren mit neuronalen 3D-Priors
aus dem MASt3R-Modell. Dadurch werden sowohl Tracking als auch Mapping stabilisiert
und durch GPU-Beschleunigung in Echtzeit ermöglicht.
Die Pipeline integriert lokale Optimierung mit globaler Konsistenzprüfung und nutzt
retrievalbasierte Mechanismen zur Erkennung von Schleifenschlüssen.
MASt3R-SLAM eignet sich insbesondere für kontinuierliche Bewegungen in VR-Umgebungen
und steht methodisch zwischen rein volumetrischen und rein strahlbasierten Ansätzen.

\subsection{Strahlbasierte Verfahren}

\textbf{SLAM3R} kombiniert die Idee von Neural Radiance Fields (NeRF) mit SLAM-Mechanismen,
um photometrisch konsistente, dichte Rekonstruktionen zu erzeugen.
Durch die Modellierung von Strahlen im 3D-Raum werden neuartige Sichten und feine
Strukturdetails ermöglicht, während die SLAM-Kopplung die Kameraposen stabilisiert.
SLAM3R repräsentiert damit die jüngste Generation strahlbasierter Echtzeitverfahren
und steht für die Verschmelzung von geometrischen und neuralen Repräsentationen.

\section{Begründung der Modellwahl}

Die Auswahl der vier Modelle NeuralRecon, VisFusion, MASt3R-SLAM und SLAM3R
erfolgt mit dem Ziel, die drei dominanten Paradigmen der Echtzeit-3D-Rekonstruktion
— volumetrisch, SLAM-basiert und strahlbasiert — innerhalb einer einheitlichen Systemarchitektur
vergleichend zu untersuchen.
Alle Modelle sind Open-Source verfügbar, GPU-beschleunigt und für fragmentbasierte
Echtzeitverarbeitung geeignet, was ihre Integration in containerisierte Worker-Module ermöglicht.
Durch diese Auswahl kann die Leistungsfähigkeit des modularen Ansatzes
unter identischen Bedingungen hinsichtlich Latenz, Genauigkeit und Vollständigkeit evaluiert werden.

\section{Zwischenfazit}

Der aktuelle Stand der Technik zeigt eine deutliche Entwicklung von klassischen,
sensorgestützten SLAM-Verfahren hin zu lernbasierten End-to-End-Systemen,
die zunehmend in der Lage sind, 3D-Rekonstruktionen aus monokularen RGB-Streams
in nahezu Echtzeit zu erzeugen.
Trotz dieser Fortschritte bleiben bestehende Ansätze häufig monolithisch aufgebaut
und lassen sich nur schwer in externe Systeme integrieren.
Hier setzt das im Rahmen dieser Arbeit entwickelte \textit{RTReconstruct}-System an,
das auf einer containerisierten, modular erweiterbaren Architektur basiert und
die parallele Integration heterogener Rekonstruktionsmodelle ermöglicht.
Das folgende Kapitel beschreibt die konzeptionelle Ausgestaltung dieser Architektur.