\chapter{Stand der Technik}

Während Kapitel~2 die theoretischen und technischen Grundlagen der 3D-Rekonstruktion erläuterte,
rückt dieses Kapitel den aktuellen Forschungsstand VR-tauglicher Rekonstruktionssysteme in den Fokus.
\emph{VR-tauglich} bezeichnet dabei Verfahren, die Kameradaten während der Bewegung fortlaufend verarbeiten
und die Szene in einer laufenden Sitzung inkrementell aktualisieren; gemeint sind Rekonstruktionsupdates,
nicht die Display-Latenz des HMD.
Ziel ist es, etablierte End-to-End-Pipelines sowie ausgewählte Einzelverfahren systematisch einzuordnen
und ihre Relevanz für die in dieser Arbeit entwickelte modulare Architektur zu begründen.

\section{End-to-End-Rekonstruktionspipelines}

Die Entwicklung echtzeitfähiger 3D-Rekonstruktionssysteme wurde maßgeblich durch zwei Paradigmen geprägt:
RGB-D-basierte Ansätze, die auf Tiefensensoren aufbauen, und rein monokulare Verfahren, die ausschließlich
RGB-Bildströme verarbeiten.

\subsection{RGB-D-basierte Pipelines}

Frühe echtzeitfähige Systeme wie KinectFusion~\cite{kinectfusion} etablierten die volumetrische Fusion
von Tiefendaten mittels TSDF-Repräsentation. Diese Architektur ermöglichte erstmals die inkrementelle
Rekonstruktion während der Kamerabewegung und wurde in zahlreichen Arbeiten erweitert, etwa durch
dynamisches Hashing (VoxelHashing~\cite{voxelhashing}), elastische Oberflächenverfolgung
(DynamicFusion~\cite{dynamicfusion}) oder Multi-View-Konsistenz (BundleFusion~\cite{bundlefusion}).

Allen gemein ist die Annahme einer verfügbaren RGB-D-Kamera, die synchronisierte Farb- und Tiefendaten liefert.
Diese Hardware-Abhängigkeit schränkt die Anwendbarkeit auf VR-Headsets mit integrierten Tiefensensoren ein --
eine Voraussetzung, die bei gängigen Consumer-VR-Systemen wie der Meta Quest-Serie typischerweise nicht erfüllt ist.
Für Multi-User-VR-Szenarien bedeutet dies, dass entweder alle Teilnehmenden über identische, tiefenfähige Hardware
verfügen müssen oder RGB-D-Verfahren als Lösungsklasse ausscheiden.
Zudem sind viele dieser Systeme primär für Einzelnutzende konzipiert; eine kontinuierliche Verteilung
inkrementeller Rekonstruktionsupdates an mehrere synchronisierte Clients wird nicht nativ adressiert.

\subsection{Monokulare Pipelines}

Mit DTAM~\cite{dtam}, LSD-SLAM~\cite{lsdslam} und ORB-SLAM~\cite{orbslam} entstanden monokulare Systeme,
die Tiefe aus Kamerabewegung und photometrischer bzw.\ Feature-basierter Konsistenz ableiten.
Diese Verfahren sind hardwareseitig mit Standard-RGB-Kameras kompatibel und damit prinzipiell
für Consumer-VR-Headsets geeignet.

Allerdings wurden klassische Systeme häufig für Offline-Verarbeitung oder spezifische Anwendungsfälle
(z.\,B.\ robotische Navigation) entwickelt.
Eine Integration in interaktive VR-Umgebungen erfordert typischerweise Anpassungen an Daten-Pipelines,
da Eingabeformate, Tracking-Annahmen und Ausgaberepräsentationen eng an die jeweilige Implementierung gekoppelt sind.
Neuere lernbasierte Ansätze wie DeepVideoMVS~\cite{deepvideomvs} oder NICE-SLAM~\cite{niceslam} kombinieren
neuronale Tiefenschätzung mit Mapping-Komponenten, bleiben aber häufig an spezifische Frameworks und Datenformate gebunden.
Ein systematischer Vergleich mehrerer Verfahren unter identischen Bedingungen ist dadurch mit erheblichem
Integrationsaufwand verbunden.

\subsection{Limitationen für VR-Integration}

Trotz erheblicher Fortschritte auf Algorithmusseite teilen viele End-to-End-Pipelines zentrale Limitationen
für den Einsatz in Multi-User-VR-Umgebungen:

\begin{enumerate}
\item \textbf{Monolithische Architektur:}
Rekonstruktionslogik, Tracking und Ausgabeformate sind häufig fest in die jeweilige Pipeline integriert.
Ein Austausch einzelner Komponenten oder der parallele Betrieb mehrerer Modelle erfordert pro Verfahren
individuelle Code-Anpassungen, separate Laufzeitumgebungen und dedizierte Test-Setups.

\item \textbf{Fehlende Streaming-Infrastruktur:}
Viele Systeme sind für Batch-Verarbeitung oder Single-Client-Szenarien ausgelegt.
Eine kontinuierliche Verteilung inkrementeller Updates an mehrere synchronisierte VR-Clients während einer
laufenden Sitzung wird nicht nativ unterstützt; Rekonstruktionsergebnisse müssen oft exportiert und anschließend
in die VR-Engine importiert werden.

\item \textbf{Unzureichende Pose-Flexibilität:}
Verfahren mit externer Pose-Anforderung (z.\,B.\ TSDF-basierte volumetrische Rekonstruktion) und Verfahren mit
interner Pose-Schätzung (z.\,B.\ SLAM-basierte Ansätze) benötigen unterschiedliche Datenflüsse.
Viele bestehende Pipelines sind nicht darauf ausgelegt, beide Kategorien parallel zu unterstützen oder zwischen ihnen
zu wechseln, ohne Frontend- oder Pipeline-Logik anzupassen.
\end{enumerate}

Diese Einschränkungen motivieren die Entwicklung einer modularen, containerisierten Architektur, die unterschiedliche
Rekonstruktionsverfahren -- unabhängig von ihrer internen Implementierung -- über eine standardisierte Schnittstelle
in VR-Umgebungen integriert und dabei sowohl monolithische Kopplung als auch Hardware-Abhängigkeiten reduziert.

\section{Ausgewählte Rekonstruktionsverfahren}

Zur Evaluation der Architektur unter realistischen Heterogenitäten wurden vier Modelle integriert:
\textit{NeuralRecon}, \textit{VisFusion}, \textit{MASt3R-SLAM} und \textit{SLAM3R}.
Die Auswahl erfolgte nach folgenden Kriterien:
\begin{itemize}
    \item \textbf{Paradigmenvielfalt:} Abdeckung unterschiedlicher Rekonstruktionsansätze (volumetrisch, SLAM-basiert, feed-forward)
    \item \textbf{Pose-Heterogenität:} Verfahren mit externer Pose-Anforderung und solche mit interner Schätzung
    \item \textbf{Repräsentationsformate:} Sowohl geschlossene Oberflächenmodelle (Mesh) als auch Punktwolken
    \item \textbf{Aktualität und Verfügbarkeit:} Publikationen aus den Jahren 2021--2024 mit verfügbaren Open-Source-Implementierungen
    \item \textbf{GPU-Kompatibilität:} Echtzeitfähigkeit auf Consumer-Hardware (NVIDIA RTX 3090/4090)
\end{itemize}

Die vier Modelle wurden so kombiniert, dass sie die in dieser Arbeit relevanten Schnittstellen-Varianten abdecken:
volumetrische Mesh/TSDF-Ausgabe mit externen Posen (NeuralRecon, VisFusion) sowie punktbasierte, inkrementelle Rekonstruktion mit interner/impliziter Pose-Schätzung (MASt3R-SLAM, SLAM3R).
Damit kann die Architektur unter heterogenen Ausgabeformaten und Tracking-Annahmen evaluiert werden, ohne die Kommunikations- und Visualisierungsschnittstellen zu verändern.

\begin{table}[H]
\centering
\caption{Vergleich der integrierten Rekonstruktionsverfahren hinsichtlich Schnittstellenanforderungen und Ausgabedaten.}
\label{tab:model_comparison}
\begin{tabularx}{\linewidth}{@{}l l l l X@{}}
\toprule
\textbf{Verfahren} & \textbf{Paradigma} & \textbf{Pose-Annahme} & \textbf{Repräsentation} & \textbf{Output-Eignung in VR} \\
\midrule
NeuralRecon        & Volumetrisch & Extern & TSDF/Mesh  & Geschlossene Oberflächen, Interaktion/Collider möglich \\
VisFusion          & Volumetrisch & Extern & TSDF/Mesh  & Detailreiche Oberflächen, gut triangulierbar \\
MASt3R-SLAM        & Hybrid       & Intern (SLAM) & Punktwolke   & Inkrementell, detailreich, Meshing optional \\
SLAM3R             & Feed-forward & Intern/implizit & Punktwolke & Inkrementell, ohne explizite Pose-Optimierung \\
\bottomrule
\end{tabularx}
\end{table}

Bewusst nicht integriert wurden klassische Sparse-SLAM-Verfahren (z.\,B.\ ORB-SLAM ohne Dense-Mapping), da diese keine für VR-Visualisierung geeigneten dichten Repräsentationen liefern, sowie NeRF-basierte Verfahren, die aufgrund langer Trainingszeiten für Streaming-Szenarien ungeeignet sind.
Ebenfalls nicht integriert wurde 3D Gaussian Splatting (3DGS) trotz hoher Aktualität, da der Ansatz primär auf \emph{Real-Time Radiance Field Rendering} bzw.\ Novel-View-Synthesis ausgelegt ist und typischerweise eine optimierungsbasierte Szenenrepräsentation aus (vor-)kalibrierten Mehransichten erstellt, anstatt eine engine-nahe, explizite Geometrie-Repräsentation (z.\,B.\ Mesh/TSDF) für Interaktion, Kollision und Navigation bereitzustellen.

\subsection{Volumetrische Verfahren}

\textbf{NeuralRecon} rekonstruiert lokale TSDF-Fragmente anstatt pro Keyframe isolierte Tiefenkarten~\cite{sun2021neuralrecon}.
Das Verfahren nimmt RGB-Bilder mit extern bereitgestellten Posen entgegen, projiziert extrahierte Bildmerkmale
in ein sparsifiziertes Voxelvolumen und integriert diese über rekurrente Fusionsmechanismen (GRU).
Daraus entstehen glatte, gut triangulierbare Oberflächen, die als geschlossene Meshes ausgegeben werden und sich
für Kollisionsabfragen oder Navigation eignen.
Die Abhängigkeit von externen Posen erfordert robustes Tracking durch das VR-System, während die festgelegte
Voxelauflösung den erreichbaren Detailgrad limitiert.

\textbf{VisFusion} erweitert volumetrische Online-Rekonstruktion um explizite Modellierung von Sichtbarkeit während
der Feature-Fusion~\cite{gao2023visfusion}.
Wie NeuralRecon benötigt es extern bereitgestellte Posen und liefert TSDF-basierte Oberflächenrepräsentationen.
Im Vergleich zu Verfahren ohne Sichtbarkeitsprüfung nutzt VisFusion vorhergesagte Sichtbarkeitsgewichte sowie
\textit{ray-based sparsification}, um Details zu erhalten und konkurrierende Beobachtungen konsistenter zu integrieren.
Dies kann zu konsistenterer Fragmentfusion und besserer Detailerhaltung beitragen, verlagert jedoch zusätzliche Kosten
in die Laufzeit.

\subsection{Hybride SLAM-basierte Verfahren}

\textbf{MASt3R-SLAM} ist ein echtzeitfähiges monokulares Dense-SLAM-System, das klassische SLAM-Komponenten mit
einem lernbasierten 3D-Rekonstruktions- und Matching-Prior kombiniert~\cite{murai2025mast3rslam}.
Anders als volumetrische Verfahren schätzt es Kameraposen intern (u.\,a.\ über Feature-Matching und Bundle-Adjustment),
wodurch keine externe Tracking-Quelle benötigt wird.
Als Ergebnis entsteht eine dichte, farbige Punktwolkenrepräsentation, die inkrementell aktualisiert werden kann und
keine explizite Meshing-Stufe voraussetzt.
Der erhöhte GPU-Speicherbedarf durch transformerbasierte Komponenten kann bei begrenzten Ressourcen zu Engpässen führen.

\subsection{Feed-forward-Verfahren}

\textbf{SLAM3R} verfolgt einen feed-forward-basierten Ansatz zur dichten Rekonstruktion aus monokularen RGB-Videos~\cite{slam3r2024}.
Das System unterteilt die Verarbeitung in ein \textit{Image-to-Points}-Modul zur lokalen Rekonstruktion und ein
\textit{Local-to-World}-Modul zur inkrementellen globalen Registrierung~\cite{slam3r2024arxiv}.
Kameraposen werden dabei nicht explizit optimiert, sondern implizit durch gelernte Registrierung zwischen lokalen Punktkarten
abgeleitet.
Dies ermöglicht schnelle Verarbeitung ohne klassische Bundle-Adjustment-Schleifen, kann jedoch bei längeren oder komplexen
Kamerabewegungen zu akkumuliertem Drift führen, da ohne explizite globale Pose-Optimierung nachträgliche Korrekturen nur
begrenzt möglich sind.

\section{Forschungslücke und Abgrenzung}
\label{sec:gap}

Die dargestellten Limitationen bestehender End-to-End-Pipelines (Abschnitt~3.1) sowie die daraus resultierenden,
heterogenen Schnittstellen- und Ausgabeanforderungen der ausgewählten Modelle (Abschnitt~3.2, Tabelle~\ref{tab:model_comparison})
verdeutlichen eine Lücke an der Schnittstelle zwischen Forschungsprototypen und VR-Deployment.
Trotz erheblicher Fortschritte auf Algorithmusseite fehlt es an wiederverwendbarer Infrastruktur, die diese Verfahren in
laufende Multi-User-VR-Sitzungen integriert.

Konkret fehlen Infrastrukturen, die folgende Anforderungen \textit{gleichzeitig} erfüllen:

\begin{enumerate}
\item \textbf{Modellunabhängige Integration:}
Unterschiedliche Rekonstruktionsverfahren (volumetrisch, SLAM-basiert, feed-forward) sollen ohne Code-Anpassung in eine
gemeinsame Architektur eingebettet werden können.
Dies erfordert eine standardisierte Schnittstelle, die unabhängig von internen Frameworks und Tracking-Annahmen funktioniert.

\item \textbf{Kontinuierliches Streaming für Multi-User-VR:}
Ergebnisse müssen inkrementell in laufende VR-Szenen mit mehreren synchronisierten Clients gestreamt werden, während die
Rekonstruktion fortschreitet.
Batch-orientierte Pipelines (Aufnahme \(\rightarrow\) Offline-Verarbeitung \(\rightarrow\) Import) unterstützen dies nicht,
und Forschungsprototypen bieten selten standardisierte Streaming-Protokolle oder Multi-Client-Synchronisation.

\item \textbf{Heterogene Ausgabeformate:}
Die Architektur muss unterschiedliche 3D-Repräsentationen (TSDF/Mesh vs.\ Punktwolke) einheitlich visualisieren und dabei
formatspezifische Charakteristika (z.\,B.\ Chunking, LOD, inkrementelle Updates) berücksichtigen.

\item \textbf{Pose-Flexibilität:}
Systeme, die externe Posen benötigen, und solche mit interner/impliziter Schätzung sollen parallel betrieben werden können,
ohne Frontend-Logik oder Kommunikationsprotokoll anzupassen.
\end{enumerate}

Aktuelle Publikationen zu den evaluierten Modellen adressieren primär Rekonstruktionsqualität und algorithmische Verbesserungen,
nicht jedoch Deployment-Architekturen für VR-Integration oder den systematischen Vergleich heterogener Verfahren unter identischen Bedingungen.
Der praktische Integrationsaufwand (u.\,a.\ Anpassung von Datenformaten, Implementierung von Streaming-Logik, Synchronisation mehrerer Clients)
bleibt dadurch weitgehend den Anwendenden überlassen.

Das in dieser Arbeit entwickelte System \textit{RTReconstruct} adressiert diese Lücke durch eine containerisierte, modellunabhängige Architektur
mit standardisiertem Streaming-Protokoll und bildet die Grundlage für die in Kapitel~4 beschriebene Konzeption, Implementierung und Evaluation.
