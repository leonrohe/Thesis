\chapter{Implementierung}

Dieses Kapitel beschreibt die technische Realisierung der in Kapitel 4 entworfenen Systemarchitektur. Es detailliert die Umsetzung des Backends als asynchronen Verteilknoten, die Entwicklung des Unity-basierten Frontends sowie das entworfene Kommunikationsprotokoll. Besonderes Augenmerk liegt dabei auf den Designentscheidungen, die getroffen wurden, um die Anforderungen an Echtzeitfähigkeit und Modularität zu erfüllen.

\section{Gesamtarchitektur und Technologie-Stack}

Das System RTReconstruct realisiert eine modulare Streaming-Pipeline, die räumlich verteilte VR-Clients mit rechenintensiven Rekonstruktionsdiensten verbindet. Die Implementierung gliedert sich in zwei Hauptkomponenten: ein Python-basiertes Backend, das mittels Container-Virtualisierung orchestriert wird, und ein C\#-basiertes Frontend innerhalb der Unity-Engine.

\subsection{Backend-Technologien}

Für die Umsetzung des Backends fiel die Wahl auf FastAPI in Kombination mit dem ASGI-Server Uvicorn. Im Gegensatz zu synchronen Frameworks (wie Flask) ermöglicht FastAPI durch die konsequente Nutzung von Pythons \texttt{asyncio}-Bibliothek eine nicht-blockierende Verarbeitung von Netzwerkanfragen~\cite{ramirez2023fastapi}. Dies ist für die Architektur essenziell: Der zentrale Router agiert als Vermittlungsstelle, die gleichzeitig hochfrequente Bilddaten von Clients empfängt und an mehrere GPU-Worker verteilt. Da die Inferenzzeiten der Modelle (z.\,B. MASt3R-SLAM oder NeuralRecon) stark variieren, würde ein synchrones Design dazu führen, dass der Router blockiert. Durch das asynchrone Design werden Netzwerk-I/O und Rechenoperationen entkoppelt, was den Durchsatz des Gesamtsystems maximiert.

Die Persistenz der Rekonstruktionsergebnisse wird durch eine SQLite-Datenbank gewährleistet, die Metadaten und Binär-Blobs (GLB-Dateien) speichert. Aufgrund der modularen Struktur werden alle Komponenten mittels Docker Compose orchestriert, wobei das NVIDIA Container Toolkit den Zugriff der Worker-Container auf die Host-GPU ermöglicht.

\subsection{Frontend-Technologien}

Das Frontend wurde in Unity \textit{6000.2.12f1} implementiert. Für die Netzwerkkommunikation kommt die Bibliothek \textit{NativeWebSocket} zum Einsatz, da sie im Gegensatz zu Standard-.NET-Sockets eine bessere Kompatibilität mit WebGL- und Android-Builds (Meta Quest) bietet~\cite{endel2024nativewebsocket}. Das Rendering der 3D-Rekonstruktionen erfolgt hybrid: Punktwolken werden über den Visual Effect Graph (VFX Graph) GPU-beschleunigt dargestellt, während Mesh-Daten mittels der Bibliothek \textit{glTFast} zur Laufzeit importiert werden~\cite{atteneder2024gltfast}.

\section{Kommunikationsstandard}

Um die Latenz bei der Übertragung großer Bild- und Geometriedaten zu minimieren, wurde statt eines textbasierten Formats (wie JSON) ein maßgeschneidertes binäres Protokoll implementiert. Die Wahl eines binären Protokolls begründet sich durch die Vermeidung des ca.\ 33\,\% Overhead von Base64-Kodierung~\cite{josefsson2006rfc4648}, was insbesondere im drahtlosen VR-Betrieb die Bandbreite signifikant schont. Durch die JPEG-Komprimierung der Bilder und das GLB-Format für 3D-Modelle wird die Datenmenge weiter reduziert.

\subsection{Allgemeiner Aufbau}

Jedes Paket beginnt mit einem einheitlichen Header, der der Validierung und Versionierung dient. Dadurch kann der Empfänger ungültige oder inkompatible Pakete frühzeitig erkennen und verwerfen. Der folgende C-Pseudocode dient ausschließlich der Veranschaulichung und zeigt den grundlegenden Aufbau der Basistypen sowie des Paket-Headers.

\bigskip

\begin{lstlisting}[caption={Definition der Basistypen und des Headers}, label={lst:protocol_basics}]
// Basistypen
type int32   = 4 Byte Signed Integer (Little Endian);
type float32 = 4 Byte Floating Point (IEEE 754);
type byte    = 1 Byte Unsigned Integer;

// Zusammengesetzte Typen
struct String {
    int32  length;       // Laenge des Strings in Bytes
    byte[] utf8_bytes;   // UTF-8 kodierter Inhalt
};

struct Vector3 {
    float32 x; float32 y; float32 z;
};

struct Quaternion {
    float32 x; float32 y; float32 z; float32 w;
};

// Gemeinsamer Header fuer alle Pakete
struct PacketHeader {
    char[4] magic;      // ASCII
    int32   version;    // Protokollversion
};
\end{lstlisting}

\subsection{Pakettypen}

Basierend auf den in Kapitel 4 definierten Anforderungen wurden drei Hauptpakettypen spezifiziert: \texttt{ModelFragment}, \texttt{ModelResult} und \texttt{TransformFragment}. Jeder Pakettyp ist für eine spezifische Kommunikationsrichtung und Funktionalität zuständig.

\subsubsection{ModelFragment (Client an Server)}
Das \texttt{ModelFragment} bündelt eine Sequenz von Frames (Fenster) für die Inferenz. Um die Serialisierung effizient zu gestalten, werden die Arrays für Bilder, Intrinsics und Extrinsics nacheinander geschrieben, anstatt sie pro Frame zu verschachteln.

\bigskip

\begin{lstlisting}[caption={Struktur des ModelFragments (Upload)}, label={lst:struct_modelfragment}]
struct ModelFragment {
    PacketHeader header;      // Version = 1
    int32        window_size; // Anzahl der Frames (N)
    
    // Metadaten
    String       model_name;  // Zielmodell (z.B. "neucon")
    String       scene_name;  // Eindeutige Szenen-ID
    
    // Nutzdaten (Sequentiell angeordnet)
    FrameData[window_size]  frames;     // Liste der JPEG-Bilder
    Intrinsics[window_size] intrinsics; // Liste der Kameraparameter
    Extrinsics[window_size] extrinsics; // Liste der Kameraposen
};

// Hilfsstrukturen fuer ModelFragment
struct FrameData {
    int32   image_size;
    byte[]  jpeg_data;
    float32 width;
    float32 height;
};

struct Intrinsics {
    float32 fx; float32 fy;   // Brennweite
    float32 cx; float32 cy;   // Hauptpunkt
};

struct Extrinsics {
    Vector3    position;
    Quaternion rotation;
};
\end{lstlisting}

\subsubsection{ModelResult (Server an Client)}
Das \texttt{ModelResult} enthält das Ergebnis einer Rekonstruktion. Es besteht aus Transformationsdaten für die Ausrichtung in der Unity-Szene und einem binären Blob, der das 3D-Modell im GLB-Format (glTF Binary) enthält. Das GLB-Format wurde gewählt, da es Geometrie und Texturen effizient bündelt.

\bigskip

\begin{lstlisting}[caption={Struktur des ModelResults (Download)}, label={lst:struct_modelresult}]
struct ModelResult {
    PacketHeader header;         // Version = 2
    
    String       scene_name;     // Referenzierte Szene
    bool         is_pointcloud;  // 1 Byte (True/False)
    
    // Globale Transformation fuer Unity
    Vector3      position;
    Quaternion   rotation;
    Vector3      scale;
    
    // 3D-Modell
    byte[]       glb_payload;    // Rest des Datenstroms bis EOF
};
\end{lstlisting}

\subsubsection{TransformFragment (Client an Server)}
Das \texttt{TransformFragment} ist ein leichtgewichtiges Steuerpaket. Es dient dazu, die Position eines Modells auf dem Server nachträglich zu korrigieren (z.\,B. bei manuellem Alignment durch den Nutzer), ohne die rechenintensiven Bilddaten erneut zu übertragen.

\bigskip

\begin{lstlisting}[caption={Struktur des TransformFragments (Sync)}, label={lst:struct_transformfragment}]
struct TransformFragment {
    PacketHeader header;      // Version = 3
    
    String       scene_name;  // Zielszene
    
    // Neue Transformationsdaten
    Vector3      position;
    Quaternion   rotation;
    Vector3      scale;
};
\end{lstlisting}

\section{Backend-Implementierung}

Das Backend fungiert als Schaltzentrale des Systems. Es setzt sich aus dem zentralen Router und den modellspezifischen Workern zusammen.

\subsection{Asynchrones Routing und Fan-Out}

Der Router (\texttt{fragment\_hub.py}) verwaltet den globalen Zustand aller verbundenen Clients und aktiven Szenen. Er implementiert zwei WebSocket-Endpunkte:
\begin{enumerate}
    \item \texttt{/ws/client}: Hier verbinden sich Unity-Clients. Nach einem initialen Handshake, in dem der Client seine Rolle (Host/Visitor) und die gewünschte Szene übermittelt, registriert der Router den Client in der \texttt{connected\_clients}-Map.
    \item \texttt{/ws/model/\{name\}}: Hier verbinden sich die Modell-Worker. Für jedes Modell wird eine \texttt{asyncio.Queue} angelegt.
\end{enumerate}

Eingehende Fragmente von Clients werden vom Router validiert, deserialisiert und in die Warteschlange des adressierten Modells gelegt. Sobald ein Modell-Worker ein Ergebnis liefert, wird dieses zunächst in der Datenbank persistiert gespeichert. Anschließend iteriert der Router über alle Clients, die der entsprechenden Szene zugeordnet sind, und sendet das Ergebnis asynchron über deren WebSocket-Verbindungen nach dem Fan-Out Prinzip~\cite{hohpe2004enterprise}. Durch die Nutzung von \texttt{asyncio.gather()} werden diese Sendevorgänge parallel ausgeführt, ohne die Verarbeitung neuer Fragmente zu blockieren.

\subsection{Modell-Worker}

Die Modell-Worker implementieren die eigentliche 3D-Rekonstruktion und laufen als eigenständige Prozesse, die über WebSockets mit dem Backend-Router kommunizieren. Grundlage dafür ist die abstrakte Klasse \texttt{BaseReconstructionModel}, die die gemeinsame Kommunikationslogik kapselt und von allen konkreten Modell-Workern erweitert wird.

\subsubsection{Basisklasse und Kommunikationsfluss}

Die Klasse \texttt{BaseReconstructionModel} initialisiert sich mit einem Modellnamen und einer Server-URL und leitet daraus den WebSocket-Endpunkt \texttt{/ws/model/<modelname>} des Backends ab. Die Methode \texttt{connect()} stellt die WebSocket-Verbindung her und startet asynchron die Schleife \texttt{listen()} zum Empfangen von Fragmenten. Eingehende Nachrichten werden in \texttt{listen()} mittels der Hilfsfunktion zur Fragment-Deserialisierung in Python Datenstrukturen überführt und jeweils an die abstrakte Methode \texttt{handle\_fragment()} weitergegeben, die von den konkreten Unterklassen zu implementieren ist.

\bigskip

\begin{lstlisting}[language=python]
class BaseReconstructionModel(ABC):
    def __init__(self, model_name: str, server_url: str):
        # speichert Modellname und abgeleitete WebSocket-URL
        ...

    async def connect(self):
        # WebSocket-Verbindung herstellen und listen/processloop starten
        ...

    async def listen(self):
        # eingehende Bytes lesen, zu Fragmenten deserialisieren
        # und für jedes Fragment handle_fragment aufrufen
        ...

    async def send_result(self, result: ModelResult, success: bool = True):
        # Ergebnis serialisieren und als Bytes senden
        # Sonderfälle: Fehler (ein Null-Byte),
        # oder kein relevantes Ergebnis (ein Eins-Byte)
        ...

    @abstractmethod
    async def handle_fragment(self, fragment: dict):
        # von Unterklassen zu implementieren
        ...
\end{lstlisting}

\subsubsection{Spezialisierte Rekonstruktionsmodelle}

Die konkreten Modell-Worker laden jeweils ein externes Rekonstruktionsmodell (NeuralRecon, MASt3R-SLAM, SLAM3R oder VisFusion) und implementieren \texttt{handle\_fragment()}, um die empfangenen Fragmente in das jeweils benötigte Eingabeformat zu transformieren. Nach der Inferenz wird das Ergebnis als \texttt{ModelResult} mit der rekonstruierten Szene, im GLB-Format und optionalen Transformationen, erzeugt und über \texttt{send\_result()} an den Router zurückgesendet.

\subsubsection{Einsatz im Container-Setup}

Für den produktiven Betrieb werden die Worker als eigenständige Prozesse bzw. Container gestartet, denen über Umgebungsvariablen der jeweilige Modellname (\texttt{MODELNAME}) und die Server-URL (\texttt{SERVERURL}) übergeben werden. Das Skript initialisiert daraus die passende Unterklasse von \texttt{BaseReconstructionModel} und startet mit \texttt{asyncio.run(model.connect())} die oben beschriebene Kommunikationsschleife, sodass sich jeder Modell-Worker automatisch beim zentralen Backend-Router registriert.

\section{Frontend-Implementierung}

Das Frontend ist für die Datenerfassung, die Netzwerkkommunikation sowie die Visualisierung verantwortlich.

\subsection{Hardware-Abstraktion mittels Strategy Pattern}

Um das System plattformunabhängig zu halten, wurde für den Zugriff auf Kamerasensoren das Strategy Pattern angewandt~\cite{gamma1994design}. Das Interface \texttt{ICaptureDevice} definiert Methoden zum Abruf von Bildern (\texttt{GetFrame}) sowie von intrinsischen (\texttt{GetIntrinsics}) und extrinsischen (\texttt{GetExtrinsics}) Parametern. Es existieren spezifische Implementierungen für verschiedene Hardware-Typen:

\begin{itemize}
    \item \textbf{MetaQuestCaptureDevice}: Nutzt die Passthrough-API der Meta Quest, um Zugriff auf die Umgebungskameras und die Headset-Position zu erhalten.
    \item \textbf{SmartphoneCaptureDevice}: Basiert auf ARFoundation~\cite{unity2024arfoundation} und ermöglicht die Nutzung von Android/iOS-Geräten als externe Kameraquellen.
    \item \textbf{EvalCaptureDevice}: Eine Implementierung für die Aufnahme synthetischer Szenen (siehe Kapitel~\ref{chap:evaluation}). Sie nutzt dennoch die Intrinsiken/Extrinsiken des \textit{MetaQuestCaptureDevice} um eine möglichst realitätsnahe Repräsentation zu gewährleisten.
\end{itemize}

\subsection{Fragmentsammlung mittels ModelCollector}

Der ModelCollector überprüft in jedem Frame die aktuellen Kameraparameter des aktiven \textit{ICaptureDevice}. Bilddaten werden nur aufgenommen, wenn seit dem letzten gültigen Frame eine hinreichende Bewegung festgestellt wird. Falls Modelle spezifische Werte für die Aufnahme benötigen, können diese überschrieben werden, aber standardmäßig muss mindestens eine der folgenden Bedingungen erfüllt sein:

\begin{enumerate}
    \item eine Positionsänderung von mindestens \(\approx 0.1\,\text{m}\),
    \item oder eine Rotationsänderung von mindestens \(\approx 15^\circ\).
\end{enumerate}

Frames, die diese Bewegungskriterien und die Verfügbarkeit aller notwendigen Sensordaten erfüllen, werden gepuffert, bis die konfigurierte Fenstergröße erreicht ist. Das vollständige Fragment wird dann an den \textit{ReconstructionClient} weitergereicht. So stellt der \textit{ModelCollector} sicher, dass nur informative und ausreichend unterschiedliche Ansichten an das Backend übertragen werden.

\subsection{Visualisierung mittels Spatial Hashing}

Die Darstellung der empfangenen 3D-Daten muss in Echtzeit erfolgen, ohne die Framerate der VR-Anwendung maßgeblich zu beeinträchtigen. Für Mesh-Daten (z.\,B.\ von NeuralRecon) wurde aus diesem Grund ein Chunking-Verfahren implementiert (\texttt{MeshUtils.ChunkMesh}). Da empfangene GLB-Modelle geometrisch komplex sein können, werden sie nach dem Import in ein $N \times N \times N$ Gitter unterteilt. Die Chunk-Größe wurde auf $5 \times 5 \times 5$ Meter festgelegt, was ein gutes Gleichgewicht zwischen Frustum-Culling-Effizienz und Verwaltungsoverhead darstellt. Dies ermöglicht der Unity-Engine, nicht sichtbare Bereiche mittels Frustum Culling effizient auszublenden und erhöht die Rendering-Performance auf mobiler Hardware signifikant.

Punktwolken (z.,B.\ von SLAM3R) werden über \texttt{GraphicsBuffer} direkt an die GPU übergeben und mittels eines Compute-Shaders im VFX Graph gerendert, anstatt sie als klassische GameObjects zu instanziieren. Dieses Vorgehen reduziert die CPU-Last erheblich und ermöglicht eine effizientere Verarbeitung großer Datenmengen.
Die Darstellung ist derzeit auf 100.000 Punkte limitiert, da die Meta Quest 3 bei höheren Punktzahlen signifikante Performance-Einbußen zeigt. Diese Einschränkung stellt einen bewussten Kompromiss zwischen visueller Detailgenauigkeit und Systemleistung dar, um eine flüssige Bildwiederholrate in der VR-Umgebung zu gewährleisten.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/mesh_pointcloud_vis.png}
    \caption{Visualisierung einer Mesh-Rekonstruktion (VisFusion, links) und einer Punktwolken-Rekonstruktion (MASt3Rrechts)}
\end{figure}

\subsection{Integration in das Va.Si.Li-Lab}

Die Integration von \emph{RTReconstruct} in das Va.Si.Li-Lab erfolgt als eigenständiger, entkoppelter Dienst, der sich durch vorkonfigurierte Unity-Prefabs in bestehende Szenen einbinden lässt. Das System ist somit kein integraler Bestandteil der Lab-Infrastruktur, sondern eine optionale Erweiterungskomponente, die unabhängig von der etablierten Szenenlogik und Mehrbenutzerarchitektur des Va.Si.Li-Labs betrieben werden kann.

\subsubsection{Rollen- und Szenenkonzept}

Die Nutzung von RTReconstruct innerhalb des Va.Si.Li-Lab orientiert sich an einem einfachen Rollen- und Szenenkonzept, das lediglich die Benutzeroberfläche des Labs übernimmt. Beim Start einer Sitzung wählen die Nutzenden eine Rolle (\emph{Host} oder \emph{Visitor}) sowie einen Szenennamen. Diese Auswahl dient ausschließlich der clientseitigen Verwaltung der Rekonstruktionsdaten und hat keinen Einfluss auf die übrige Funktionalität des Va.Si.Li-Labs.

\emph{Hosts} initiieren die Datenerfassung und senden Sensordaten über das WebSocket-basierte Kommunikationsprotokoll an das RTReconstruct-Backend. \emph{Visitors} empfangen die resultierenden Rekonstruktionen zur Visualisierung, ohne selbst Daten beizusteuern. Diese Rollentrennung wird vollständig innerhalb der RTReconstruct-Prefabs verwaltet und erfordert keine Anpassung der Va.Si.Li-Lab-Mehrbenutzerlogik.

\subsubsection{Integrationsprinzipien}

Die Einbindung von RTReconstruct folgt dem Prinzip einer additiven, nicht-invasiven Erweiterung. Das System wird nicht in die bestehende Infrastruktur des Va.Si.Li-Lab eingewoben, sondern als geschlossene Komponente bereitgestellt, die über standardisierte Unity-Prefabs in beliebige Szenen integriert werden kann.

Daraus ergeben sich drei zentrale Integrationsprinzipien: Erstens erfolgt die Einbindung durch Instanziierung vorgefertigter Prefabs (\texttt{ReconstructionManager}, \texttt{RoomReconstructor} und \texttt{ReconstructionClient}), die alle erforderlichen Komponenten kapseln. Zweitens besteht keine technische Abhängigkeit zwischen RTReconstruct und den bestehenden Va.Si.Li-Lab-Komponenten, beide Systeme teilen lediglich die gemeinsame Grundlage des Meta SDK zur Erfassung von Kamera- und Posendaten. Drittens bleibt die Mehrbenutzerumgebung des Va.Si.Li-Lab vollständig funktionsfähig, unabhängig davon, ob RTReconstruct aktiv genutzt wird oder nicht; das Rekonstruktionssystem verhält sich als passiver, zuschaltbarer Dienst ohne Eingriff in die Szenenlogik des Labs.

\section{Zusammenfassung}

In diesem Kapitel wurde die Implementierung von RTReconstruct erläutert. Durch den Einsatz von FastAPI und asynchronen Warteschlangen im Backend konnte ein Router mit hoher Performance realisiert werden, der auch bei heterogenen Inferenzzeiten der Modelle stabil bleibt. Im Frontend ermöglichen Design-Patterns wie Strategy und Dispatcher eine saubere Trennung von Hardware-Zugriff, Netzwerklogik und Rendering. Das entwickelte binäre Protokoll sorgt dabei für eine effiziente Datenübertragung. Somit ist die Architektur robust genug, um in der anschließenden Evaluation unter Echtzeitbedingungen geprüft zu werden.