\chapter{Implementierung}
Dieses Kapitel beschreibt die konkrete Umsetzung der in Kapitel 4 entworfenen Architektur. Es gliedert sich in einen Überblick zu Technologien und Laufzeitumgebung, die Backend- und Frontend-Realisierung, echtzeitkritische Pipeline-Aspekte, die Integration ins Va.Si.Li-Lab sowie reflektierte Designentscheidungen.

\section{Gesamtarchitektur \& Technologien}
\textbf{Ziel der Implementierung}

\noindent
Das System realisiert eine modulare Streaming-Pipeline, in der ein Unity-Client Bild- und Pose-Daten fensterbasiert an einen WebSocket-Router sendet. Der Router verteilt Fragmente an modell­spezifische Worker (z. B. SLAM3R, MASt3R, VisFusion), sammelt deren Ergebnisse (GLB/Punktwolke/Mesh) und publiziert sie an alle Clients derselben Szene. Die Ergebnisse werden zusätzlich persistiert. Die Router-Seite ist als FastAPI-Anwendung umgesetzt; modellseitig fungieren asynchrone WebSocket-Worker als Producer/Consumer. 

\medskip
\noindent
\textbf{Technologie-Stack}

\noindent
Backend: Python, FastAPI/Uvicorn, websockets, SQLite; Containerisierung via Docker Compose; GPU-fähige Worker-Container für Modelle (Neucon, VisFusion, SLAM3R, MASt3R). Frontend: Unity (C\#), NativeWebSocket, glTFast für GLB-Import, VFX-basierte Punktwolken-Darstellung. Die Compose-Orchestrierung startet den Router (Port 5000) und mehrere GPU-Worker; Quellverzeichnisse werden in die Container gemountet.

\section{Kommunikationsstandard}
Um eine effiziente WebSocket-Kommunikation zwischen dem Front- und Backend zu gewährleisten, werden einige standardisierte Byte-Frames definiert. Die wichtigsten Nachrichtentypen sind:
\begin{enumerate}
    \item ModelFragment (Client $\rightarrow$ Router $\rightarrow$ Modell)
    \par
    \noindent
    Header: `LEON', Version 1, Fenstergröße; dann Modellname und Szenenname. Es folgen Fensterweise:
    \begin{itemize}
        \item Bilddaten (JPEG-Bytes + Breite/Höhe)
        \item Intrinsics (Fx, Fy, Cx, Cy)
        \item Extrinsics (Kamera-Position xyz, Rotation xyzw)
    \end{itemize}
    Der Router erkennt Version 1, extrahiert Modell-/Szenennamen und legt das Fragment in die Queue des adressierten Modells.

    \item ModelResult (Modell $\rightarrow$ Router $\rightarrow$ Client)
    \par
    \noindent
    Header: LEON, Version 2, Szenenname; dann isPointcloud, Translation (xyz), Rotation (xyzw), Scale (xyz), gefolgt vom GLB-Payload. Der Router speichert pro Szene/Modell das jüngste Resultat und verteilt es an alle abonnierten Clients (bei bereits vorhandenen Ergebnissen direkt nach dem Handshake).

    \item (optional) TransformFragment (Client $\rightarrow$ Router)
    \par
    \noindent
    Header: LEON, Version 2, Szenenname; dann Translation (xyz), Rotation (Quaternion xyzw), Skalierung (xyz). Der Router aktualisiert damit die Pose aller bereits vorliegenden Ergebnisse der Szene und benachrichtigt verbundene Clients.
\end{enumerate}

\section{Backend}

\subsection{Architektur und Komponenten}

\textbf{Router (FastAPI) }

\noindent
Der Router stellt zwei WebSocket-Endpunkte bereit:
\begin{enumerate}
    \item \verb|/ws/client| für Unity-Clients (Handshake, Ergebnis-Fan-out)
    \item \verb|/ws/model/{model_name}| für Modell-Worker (Fragment-Zustellung, Ergebnis-Rückkanal)
\end{enumerate}
Der Router verwaltet außerdem den globalen Zustand von verbundenen Clients, welcher Szene diese zugeordnet sind, einer Eingabe-Queue für jedes Modell und eine Modelausgabe pro Modell pro Szene.

Beim Verbindungsaufbau mit dem Client sendet der Router einen Handshake als JSON mit einer Liste an aktuell verfügbaren Modellen.

\medskip
\noindent
\textbf{Modell-Worker}

\noindent
Ein Worker verbindet sich per WebSocket, lauscht auf eingehende Fragment, verarbeitet sie und sendet ein \texttt{ModelResult} zurück. Die abstrakte Basisklasse kapselt Verbindungs-/Sende-Logik und definiert die Funktionen \texttt{handle\_fragment(fragment)} sowie \texttt{send\_result(result)}.

\medskip
\noindent
\textbf{Persistenz}

\noindent
Ergebnisse werden in SQLite (db/results.db) gespeichert, inklusive Szene, Transform, Pointcloud-Flag und GLB-Blob; parallel protokolliert der Router je Modell CSV-Zeilen (input size, inference time, output size) zur späteren Auswertung.

\subsection{Datenfluss}
\begin{itemize}
    \item \textbf{Client-Endpoint} \verb|/ws/client|: Handshake (Rolle/Szene) → Modellliste senden → Empfang von LEON v1/v2 → v1 an Modell-Queue, v2 als Transform anwenden → Event-Signale an Szene-Abonnenten. Bereits vorhandene Ergebnisse der Szene werden unmittelbar gesendet.
    \item \textbf{Modell-Endpoint} \verb|/ws/model/{model_name}|:
Worker verbindet sich, erhält Fragmente aus der Queue, sendet nach Inferenz ModelResult v2 zurück; Router misst Latenz, schreibt CSV-Zeile, persistiert in SQLite und setzt Client-Events zur Auslieferung. Sondercodes b'0'/b'1' signalisieren Fehler bzw. „unwichtiges“ Ergebnis.
\end{itemize}

\subsection{Containerisierung}
Die docker-compose.yml baut Router und Worker, setzt GPU-Reservierungen und übergibt Modell-Namen und Server-URL (ws://router:5000/ws/model) als Umgebungsvariablen; Worker-Entry-Points verbinden sich selbsttätig.

\section{Frontend}

\subsection{Architektur und Komponenten}
Die Frontend-Architektur basiert auf vier zentralen Komponenten, die über definierte Schnittstellen miteinander interagieren.

\medskip

ICaptureDevice abstrahiert den hardwarespezifischen Sensorzugriff und ermöglicht plattformunabhängige Datenerfassung. Die Implementierung umfasst MetaQuestCaptureDevice für Passthrough-Kameras, SmartphoneCaptureDevice für ARFoundation-basierte mobile Geräte sowie Eval-Varianten zum Abspielen vorgezeichneter Pose-Sequenzen für reproduzierbare Tests. Alle Implementierungen liefern RGB-Bilder, Intrinsics (Brennweite, Hauptpunkt) und Extrinsics (Position, Rotation) in einheitlichem Format.

\medskip

IModelCollector verwaltet eingehende Frames in einem Sliding-Window-Puffer und entscheidet anhand von Schwellenwerten für Translation, Rotation und zeitlichen Abstand, ob ein Frame als Keyframe aufgenommen wird. Modellspezifische Varianten wie NeuralReconCollector und SLAM3RCollector definieren unterschiedliche Sampling-Strategien.

\medskip

ReconstructionClient implementiert die WebSocket-basierte Kommunikation mit dem Backend über NativeWebSocket. Nach dem Handshake sendet der Client Rolle und Szenenname und erhält die Liste verfügbarer Modelle zurück. Eine asynchrone Sendewarteschlange verwaltet die Nachrichtenserialisierung im Background-Thread, wobei Transform-Updates bevorzugt behandelt werden.

\medskip

RoomReconstructor verarbeitet eingehende Rekonstruktionsergebnisse thread-sicher und visualisiert sie in der Unity-Szene. Mesh-basierte Ergebnisse werden mit glTFast importiert und mittels MeshUtils.ChunkMesh() in kleinere Chunks unterteilt, Punktwolken werden über den Visual Effects Graph mit GPU-Buffern gerendert.

\subsection{Capture Pipeline}
Die Datenerfassung erfolgt frame-synchron als Coroutine, die mit Unity's Rendering-Zyklus koordiniert. Der ReconstructionManager führt CaptureLoop() aus, die nach WaitForEndOfFrame() Pose und Bilddaten zeitlich konsistent erfasst. Die Intrinsics werden basierend auf dem Verhältnis zwischen nativer und verwendeter Auflösung (640×480) skaliert, das Kamerabild wird in eine RenderTexture gerendert, per GPU-Readback kopiert und JPEG-kodiert. Die Keyframe-Selektion prüft mittels ShouldCollect(), ob räumliche oder zeitliche Schwellenwerte überschritten wurden.

\subsection{Clientseitige Kommunikation}
Das Frontend kommuniziert über eine dauerhafte WebSocket-Verbindung mit dem Router. Nach dem initialen Handshake werden Fragmente im binären LEON-v1-Format übertragen.
Merkmale der Übertragungsschicht:

\begin{itemize}
    \item Asynchrone Sendequeue zur Entkopplung der VR-Hauptschleife
    \item Background-Thread-Serialisierung für geringe Latenz
    \item Priorisierte Behandlung von Transform-Updates
\end{itemize}

Bereits vorhandene Rekonstruktionsergebnisse werden direkt nach Verbindungsaufbau zugestellt.

\subsection{Verarbeitung und Rendering rekonstruktiver Ergebnisse}
Rekonstruktionsergebnisse werden asynchron empfangen und thread-sicher in die Unity-Szene integriert. Das LEON Version 2-Protokoll enthält neben Header und Szenenname ein Transform (Translation, Rotation, Scale) sowie die GLB-Payload.

Alle Unity-API-Operationen werden über eine mainThreadActions-Queue gesammelt und im Update()-Zyklus abgearbeitet. Mesh-basierte Rekonstruktionen werden mit glTFast geladen und mittels MeshUtils.ChunkMesh() in ein $n\times n \times n$ Grid unterteilt, um Frustum Culling zu verbessern.

Punktwolken nutzen strukturierte GPU-Buffer für Positionen und Farben, die an den Visual Effect Graph übergeben werden. Nach dem Setzen der Buffer triggert Reinit() die GPU-seitige Aktualisierung.

\subsection{Geräteintegration}
Die Geräteintegration erfolgt über das ICaptureDevice-Interface, das hardwarespezifische Details abstrahiert. Das Frontend ist dennoch primär für Live-VR-Erfassung auf Meta Quest ausgelegt:

\begin{itemize}
    \item Passthrough-RGB-Kamera der Meta Quest
    \item Original-Headset-Pose aus XR-Tracking
    \item Unity-Render-Pipeline für Echtzeit-Anzeige
\end{itemize}

Weitere Sensorquellen (Smartphone, virtuelle Testkamera) werden über das gleiche Capture-Interface angebunden.

\section{Integration in das Va.Si.Li-Lab}
Die Integration erfolgte in die bestehende Unity-basierte Va.Si.Li-Lab-Umgebung, die eine mehrbenutzerfähige VR-Plattform für simulationsbasiertes Lernen und Forschung bereitstellt. Ziel war eine nahtlose Einbindung der Rekonstruktionspipeline, ohne bestehende Mehrbenutzer- und Interaktionsmechaniken zu beeinträchtigen.

\subsection{Zielumgebung und Anforderungen}
Das Va.Si.Li-Lab basiert auf Unity mit Ubiq als Mehrbenutzer-Framework und erfasst multimodale Interaktionen in Echtzeit. Für die Einbindung war erforderlich, die Rekonstruktionsdaten als eigenständige, netzwerkagnostische Szeneinhalte zu führen und die Frontend-Clientlogik strikt von Lab-spezifischen Synchronisationsdiensten zu entkoppeln. Dadurch bleibt die Rekonstruktion unabhängig von Session-Logik und kann in beliebige Lernszenarien eingebettet werden.

\subsection{Architekturankopplung}
Die Integration nutzt den bestehenden Szenen-Lebenszyklus des Labs und bindet den ReconstructionManager sowie RoomReconstructor als additive Komponenten in die Lab-Hauptszene ein. Die Kommunikation mit dem Backend erfolgt ausschließlich über die WebSocket-Schnittstelle des ReconstructionClient, der zu keiner Zeit Lab-interne Netzwerkpfade (z. B. Ubiq-Channels) verwendet, um Kollisionen mit dem Mehrbenutzer-Stack zu vermeiden. Resultate werden als reguläre Unity-GameObjects instanziiert und über Parent-Transforms an die Lab-Koordinaten verankert, wodurch Persistenz- und Analyse-Logik des Labs unberührt bleibt.

\subsection{Mehrbenutzer-Verhalten}
Die Rekonstruktionspipeline ist netzwerkagnostisch und backendgetrieben: Der Router publiziert Ergebnisse szenenweit, sodass mehrere Clients im selben Lab-Raum identische Resultate erhalten, ohne dass die Lab-Mehrbenutzer-Logik erweitert werden muss. Für den Fall, dass Clients zu unterschiedlichen Zeiten beitreten, liefert der Router das jüngste Ergebnis unmittelbar nach dem Handshake nach, wodurch ein konsistenter Zustand über die Sitzung hinweg gewährleistet bleibt.

\subsection{Deployment und Konfiguration}
Die Backend-Seite wird mittels Docker Compose im  bereitgestellt; Router und Worker-Container starten mit vordefinierten Modellnamen und GPU-Ressourcenzuteilung. Der Unity-Client wird mit Lab-spezifischen Endpunkten und Szenennamen konfiguriert; Umschalten zwischen Meta Quest Live-Capture und Evaluationsmodus erfolgt über das ICaptureDevice ohne Änderungen an der Lab-Szene. Bestehende Logging- und Persistenzmechanismen des Routers sichern Ergebnisse pro Szene und Modell, was Lab-seitig die nachträgliche Analyse ermöglicht, ohne zusätzliche Integrationsarbeit im Lab-Projekt.