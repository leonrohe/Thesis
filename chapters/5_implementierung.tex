\chapter{Implementierung}

Dieses Kapitel beschreibt die technische Realisierung der in Kapitel 4 entworfenen Systemarchitektur. Es detailliert die Umsetzung des Backends als asynchronen Verteilknoten, die Entwicklung des Unity-basierten Frontends sowie das entworfene Kommunikationsprotokoll. Besonderes Augenmerk liegt dabei auf den Designentscheidungen, die getroffen wurden, um die Anforderungen an Echtzeitfähigkeit, Modularität und Thread-Sicherheit zu erfüllen.

\section{Gesamtarchitektur und Technologie-Stack}

Das System RTReconstruct realisiert eine modulare Streaming-Pipeline, die räumlich verteilte VR-Clients mit rechenintensiven Rekonstruktionsdiensten verbindet. Die Implementierung gliedert sich in zwei Hauptkomponenten: ein Python-basiertes Backend, das mittels Container-Virtualisierung orchestriert wird, und ein C\#-basiertes Frontend innerhalb der Unity-Engine.

\subsection{Backend-Technologien}

Für die Umsetzung des Backends fiel die Wahl auf FastAPI in Kombination mit dem ASGI-Server Uvicorn. Im Gegensatz zu synchronen Frameworks (wie Flask) ermöglicht FastAPI durch die konsequente Nutzung von Pythons \texttt{asyncio}-Bibliothek eine nicht-blockierende Verarbeitung von Netzwerkanfragen. Dies ist für die Architektur essenziell: Der zentrale Router agiert als Vermittlungsstelle, die gleichzeitig hochfrequente Bilddaten von Clients empfängt und an mehrere GPU-Worker verteilt. Da die Inferenzzeiten der Modelle (z.\,B. MASt3R-SLAM oder NeuralRecon) stark variieren, würde ein synchrones Design dazu führen, dass der Router blockiert. Durch das asynchrone Design werden Netzwerk-I/O und Rechenoperationen entkoppelt, was den Durchsatz des Gesamtsystems maximiert.

Die Persistenz der Rekonstruktionsergebnisse wird durch eine SQLite-Datenbank gewährleistet, die Metadaten und Binär-Blobs (GLB-Dateien) speichert. Aufgrund der modularen Struktur werden alle Komponenten mittels Docker Compose orchestriert, wobei das NVIDIA Container Toolkit den Zugriff der Worker-Container auf die Host-GPU ermöglicht.

\subsection{Frontend-Technologien}

Das Frontend wurde in Unity 2022.3 LTS implementiert. Für die Netzwerkkommunikation kommt die Bibliothek \textit{NativeWebSocket} zum Einsatz, da sie im Gegensatz zu Standard-.NET-Sockets eine bessere Kompatibilität mit WebGL- und Android-Builds (Meta Quest) bietet. Das Rendering der 3D-Rekonstruktionen erfolgt hybrid: Punktwolken werden über den Visual Effect Graph (VFX Graph) GPU-beschleunigt dargestellt, während Mesh-Daten mittels der Bibliothek \textit{glTFast} zur Laufzeit importiert werden.

\section{Kommunikationsstandard}

Um die Latenz bei der Übertragung großer Bild- und Geometriedaten zu minimieren, wurde statt eines textbasierten Formats (wie JSON) ein maßgeschneidertes binäres Protokoll implementiert. Die Wahl eines binären Protokolls begründet sich durch die Vermeidung des ca.\ 33\,\% Overhead von Base64-Kodierung \needcitation, was insbesondere im drahtlosen VR-Betrieb die Bandbreite signifikant schont. Durch die JPEG-Komprimierung der Bilder und das GLB-Format für 3D-Modelle wird die Datenmenge weiter reduziert.

\subsection{Allgemeiner Aufbau}

Jedes Paket beginnt mit einem einheitlichen Header zur Validierung und Versionierung. Dies ermöglicht dem Empfänger, ungültige Pakete frühzeitig zu verwerfen und die Synchronisation im Fehlerfall wiederherzustellen. Der folgende C-Pseudocode, welcher der reinen Visualisierung dient, zeigt den allgemeinen Aufbau der Basistypen und des Headers:

\begin{lstlisting}[caption={Definition der Basistypen und des Headers}, label={lst:protocol_basics}]
// Basistypen
type int32   = 4 Byte Signed Integer (Little Endian);
type float32 = 4 Byte Floating Point (IEEE 754);
type byte    = 1 Byte Unsigned Integer;

// Zusammengesetzte Typen
struct String {
    int32  length;       // Laenge des Strings in Bytes
    byte[] utf8_bytes;   // UTF-8 kodierter Inhalt
};

struct Vector3 {
    float32 x; float32 y; float32 z;
};

struct Quaternion {
    float32 x; float32 y; float32 z; float32 w;
};

// Gemeinsamer Header fuer alle Pakete
struct PacketHeader {
    char[4] magic;      // ASCII
    int32   version;    // Protokollversion
};
\end{lstlisting}

\subsection{Pakettypen}

Basierend auf den in Kapitel 4 definierten Anforderungen wurden drei Hauptpakettypen spezifiziert: \texttt{ModelFragment}, \texttt{ModelResult} und \texttt{TransformFragment}. Jeder Pakettyp ist für eine spezifische Kommunikationsrichtung und Funktionalität zuständig.

\subsubsection{ModelFragment (Client an Server)}
Das \texttt{ModelFragment} (Version 1) bündelt eine Sequenz von Frames (Fenster) für die Inferenz. Um die Serialisierung effizient zu gestalten, werden die Arrays für Bilder, Intrinsics und Extrinsics nacheinander geschrieben, anstatt sie pro Frame zu verschachteln.

\begin{lstlisting}[caption={Struktur des ModelFragments (Upload)}, label={lst:struct_modelfragment}]
struct ModelFragment {
    PacketHeader header;      // Version = 1
    int32        window_size; // Anzahl der Frames (N)
    
    // Metadaten
    String       model_name;  // Zielmodell (z.B. "neucon")
    String       scene_name;  // Eindeutige Szenen-ID
    
    // Nutzdaten (Sequentiell angeordnet)
    FrameData[window_size]  frames;     // Liste der JPEG-Bilder
    Intrinsics[window_size] intrinsics; // Liste der Kameraparameter
    Extrinsics[window_size] extrinsics; // Liste der Kameraposen
};

// Hilfsstrukturen fuer ModelFragment
struct FrameData {
    int32   image_size;
    byte[]  jpeg_data;
    float32 width;
    float32 height;
};

struct Intrinsics {
    float32 fx; float32 fy;   // Brennweite
    float32 cx; float32 cy;   // Hauptpunkt
};

struct Extrinsics {
    Vector3    position;
    Quaternion rotation;
};
\end{lstlisting}

\subsubsection{ModelResult (Server an Client)}
Das \texttt{ModelResult} (Version 2) enthält das Ergebnis einer Rekonstruktion. Es besteht aus Transformationsdaten für die Ausrichtung in der Unity-Szene und einem binären Blob, der das 3D-Modell im GLB-Format (glTF Binary) enthält. Das GLB-Format wurde gewählt, da es Geometrie und Texturen effizient bündelt.

\begin{lstlisting}[caption={Struktur des ModelResults (Download)}, label={lst:struct_modelresult}]
struct ModelResult {
    PacketHeader header;         // Version = 2
    
    String       scene_name;     // Referenzierte Szene
    bool         is_pointcloud;  // 1 Byte (True/False)
    
    // Globale Transformation fuer Unity
    Vector3      position;
    Quaternion   rotation;
    Vector3      scale;
    
    // 3D-Modell
    byte[]       glb_payload;    // Rest des Datenstroms bis EOF
};
\end{lstlisting}

\subsubsection{TransformFragment (Client an Server)}
Das \texttt{TransformFragment} (Version 2) ist ein leichtgewichtiges Steuerpaket. Es dient dazu, die Position eines Modells auf dem Server nachträglich zu korrigieren (z.\,B. bei manuellem Alignment durch den Nutzer), ohne die rechenintensiven Bilddaten erneut zu übertragen.

\begin{lstlisting}[caption={Struktur des TransformFragments (Sync)}, label={lst:struct_transformfragment}]
struct TransformFragment {
    PacketHeader header;      // Version = 2
    
    String       scene_name;  // Zielszene
    
    // Neue Transformationsdaten
    Vector3      position;
    Quaternion   rotation;
    Vector3      scale;
};
\end{lstlisting}

\section{Backend-Implementierung}

Das Backend fungiert als Schaltzentrale des Systems. Es besteht aus dem zentralen Router und den modellspezifischen Workern.

\subsection{Asynchrones Routing und Fan-Out}

Der Router (\texttt{fragment\_hub.py}) verwaltet den globalen Zustand aller verbundenen Clients und aktiven Szenen. Er implementiert zwei WebSocket-Endpunkte:
\begin{enumerate}
    \item \texttt{/ws/client}: Hier verbinden sich Unity-Clients. Nach einem initialen Handshake, in dem der Client seine Rolle (Host/Visitor) und die gewünschte Szene übermittelt, registriert der Router den Client in der \texttt{connected\_clients}-Map.
    \item \texttt{/ws/model/\{name\}}: Hier verbinden sich die Rekonstruktions-Worker. Für jedes Modell wird eine \texttt{asyncio.Queue} angelegt.
\end{enumerate}
Eingehende Fragmente von Clients werden vom Router validiert, deserialisiert und in die Warteschlange des adressierten Modells gelegt. Sobald ein Worker ein Ergebnis liefert, wird dieses zunächst in der Datenbank persistiert gespeichert. Anschließend iteriert der Router über alle Clients, die der entsprechenden Szene zugeordnet sind, und sendet das Ergebnis asynchron über deren WebSocket-Verbindungen nach dem Fan-Out Prinzip. Durch die Nutzung von \texttt{asyncio.gather()} werden diese Sendevorgänge parallel ausgeführt, ohne die Verarbeitung neuer Fragmente zu blockieren.

\subsection{Modell-Worker}

Die Modell-Worker implementieren die eigentliche 3D-Rekonstruktion und laufen als eigenständige Prozesse, die über WebSockets mit dem Backend-Router kommunizieren. Grundlage ist die abstrakte Klasse \texttt{BaseReconstructionModel}, die die gemeinsame Kommunikationslogik kapselt und von allen konkreten Modellen erweitert wird.

\subsubsection{Basisklasse und Kommunikationsfluss}

Die Klasse \texttt{BaseReconstructionModel} initialisiert sich mit einem Modellnamen und einer Server-URL und leitet daraus den WebSocket-Endpunkt \texttt{/ws/model/<modelname>} des Backends ab. Die Methode \texttt{connect()} stellt die WebSocket-Verbindung her und startet asynchron die Schleife: \texttt{listen()} zum Empfangen von Fragmenten. Eingehende Nachrichten werden in \texttt{listen()} mittels der Hilfsfunktion zur Fragment-Deserialisierung in Python Datenstrukturen überführt und jeweils an die abstrakte Methode \texttt{handle\_fragment()} weitergegeben, die von den Unterklassen implementiert wird.

\begin{lstlisting}[language=python]
class BaseReconstructionModel(ABC):
    def __init__(self, model_name: str, server_url: str):
        # speichert Modellname und abgeleitete WebSocket-URL
        ...

    async def connect(self):
        # WebSocket-Verbindung herstellen und listen/processloop starten
        ...

    async def listen(self):
        # eingehende Bytes lesen, zu Fragmenten deserialisieren
        # und für jedes Fragment handle_fragment aufrufen
        ...

    async def send_result(self, result: ModelResult, success: bool = True):
        # Ergebnis serialisieren und als Bytes senden
        # Sonderfälle: Fehler (ein Null-Byte),
        # oder kein relevantes Ergebnis (ein Eins-Byte)
        ...

    @abstractmethod
    async def handle_fragment(self, fragment: dict):
        # von Unterklassen zu implementieren
        ...
\end{lstlisting}

\subsubsection{Spezialisierte Rekonstruktionsmodelle}

Die konkreten Modell-Worker laden jeweils ein externes Rekonstruktionsmodell (NeuralRecon, MASt3R-SLAM, SLAM3R oder VisFusion) und implementieren \texttt{handle\_fragment()}, um die empfangenen Fragmente in das jeweils benötigte Eingabeformat zu transformieren. Nach der Inferenz wird das Ergebnis als \texttt{ModelResult} mit der rekonstruierten Szene (GLB-Datei) und optionalen Transformationen erzeugt und über \texttt{send\_result()} an den Router zurückgesendet.

\subsubsection{Einsatz im Container-Setup}

Für den produktiven Betrieb werden die Worker-Skripte als eigenständige Prozesse bzw. Container gestartet, denen über Umgebungsvariablen der jeweilige Modellname (\texttt{MODELNAME}) und die Server-URL (\texttt{SERVERURL}) übergeben werden. Das Skript initialisiert daraus die passende Unterklasse von \texttt{BaseReconstructionModel} und startet mit \texttt{asyncio.run(model.connect())} die oben beschriebene Kommunikationsschleife, sodass sich jeder Modell-Worker automatisch beim zentralen Backend-Router registriert.

\section{Frontend-Implementierung}

Das Frontend ist für die Datenerfassung (Capture), die Netzwerkkommunikation und die Visualisierung zuständig.

\subsection{Hardware-Abstraktion mittels Strategy Pattern}

Um das System plattformunabhängig zu halten, wurde für den Zugriff auf Kamerasensoren das Strategy Pattern angewandt. Das Interface \texttt{ICaptureDevice} definiert Methoden zum Abruf von Bildern (\texttt{GetFrame}) sowie intrinsischen und extrinsischen Parametern. Es existieren spezifische Implementierungen für verschiedene Hardware-Typen:

\begin{itemize}
\item \textbf{MetaQuestCaptureDevice}: Nutzt die Passthrough-API der Meta Quest, um Zugriff auf die Umgebungskameras zu erhalten. Da die native Auflösung der Kameras variieren kann, werden die Intrinsics dynamisch auf die Zielauflösung (640$\times$480) skaliert.
\item \textbf{SmartphoneCaptureDevice}: Basiert auf ARFoundation und ermöglicht die Nutzung von Android/iOS-Geräten als externe Kameraquellen.
\item \textbf{EvalCaptureDevice}: Eine Implementierung für Reproduzierbarkeit, die statt echter Sensordaten eine virtuelle Kamera entlang aufgezeichneter Pfade bewegt.
\end{itemize}

\subsection{Fragmentsammlung mittels ModelCollector}

Der \texttt{ModelCollector} sammelt pro Frame Bilddaten und Kamera­parameter aus dem aktiven \texttt{ICaptureDevice} und nimmt einen Frame nur dann in das aktuelle Fenster auf, wenn eine ausreichende Bewegung seit dem letzten gültigen Frame vorliegt. Dafür müssen zwei Bedingungen erfüllt sein:
\begin{enumerate}
    \item eine Positionsänderung von mindestens \(\approx n\,\text{m}\),
    \item oder eine Rotationsänderung von mindestens \(\approx n^\circ\).
\end{enumerate}
Frames, die diese Kriterien sowie die Verfügbarkeit aller notwendigen Sensordaten erfüllen, werden gepuffert, bis die konfigurierte Fenstergröße erreicht ist und das vollständige Fragment an den \texttt{ReconstructionClient} weitergereicht wird. Auf diese Weise stellt der \texttt{ModelCollector} sicher, dass nur informative und ausreichend unterschiedliche Ansichten an das Backend übertragen werden.

\subsection{Thread-Sicherheit und Synchronisation}

Eine zentrale Herausforderung in der Entwicklung des Unity-Frontends stellt die fehlende Thread-Sicherheit der Engine-API dar. Da der \texttt{ReconstructionClient} eingehende Netzwerkpakete asynchron in Hintergrund-Threads empfängt, ist ein direkter Zugriff auf Szenenobjekte zur Laufzeit nicht möglich. Zur Lösung dieses Konflikts wurde im \texttt{RoomReconstructor} ein Dispatcher-Pattern implementiert. Eingehende Daten werden hierbei zunächst in einer thread-sicheren Warteschlange (\texttt{mainThreadActions}) gepuffert und anschließend synchronisiert im Hauptzyklus (Main Thread) der Engine verarbeitet. Dieser Ansatz entkoppelt die rechenintensive Deserialisierung von der Darstellung und gewährleistet eine stabile Szenenmanipulation ohne Blockierung des Render-Threads.

\subsection{Visualisierung mittels Spatial Hashing}

Die Darstellung der empfangenen 3D-Daten muss in Echtzeit erfolgen, ohne die Framerate der VR-Anwendung zu beeinträchtigen. Für Mesh-Daten (z.\,B.\ von NeuralRecon) wurde ein Chunking-Verfahren implementiert (\texttt{MeshUtils.ChunkMesh}). Da empfangene GLB-Modelle geometrisch komplex sein können, werden sie nach dem Import in ein $N \times N \times N$ Gitter unterteilt. Die Chunk-Größe wurde auf $5 \times 5 \times 5$ Meter festgelegt, was ein gutes Gleichgewicht zwischen Frustum-Culling-Effizienz und Verwaltungsoverhead darstellt. Dies ermöglicht der Unity-Engine, nicht sichtbare Bereiche mittels Frustum Culling effizient auszublenden und erhöht die Rendering-Performance auf mobiler Hardware signifikant.

Punktwolken (z.\,B.\ von SLAM3R) werden nicht als klassische GameObjects, sondern über \texttt{GraphicsBuffer} direkt an die GPU übergeben und mittels eines Compute-Shaders im VFX Graph gerendert, um den Overhead der CPU zu umgehen. Die aktuelle Darstellung ist auf 100\,000 Punkte begrenzt, da die Meta Quest 3 bei höheren Punktzahlen deutliche Framerate-Einbrüche zeigt. Dieser Trade-off zwischen Detailgrad und Performance wurde bewusst zugunsten einer flüssigen VR-Erfahrung gewählt.

\subsection{Integration in das Va.Si.Li-Lab}

Die Integration von \emph{RTReconstruct} in das Va.Si.Li-Lab verfolgt das Ziel, die bestehende Mehrbenutzerumgebung um einen rekonstruierbaren Raumkontext zu erweitern, ohne die etablierte Szenenlogik und Interaktionsmechanik des Labors grundlegend zu verändern. Statt eine separate Anwendung einzuführen, wird die Rekonstruktionsfunktionalität als optionaler Dienst in bestehende Szenen eingebettet und orientiert sich dabei an den vorhandenen Bedienkonzepten des Labors.
\subsubsection{Rollen- und Szenenkonzept}

Die Einbindung in die Nutzerführung des Va.Si.Li-Lab erfolgt über ein Rollen- und Szenenkonzept, das direkt in die bestehende UI integriert ist. Beim Start wählen die Nutzenden zunächst eine Rolle (\emph{Host} oder \emph{Visitor}) und anschließend eine Szene, die als gemeinsamer Bezeichner für die Sitzung dient. Diese Entscheidung legt fest, ob ein Client lediglich rekonstruierte Inhalte konsumiert oder zusätzlich die Datenerfassung initiiert.

\emph{Hosts} übernehmen die aktive Rolle in der Rekonstruktion, indem sie der gewählten Szene Sensordaten zuordnen und den Aufbau eines räumlichen Modells anstoßen. \emph{Visitors} treten derselben Szene bei, erhalten jedoch ausschließlich die daraus resultierenden Rekonstruktionen zur Visualisierung. Auf diese Weise wird verhindert, dass mehrere Teilnehmende parallel identische Umgebungen erfassen, während alle weiterhin in die gemeinsame Mehrbenutzersitzung des Va.Si.Li-Lab eingebunden bleiben.

\subsubsection{Integrationsprinzipien}

Zentral für die Gestaltung der Integration ist eine additive und möglichst invasive Einbindung in die bestehende Infrastruktur. \emph{RTReconstruct} wird nicht als neuer Hauptakteur der Anwendung eingeführt, sondern ergänzt das Va.Si.Li-Lab um einen weiteren, bei Bedarf zuschaltbaren Kontextkanal: Neben Avataren, Sprache und Objekten existiert nun ein rekonstruierter Raumzustand, der derselben Szene zugeordnet ist.

Daraus ergeben sich drei Integrationsprinzipien: Erstens wird an bestehende Konzepte angeknüpft (Rollenwahl, Szenenbezeichner), statt neue Metaphern einzuführen. Zweitens bleibt die Kopplung zwischen Rekonstruktionsdienst und Laborsystem lose, indem die Kommunikation im Wesentlichen über Szenennamen und räumliche Anker erfolgt. Drittens bleibt die Mehrbenutzerumgebung funktionsfähig, auch wenn die Rekonstruktion nicht genutzt oder temporär deaktiviert wird; die Rekonstruktionskomponente verhält sich in diesem Fall passiv und greift nicht in die restliche Szenenlogik ein.

\section{Zusammenfassung}

In diesem Kapitel wurde die Implementierung von RTReconstruct dargelegt. Durch den Einsatz von FastAPI und asynchronen Warteschlangen im Backend konnte ein performanter Router realisiert werden, der auch bei heterogenen Inferenzzeiten der Modelle stabil bleibt. Im Frontend ermöglichten Design-Patterns wie Strategy und Dispatcher eine saubere Trennung von Hardware-Zugriff, Netzwerklogik und Rendering. Das entwickelte binäre Protokoll sorgt dabei für eine effiziente Datenübertragung. Die Architektur ist somit robust genug, um in der anschließenden Evaluation (Kapitel 6) unter Echtzeitbedingungen geprüft zu werden.
