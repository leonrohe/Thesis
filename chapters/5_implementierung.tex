\chapter{Implementierung}

Dieses Kapitel beschreibt die technische Realisierung der in Kapitel 4 entworfenen Systemarchitektur. Es detailliert die Umsetzung des Backends als asynchronen Verteilknoten, die Entwicklung des Unity-basierten Frontends sowie das entworfene Kommunikationsprotokoll. Besonderes Augenmerk liegt dabei auf den Designentscheidungen, die getroffen wurden, um die Anforderungen an Echtzeitfähigkeit, Modularität und Thread-Sicherheit zu erfüllen.

\section{Gesamtarchitektur und Technologie-Stack}

Das System RTReconstruct realisiert eine modulare Streaming-Pipeline, die räumlich verteilte VR-Clients mit rechenintensiven Rekonstruktionsdiensten verbindet. Die Implementierung gliedert sich in zwei Hauptkomponenten: ein Python-basiertes Backend, das mittels Container-Virtualisierung orchestriert wird, und ein C\#-basiertes Frontend innerhalb der Unity-Engine.

\subsection{Backend-Technologien}

Für die Umsetzung des Backends fiel die Wahl auf FastAPI in Kombination mit dem ASGI-Server Uvicorn. Im Gegensatz zu synchronen Frameworks (wie Flask) ermöglicht FastAPI durch die konsequente Nutzung von Pythons \texttt{asyncio}-Bibliothek eine nicht-blockierende Verarbeitung von Netzwerkanfragen. Dies ist für die Architektur essenziell: Der zentrale Router agiert als Vermittlungsstelle, die gleichzeitig hochfrequente Bilddaten von Clients empfängt und an mehrere GPU-Worker verteilt. Da die Inferenzzeiten der Modelle (z.\,B. MASt3R-SLAM oder NeuralRecon) stark variieren, würde ein synchrones Design dazu führen, dass der Router blockiert. Durch das asynchrone Design werden Netzwerk-I/O und Rechenoperationen entkoppelt, was den Durchsatz des Gesamtsystems maximiert.

Die Persistenz der Rekonstruktionsergebnisse wird durch eine SQLite-Datenbank gewährleistet, die Metadaten und Binär-Blobs (GLB-Dateien) speichert. Aufgrund der modularen Struktur werden alle Komponenten mittels Docker Compose orchestriert, wobei das NVIDIA Container Toolkit den Zugriff der Worker-Container auf die Host-GPU ermöglicht.

\subsection{Frontend-Technologien}

Das Frontend wurde in Unity 2022.3 LTS implementiert. Für die Netzwerkkommunikation kommt die Bibliothek \textit{NativeWebSocket} zum Einsatz, da sie im Gegensatz zu Standard-.NET-Sockets eine bessere Kompatibilität mit WebGL- und Android-Builds (Meta Quest) bietet. Das Rendering der 3D-Rekonstruktionen erfolgt hybrid: Punktwolken werden über den Visual Effect Graph (VFX Graph) GPU-beschleunigt dargestellt, während Mesh-Daten mittels der Bibliothek \textit{glTFast} zur Laufzeit importiert werden.

\section{Kommunikationsstandard}

Um die Latenz bei der Übertragung großer Bild- und Geometriedaten zu minimieren, wurde statt eines textbasierten Formats (wie JSON) ein maßgeschneidertes binäres Protokoll implementiert. Die Wahl eines binären Protokolls begründet sich durch die Vermeidung des ca.\ 33\,\% Overhead von Base64-Kodierung, was insbesondere im drahtlosen VR-Betrieb die Bandbreite signifikant schont.

\subsection{Protokollstruktur}

Jedes Paket beginnt mit einem 4-Byte Magic Header (\texttt{LEON}), der zur Validierung des Datenstroms und zur Synchronisation bei Verbindungsfehlern dient. Darauf folgt eine Versionierung, die eine spätere Erweiterbarkeit des Systems ohne Bruch der Abwärtskompatibilität gewährleistet. Tabelle \ref{tab:message_mapping} zeigt die Zuordnung zwischen den konzeptionellen Nachrichtentypen aus Kapitel 4 und deren konkreter Umsetzung im Protokoll.

\begin{table}[h]
\centering
\caption{Mapping der konzeptionellen Nachrichtentypen auf implementierte Protokollstrukturen}
\label{tab:message_mapping}
\begin{tabular}{lll}
\hline
\textbf{Konzept (Kap.\ 4)} & \textbf{Implementierung} & \textbf{Version} \\
\hline
Rekonstruktionsanfragen & ModelFragment & 1 \\
Rekonstruktionsantworten & ModelResult & 2 \\
Sitzungs-/Konfigurationsnachrichten & TransformFragment & 2 \\
\hline
\end{tabular}
\end{table}

Die drei Nachrichtentypen sind wie folgt strukturiert:

\begin{itemize}
\item \textbf{ModelFragment} (Version 1): Enthält die Ziel-Modellkennung, den Szenennamen sowie eine variable Anzahl an Frames. Jeder Frame umfasst ein JPEG-kodiertes Bild, intrinsische Kameraparameter (Brennweite, Hauptpunkt) und extrinsische Parameter (Position, Rotation als Quaternion).

\item \textbf{ModelResult} (Version 2): Transportiert die vom Worker berechnete 3D-Repräsentation zurück zum Client. Es enthält eine Transformation (Position, Rotation, Skalierung), ein Flag zur Unterscheidung zwischen Mesh und Punktwolke sowie die Geometriedaten als GLB-Binärstrom.

\item \textbf{TransformFragment} (Version 2): Ermöglicht nachträgliche Pose-Korrekturen bestehender Rekonstruktionen, ohne eine erneute Inferenz anzustoßen. Es enthält lediglich Szenenname, Modellkennung und die aktualisierte Transformation.
\end{itemize}

JPEG-Kompression wurde gewählt, um Latenz zu minimieren, trotz geringer Qualitätsverluste gegenüber verlustfreien Formaten. GLB als Mesh-Format bietet den Vorteil, dass es Geometrie und Textur in einer Datei bündelt und von \textit{glTFast} effizient zur Laufzeit geladen werden kann. Die Entscheidung für manuelles Byte-Packing (mittels Python \texttt{struct} und C\# \texttt{BinaryWriter}) eliminiert den Overhead, der bei einer Base64-Kodierung von Binärdaten entstehen würde.

\section{Backend-Implementierung}

Das Backend fungiert als Schaltzentrale des Systems. Es besteht aus dem zentralen Router und den modellspezifischen Workern.

\subsection{Asynchrones Routing und Fan-Out}

Der Router (\texttt{fragment\_hub.py}) verwaltet den globalen Zustand aller verbundenen Clients und aktiven Szenen. Er implementiert zwei WebSocket-Endpunkte:

\begin{enumerate}
\item \texttt{/ws/client}: Hier verbinden sich Unity-Clients. Nach einem initialen Handshake, in dem der Client seine Rolle (Host/Visitor) und die gewünschte Szene übermittelt, registriert der Router den Client in der \texttt{connected\_clients}-Map.
\item \texttt{/ws/model/\{name\}}: Hier verbinden sich die Rekonstruktions-Worker. Für jedes Modell wird eine \texttt{asyncio.Queue} angelegt.
\end{enumerate}

Eingehende Fragmente von Clients werden vom Router validiert, deserialisiert und in die Warteschlange des adressierten Modells gelegt. Sobald ein Worker ein Ergebnis liefert, wird dieses zunächst in der Datenbank persistiert. Anschließend iteriert der Router über alle Clients, die der entsprechenden Szene zugeordnet sind, und sendet das Ergebnis asynchron über deren WebSocket-Verbindungen. Durch die Nutzung von \texttt{asyncio.gather()} werden diese Sendevorgänge parallel ausgeführt, ohne die Verarbeitung neuer Fragmente zu blockieren.

\subsection{Modell-Worker}

Die Worker-Container basieren auf einer abstrakten Basisklasse \texttt{BaseReconstructionModel}. Diese kapselt die Boilerplate-Logik für Verbindungsaufbau und Fehlerbehandlung. Konkrete Implementierungen (z.\,B.\ \texttt{NeuralReconstructionModel}, \texttt{SLAM3RReconstructModel}) müssen lediglich die Methode \texttt{handle\_fragment()} überschreiben. Diese Architektur ermöglicht die Integration neuer Forschungsansätze mit minimalem Aufwand, solange sie das Ein- und Ausgabeformat einhalten. Fehlerbehandlung erfolgt über \texttt{try-except}-Blöcke in der \texttt{handle\_fragment()}-Methode, die bei GPU-Out-of-Memory-Fehlern oder korrupten Frames eine Fallback-Antwort generieren und den Worker-Zustand zurücksetzen.

\section{Frontend-Implementierung}

Das Frontend ist für die Datenerfassung (Capture), die Netzwerkkommunikation und die Visualisierung zuständig.

\subsection{Hardware-Abstraktion mittels Strategy Pattern}

Um das System plattformunabhängig zu halten, wurde für den Zugriff auf Kamerasensoren das Strategy Pattern angewandt. Das Interface \texttt{ICaptureDevice} definiert Methoden zum Abruf von Bildern (\texttt{GetFrame}) sowie intrinsischen und extrinsischen Parametern. Es existieren spezifische Implementierungen für verschiedene Hardware-Typen:

\begin{itemize}
\item \textbf{MetaQuestCaptureDevice}: Nutzt die Passthrough-API der Meta Quest, um Zugriff auf die Umgebungskameras zu erhalten. Da die native Auflösung der Kameras variieren kann, werden die Intrinsics dynamisch auf die Zielauflösung (640$\times$480) skaliert.
\item \textbf{SmartphoneCaptureDevice}: Basiert auf ARFoundation und ermöglicht die Nutzung von Android/iOS-Geräten als externe Kameraquellen.
\item \textbf{EvalCaptureDevice}: Eine Implementierung für Reproduzierbarkeit, die statt echter Sensordaten eine virtuelle Kamera entlang aufgezeichneter Pfade bewegt.
\end{itemize}

\subsection{Thread-Sicherheit mittels Dispatcher Pattern}

Eine zentrale Herausforderung bei der Integration in Unity ist die fehlende Thread-Sicherheit der Engine-API. Netzwerk-Nachrichten des \texttt{ReconstructionClient} werden asynchron in Hintergrund-Threads empfangen. Ein direkter Zugriff aus diesen Threads auf Szenen-Objekte (z.\,B.\ zum Instanziieren von Meshes) würde zu Laufzeitfehlern führen.

Um dieses Problem zu lösen, wurde ein Dispatcher-Pattern implementiert: Eingehende Nachrichten werden zunächst in einer thread-sicheren Warteschlange (\texttt{mainThreadActions}) zwischengespeichert. Die Komponente \texttt{RoomReconstructor} arbeitet diese Warteschlange innerhalb des Unity-Hauptzyklus (\texttt{Update}-Methode) ab. Dies stellt sicher, dass rechenintensive Deserialisierung parallel im Hintergrund läuft, während die eigentliche Manipulation der Szene synchronisiert im Haupt-Thread erfolgt.

\subsection{Visualisierung mittels Spatial Hashing}

Die Darstellung der empfangenen 3D-Daten muss in Echtzeit erfolgen, ohne die Framerate der VR-Anwendung zu beeinträchtigen. Für Mesh-Daten (z.\,B.\ von NeuralRecon) wurde ein Chunking-Verfahren implementiert (\texttt{MeshUtils.ChunkMesh}). Da empfangene GLB-Modelle geometrisch komplex sein können, werden sie nach dem Import in ein $N \times N \times N$ Gitter unterteilt. Die Chunk-Größe wurde auf $5 \times 5 \times 5$ Meter festgelegt, was ein gutes Gleichgewicht zwischen Frustum-Culling-Effizienz und Verwaltungsoverhead darstellt. Dies ermöglicht der Unity-Engine, nicht sichtbare Bereiche mittels Frustum Culling effizient auszublenden und erhöht die Rendering-Performance auf mobiler Hardware signifikant.

Punktwolken (z.\,B.\ von SLAM3R) werden nicht als klassische GameObjects, sondern über \texttt{GraphicsBuffer} direkt an die GPU übergeben und mittels eines Compute-Shaders im VFX Graph gerendert, um den Overhead der CPU zu umgehen. Die aktuelle Darstellung ist auf 100\,000 Punkte begrenzt, da die Meta Quest 3 bei höheren Punktzahlen deutliche Framerate-Einbrüche zeigt. Dieser Trade-off zwischen Detailgrad und Performance wurde bewusst zugunsten einer flüssigen VR-Erfahrung gewählt.

\section{Integration in das Va.Si.Li-Lab}

Die Integration erfolgte in die bestehende Infrastruktur des Va.Si.Li-Labs. Ein wesentliches Entwurfsziel war die Separation of Concerns. Die Rekonstruktionspipeline nutzt daher nicht die bestehenden Ubiq-Netzwerkkanäle des Labs, sondern baut eine dedizierte WebSocket-Verbindung auf. Dies verhindert Kollisionen mit der synchronen Multi-User-Logik des Labs. Die Trennung der Netzwerk-Ports (Ubiq nutzt standardmäßig Port 8009, RTReconstruct verwendet Port 8765) stellt sicher, dass beide Systeme ohne Konflikte parallel betrieben werden können.

Rekonstruktionsergebnisse werden als Kind-Objekte eines zentralen Referenzpunkts (``RoomAnchor'') instanziiert. Dies erlaubt es, die gesamte rekonstruierte Szene relativ zum virtuellen Raum des Labs zu transformieren, ohne die lokalen Koordinaten der einzelnen Fragmente neu berechnen zu müssen.

\section{Zusammenfassung}

In diesem Kapitel wurde die Implementierung von RTReconstruct dargelegt. Durch den Einsatz von FastAPI und asynchronen Warteschlangen im Backend konnte ein performanter Router realisiert werden, der auch bei heterogenen Inferenzzeiten der Modelle stabil bleibt. Im Frontend ermöglichten Design-Patterns wie Strategy und Dispatcher eine saubere Trennung von Hardware-Zugriff, Netzwerklogik und Rendering. Das entwickelte binäre Protokoll sorgt dabei für eine effiziente Datenübertragung. Die Architektur ist somit robust genug, um in der anschließenden Evaluation (Kapitel 6) unter Echtzeitbedingungen geprüft zu werden.
